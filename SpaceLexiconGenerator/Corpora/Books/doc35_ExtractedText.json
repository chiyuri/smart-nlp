{"content": "\nNASA/SP-2007-6105\nRev1\n\nNASA\nSystems Engineering\nHandbook\n\n\n\nNASA STI Program \u2026 in Profile\n\nSince its founding, the National Aeronautics and Space \nAdministration (NASA) has been dedicated to the ad-\nvancement of aeronautics and space science. The NASA \nScientific and Technical Information (STI) program \nplays a key part in helping NASA maintain this impor-\ntant role.\n\nThe NASA STI program operates under the auspices of \nthe Agency Chief Information Officer. It collects, orga-\nnizes, provides for archiving, and disseminates NASA\u2019s \nSTI. The NASA STI program provides access to the \nNASA Aeronautics and Space Database and its public \ninterface, the NASA technical report server, thus pro-\nviding one of the largest collections of aeronautical and \nspace science STI in the world. Results are published in \nboth non-NASA channels and by NASA in the NASA \nSTI report series, which include the following report \ntypes: \n\nTechnical Publication: z  Reports of completed research \nor a major significant phase of research that present the \nresults of NASA programs and include extensive data \nor theoretical analysis. Includes compilations of sig-\nnificant scientific and technical data and information \ndeemed to be of continuing reference value. NASA \ncounterpart of peer-reviewed formal professional pa-\npers but has less stringent limitations on manuscript \nlength and extent of graphic presentations.\nTechnical Memorandum: z  Scientific and technical \nfindings that are preliminary or of specialized interest, \ne.g., quick release reports, working papers, and bibli-\nographies that contain minimal annotation. Does not \ncontain extensive analysis.\n\nContractor Report: z  Scientific and technical findings \nby NASA-sponsored contractors and grantees.\nConference Publication: z  Collected papers from scien-\ntific and technical conferences, symposia, seminars, or \nother meetings sponsored or co-sponsored by NASA.\nSpecial Publication: z  Scientific, technical, or histor-\nical information from NASA programs, projects, and \nmissions, often concerned with subjects having sub-\nstantial public interest.\nTechnical Translation: z  English-language translations \nof foreign scientific and technical material pertinent \nto NASA\u2019s mission.\n\nSpecialized services also include creating custom the-\nsauri, building customized databases, and organizing \nand publishing research results.\n\nFor more information about the NASA STI program, see \nthe following:\n\nAccess the NASA STI program home page at   z\nwww.sti.nasa.gov\nE-mail your question via the Internet to   z\nhelp@sti.nasa.gov\nFax your question to the NASA STI help desk at   z\n301-621-0134\nPhone the NASA STI help desk at 301-621-0390 z\nWrite to: z\nNASA STI Help Desk\nNASA Center for AeroSpace Information\n7115 Standard Drive\nHanover, MD 21076-1320\n\n\n\nNASA/SP-2007-6105 Rev1\n\nSystems Engineering Handbook\n\nNational Aeronautics and Space Administration\nNASA Headquarters\nWashington, D.C. 20546\n\nDecember 2007\n\n\n\nTo request print or electronic copies or provide comments, \ncontact the Office of the Chief Engineer via \n\nSP6105rev1SEHandbook@nasa.gov \n\nElectronic copies are also available from\nNASA Center for AeroSpace Information\n\n7115 Standard Drive\nHanover, MD 21076-1320\n\nat\nhttp://ntrs.nasa.gov/\n\n\n\nNASA Systems Engineering Handbook ? iii\n\nTable of Contents\n\nPreface ..............................................................................................................................................................xiii\n\nAcknowledgments ............................................................................................................................................xv\n\n1.0 Introduction ...............................................................................................................................................1\n1.1 Purpose ....................................................................................................................................................................... 1\n1.2 Scope and Depth ........................................................................................................................................................ 1\n\n2.0 Fundamentals of Systems Engineering .....................................................................................................3\n2.1 The Common Technical Processes and the SE Engine ......................................................................................... 4\n2.2 An Overview of the SE Engine by Project Phase ................................................................................................... 6\n2.3 Example of Using the SE Engine .............................................................................................................................. 7\n\n2.3.1 Detailed Example ........................................................................................................................................... 8\n2.3.2 Example Premise ............................................................................................................................................ 8\n\n2.3.2.1 Example Phase A System Design Passes....................................................................................... 8\n2.3.2.2 Example Product Realization Passes ........................................................................................... 12\n2.3.2.3 Example Use of the SE Engine in Phases B Through D ............................................................ 14\n2.3.2.4 Example Use of the SE Engine in Phases E and F ..................................................................... 14\n\n2.4 Distinctions Between Product Verification and Product Validation ................................................................ 15\n2.5 Cost Aspect of Systems Engineering ..................................................................................................................... 16\n\n3.0 NASA Program/Project Life Cycle ............................................................................................................ 19\n3.1 Program Formulation .............................................................................................................................................. 19\n3.2 Program Implementation ....................................................................................................................................... 21\n3.3 Project Pre-Phase A: Concept Studies .................................................................................................................. 22\n3.4 Project Phase A: Concept and Technology Development .................................................................................. 22\n3.5 Project Phase B: Preliminary Design and Technology Completion ................................................................. 24\n3.6 Project Phase C: Final Design and Fabrication .................................................................................................... 25\n3.7 Project Phase D: System Assembly, Integration and Test, Launch .................................................................... 25\n3.8 Project Phase E: Operations and Sustainment ..................................................................................................... 28\n3.9 Project Phase F: Closeout ....................................................................................................................................... 28\n3.10 Funding: The Budget Cycle ..................................................................................................................................... 29\n\n4.0 System Design ......................................................................................................................................... 31\n4.1 Stakeholder Expectations Definition ..................................................................................................................... 33\n\n4.1.1 Process Description ..................................................................................................................................... 33\n4.1.1.1 Inputs .............................................................................................................................................. 33\n4.1.1.2 Process Activities ........................................................................................................................... 33\n4.1.1.3 Outputs ........................................................................................................................................... 35\n\n4.1.2 Stakeholder Expectations Definition Guidance ....................................................................................... 35\n4.1.2.1 Concept of Operations .................................................................................................................. 35\n\n4.2 Technical Requirements Definition ....................................................................................................................... 40\n4.2.1 Process Description ..................................................................................................................................... 40\n\n4.2.1.1 Inputs .............................................................................................................................................. 41\n4.2.1.2 Process Activities ........................................................................................................................... 41\n4.2.1.3 Outputs ........................................................................................................................................... 41\n\n4.2.2 Technical Requirements Definition Guidance ......................................................................................... 41\n4.2.2.1 Types of Requirements.................................................................................................................. 41\n\n\n\niv ? NASA Systems Engineering Handbook\n\nTable of Contents\n\n4.2.2.2 Human Factors Engineering Requirements ............................................................................... 45\n4.2.2.3 Requirements Decomposition, Allocation, and Validation ..................................................... 45\n4.2.2.4 Capturing Requirements and the Requirements Database ...................................................... 47\n4.2.2.5 Technical Standards ...................................................................................................................... 47\n\n4.3 Logical Decomposition ........................................................................................................................................... 49\n4.3.1 Process Description  .................................................................................................................................... 49\n\n4.3.1.1 Inputs .............................................................................................................................................. 49\n4.3.1.2 Process Activities ........................................................................................................................... 49\n4.3.1.3 Outputs ........................................................................................................................................... 51\n\n4.3.2 Logical Decomposition Guidance ............................................................................................................. 52\n4.3.2.1 Product Breakdown Structure ..................................................................................................... 52\n4.3.2.2 Functional Analysis Techniques .................................................................................................. 52\n\n4.4 Design Solution Definition ....................................................................................................................................... 55\n4.4.1 Process Description ........................................................................................................................................ 55\n\n4.4.1.1 Inputs ................................................................................................................................................ 55\n4.4.1.2 Process Activities ............................................................................................................................. 56\n4.4.1.3 Outputs ............................................................................................................................................. 61\n\n4.4.2 Design Solution Definition Guidance.......................................................................................................... 62\n4.4.2.1 Technology Assessment ................................................................................................................ 62\n4.4.2.2 Integrating Engineering Specialties into the Systems Engineering Process .......................... 62\n\n5.0 Product Realization ................................................................................................................................. 71\n5.1 Product Implementation ......................................................................................................................................... 73\n\n5.1.1 Process Description ..................................................................................................................................... 73\n5.1.1.1 Inputs .............................................................................................................................................. 73\n5.1.1.2 Process Activities  .......................................................................................................................... 74\n5.1.1.3 Outputs ........................................................................................................................................... 75\n\n5.1.2 Product Implementation Guidance ........................................................................................................... 76\n5.1.2.1 Buying Off-the-Shelf Products .................................................................................................... 76\n5.1.2.2 Heritage ........................................................................................................................................... 76\n\n5.2 Product Integration ................................................................................................................................................. 78\n5.2.1 Process Description ..................................................................................................................................... 78\n\n5.2.1.1 Inputs  ............................................................................................................................................. 79\n5.2.1.2 Process Activities ........................................................................................................................... 79\n5.2.1.3 Outputs ........................................................................................................................................... 79\n\n5.2.2 Product Integration Guidance ................................................................................................................... 80\n5.2.2.1 Integration Strategy ....................................................................................................................... 80\n5.2.2.2 Relationship to Product Implementation  .................................................................................. 80\n5.2.2.3 Product/Interface Integration Support ....................................................................................... 80\n5.2.2.4 Product Integration of the Design Solution ............................................................................... 81\n5.2.2.5 Interface Management .................................................................................................................. 81\n5.2.2.6 Compatibility Analysis.................................................................................................................. 81\n5.2.2.7 Interface Management Tasks ........................................................................................................ 81\n\n5.3 Product Verification  ............................................................................................................................................... 83\n5.3.1 Process Description ..................................................................................................................................... 83\n\n5.3.1.1 Inputs .............................................................................................................................................. 83\n5.3.1.2 Process Activities ........................................................................................................................... 84\n5.3.1.3 Outputs ........................................................................................................................................... 89\n\n5.3.2 Product Verification Guidance................................................................................................................... 89\n5.3.2.1 Verification Program ..................................................................................................................... 89\n5.3.2.2 Verification in the Life Cycle ........................................................................................................ 89\n5.3.2.3 Verification Procedures ................................................................................................................ 92\n\n\n\nTable of Contents\n\nNASA Systems Engineering Handbook ? v\n\n5.3.2.4 Verification Reports ...................................................................................................................... 93\n5.3.2.5 End-to-End System Testing  ......................................................................................................... 93\n5.3.2.6 Modeling and Simulation ............................................................................................................. 96\n5.3.2.7 Hardware-in-the-Loop ................................................................................................................. 96\n\n5.4 Product Validation ................................................................................................................................................... 98\n5.4.1 Process Description ..................................................................................................................................... 98\n\n5.4.1.1 Inputs .............................................................................................................................................. 98\n5.4.1.2 Process Activities ........................................................................................................................... 99\n5.4.1.3 Outputs ......................................................................................................................................... 104\n\n5.4.2 Product Validation Guidance ................................................................................................................... 104\n5.4.2.1 Modeling and Simulation ........................................................................................................... 104\n5.4.2.2 Software ........................................................................................................................................ 104\n\n5.5 Product Transition  ................................................................................................................................................ 106\n5.5.1 Process Description ................................................................................................................................... 106\n\n5.5.1.1 Inputs ............................................................................................................................................ 106\n5.5.1.2 Process Activities ......................................................................................................................... 107\n5.5.1.3 Outputs ......................................................................................................................................... 109\n\n5.5.2 Product Transition Guidance ................................................................................................................... 110\n5.5.2.1 Additional Product Transition Input Considerations ............................................................ 110\n5.5.2.2 After Product Transition to the End User\u2014What Next? ....................................................... 110\n\n6.0 Crosscutting Technical Management .................................................................................................. 111\n6.1 Technical Planning ................................................................................................................................................ 112\n\n6.1.1 Process Description ................................................................................................................................... 112\n6.1.1.1 Inputs ............................................................................................................................................ 112\n6.1.1.2 Process Activities ......................................................................................................................... 113\n6.1.1.3 Outputs ......................................................................................................................................... 122\n\n6.1.2 Technical Planning Guidance .................................................................................................................. 122\n6.1.2.1 Work Breakdown Structure ........................................................................................................ 122\n6.1.2.2 Cost Definition and Modeling ................................................................................................... 125\n6.1.2.3 Lessons Learned  .......................................................................................................................... 129\n\n6.2 Requirements Management .................................................................................................................................. 131\n6.2.1 Process Description ................................................................................................................................... 131\n\n6.2.1.1 Inputs ............................................................................................................................................ 131\n6.2.1.2 Process Activities ......................................................................................................................... 132\n6.2.1.3 Outputs ......................................................................................................................................... 134\n\n6.2.2 Requirements Management Guidance .................................................................................................... 134\n6.2.2.1 Requirements Management Plan .............................................................................................. 134\n6.2.2.2 Requirements Management Tools ............................................................................................. 135\n\n6.3 Interface Management .......................................................................................................................................... 136\n6.3.1 Process Description ................................................................................................................................... 136\n\n6.3.1.1 Inputs ............................................................................................................................................ 136\n6.3.1.2 Process Activities ......................................................................................................................... 136\n6.3.1.3 Outputs ......................................................................................................................................... 137\n\n6.3.2 Interface Management Guidance ............................................................................................................. 137\n6.3.2.1 Interface Requirements Document ........................................................................................... 137\n6.3.2.2 Interface Control Document or Interface Control Drawing ................................................. 137\n6.3.2.3 Interface Definition Document  ................................................................................................ 138\n6.3.2.4 Interface Control Plan ................................................................................................................. 138\n\n6.4 Technical Risk Management................................................................................................................................. 139\n6.4.1 Process Description ................................................................................................................................... 140\n\n6.4.1.1 Inputs ............................................................................................................................................ 140\n\n\n\nvi ? NASA Systems Engineering Handbook\n\nTable of Contents\n\n6.4.1.2 Process Activities ......................................................................................................................... 140\n6.4.1.3 Outputs ......................................................................................................................................... 141\n\n6.4.2 Technical Risk Management Guidance ................................................................................................... 141\n6.4.2.1 Role of Continuous Risk Management in Technical Risk Management  ............................. 142\n6.4.2.2 The Interface Between CRM and Risk-Informed Decision Analysis ................................... 142\n6.4.2.3 Selection and Application of Appropriate Risk Methods ....................................................... 143\n\n6.5 Configuration Management  ................................................................................................................................ 151\n6.5.1 Process Description ................................................................................................................................... 151\n\n6.5.1.1 Inputs ............................................................................................................................................ 151\n6.5.1.2 Process Activities ......................................................................................................................... 151\n6.5.1.3 Outputs ......................................................................................................................................... 156\n\n6.5.2 CM Guidance ............................................................................................................................................. 156\n6.5.2.1 What Is the Impact of Not Doing CM? .................................................................................... 156\n6.5.2.2 When Is It Acceptable to Use Redline Drawings? ................................................................... 157\n\n6.6 Technical Data Management ................................................................................................................................ 158\n6.6.1 Process Description ................................................................................................................................... 158\n\n6.6.1.1 Inputs ............................................................................................................................................ 158\n6.6.1.2  Process Activities ........................................................................................................................ 158\n6.6.1.3 Outputs ......................................................................................................................................... 162\n\n6.6.2 Technical Data Management Guidance .................................................................................................. 162\n6.6.2.1 Data Security and ITAR .............................................................................................................. 162\n\n6.7 Technical Assessment ............................................................................................................................................ 166\n6.7.1 Process Description ................................................................................................................................... 166\n\n6.7.1.1 Inputs ............................................................................................................................................ 166\n6.7.1.2 Process Activities ......................................................................................................................... 166\n6.7.1.3 Outputs ......................................................................................................................................... 167\n\n6.7.2 Technical Assessment Guidance .............................................................................................................. 168\n6.7.2.1 Reviews, Audits, and Key Decision Points  .............................................................................. 168\n6.7.2.2 Status Reporting and Assessment .............................................................................................. 190\n\n6.8 Decision Analysis ................................................................................................................................................... 197\n6.8.1 Process Description ................................................................................................................................... 197\n\n6.8.1.1 Inputs ............................................................................................................................................ 198\n6.8.1.2 Process Activities ......................................................................................................................... 199\n6.8.1.3 Outputs ......................................................................................................................................... 202\n\n6.8.2 Decision Analysis Guidance ..................................................................................................................... 203\n6.8.2.1 Systems Analysis, Simulation, and Performance ..................................................................... 203\n6.8.2.2 Trade Studies ................................................................................................................................ 205\n6.8.2.3 Cost-Benefit Analysis .................................................................................................................. 209\n6.8.2.4 Influence Diagrams ..................................................................................................................... 210\n6.8.2.5 Decision Trees .............................................................................................................................. 210\n6.8.2.6 Multi-Criteria Decision Analysis .............................................................................................. 211\n6.8.2.7 Utility Analysis ............................................................................................................................. 212\n6.8.2.8 Risk-Informed Decision Analysis Process Example ............................................................... 213\n\n7.0 Special Topics ........................................................................................................................................ 217\n7.1 Engineering with Contracts ................................................................................................................................. 217\n\n7.1.1 Introduction, Purpose, and Scope ........................................................................................................... 217\n7.1.2 Acquisition Strategy................................................................................................................................... 217\n\n7.1.2.1 Develop an Acquisition Strategy ............................................................................................... 218\n7.1.2.2 Acquisition Life Cycle ................................................................................................................. 218\n7.1.2.3 NASA Responsibility for Systems Engineering ....................................................................... 218\n\n7.1.3 Prior to Contract Award ........................................................................................................................... 219\n\n\n\nTable of Contents\n\nNASA Systems Engineering Handbook ? vii\n\n7.1.3.1 Acquisition Planning................................................................................................................... 219\n7.1.3.2 Develop the Statement of Work ................................................................................................. 223\n7.1.3.3 Task Order Contracts .................................................................................................................. 225\n7.1.3.4 Surveillance Plan ......................................................................................................................... 225\n7.1.3.5 Writing Proposal Instructions and Evaluation Criteria ......................................................... 226\n7.1.3.6 Selection of COTS Products ...................................................................................................... 226\n7.1.3.7 Acquisition-Unique Risks .......................................................................................................... 227\n\n7.1.4 During Contract Performance ................................................................................................................. 227\n7.1.4.1 Performing Technical Surveillance  .......................................................................................... 227\n7.1.4.2 Evaluating Work Products .......................................................................................................... 229\n7.1.4.3 Issues with Contract-Subcontract Arrangements ................................................................... 229\n\n7.1.5 Contract Completion  ............................................................................................................................... 230\n7.1.5.1 Acceptance of Final Deliverables ............................................................................................... 230\n7.1.5.2 Transition Management .............................................................................................................. 231\n7.1.5.3 Transition to Operations and Support ...................................................................................... 232\n7.1.5.4 Decommissioning and Disposal ................................................................................................ 233\n7.1.5.5 Final Evaluation of Contractor Performance ........................................................................... 233\n\n7.2 Integrated Design Facilities .................................................................................................................................. 234\n7.2.1 Introduction  .............................................................................................................................................. 234\n7.2.2 CACE Overview and Importance ............................................................................................................ 234\n7.2.3 CACE Purpose and Benefits ..................................................................................................................... 235\n7.2.4 CACE Staffing ............................................................................................................................................. 235\n7.2.5 CACE Process ............................................................................................................................................. 236\n\n7.2.5.1 Planning and Preparation ........................................................................................................... 236\n7.2.5.2 Activity Execution Phase ............................................................................................................ 236\n7.2.5.3 Activity Wrap-Up  ....................................................................................................................... 237\n\n7.2.6 CACE Engineering Tools and Techniques  ............................................................................................ 237\n7.2.7 CACE Facility, Information Infrastructure, and Staffing ..................................................................... 238\n\n7.2.7.1 Facility ........................................................................................................................................... 238\n7.2.7.2 Information Infrastructure ......................................................................................................... 238\n7.2.7.3 Facility Support Staff Responsibilities ....................................................................................... 239\n\n7.2.8 CACE Products  ......................................................................................................................................... 239\n7.2.9 CACE Best Practices .................................................................................................................................. 239\n\n7.2.9.1 People ............................................................................................................................................ 240\n7.2.9.2 Process and Tools ........................................................................................................................ 240\n7.2.9.3 Facility ........................................................................................................................................... 240\n\n7.3 Selecting Engineering Design Tools ..................................................................................................................... 242\n7.3.1 Program and Project Considerations ...................................................................................................... 242\n7.3.2 Policy and Processes .................................................................................................................................. 242\n7.3.3 Collaboration .............................................................................................................................................. 242\n7.3.4 Design Standards ....................................................................................................................................... 243\n7.3.5 Existing IT Architecture............................................................................................................................ 243\n7.3.6 Tool Interfaces ............................................................................................................................................ 243\n7.3.7 Interoperability and Data Formats .......................................................................................................... 243\n7.3.8 Backward Compatibility ........................................................................................................................... 244\n7.3.9 Platform ....................................................................................................................................................... 244\n7.3.10 Tool Configuration Control ...................................................................................................................... 244\n7.3.11 Security/Access Control ............................................................................................................................ 244\n7.3.12 Training ....................................................................................................................................................... 244\n7.3.13 Licenses ....................................................................................................................................................... 244\n7.3.14 Stability of Vendor and Customer Support ............................................................................................ 244\n\n7.4 Human Factors Engineering ................................................................................................................................ 246\n\n\n\nviii ? NASA Systems Engineering Handbook\n\nTable of Contents\n\n7.4.1 Basic HF Model .......................................................................................................................................... 247\n7.4.2 HF Analysis and Evaluation Techniques ................................................................................................ 247\n\n7.5 Environmental, Nuclear Safety, Planetary Protection, and Asset Protection Policy Compliance .............. 256\n7.5.1 NEPA and EO 12114 ................................................................................................................................. 256\n\n7.5.1.1 National Environmental Policy Act .......................................................................................... 256\n7.5.1.2 EO 12114 Environmental Effects Abroad of Major Federal Actions  ................................... 257\n\n7.5.2 PD/NSC-25 ................................................................................................................................................. 257\n7.5.3 Planetary Protection .................................................................................................................................. 258\n7.5.4 Space Asset Protection .............................................................................................................................. 260\n\n7.5.4.1 Protection Policy.......................................................................................................................... 260\n7.5.4.2 Goal ............................................................................................................................................... 260\n7.5.4.3 Scoping .......................................................................................................................................... 260\n7.5.4.4 Protection Planning .................................................................................................................... 260\n\n7.6 Use of Metric System  ............................................................................................................................................ 261\n\nAppendix A: Acronyms ................................................................................................................................. 263\n\nAppendix B: Glossary ................................................................................................................................... 266\n\nAppendix C: How to Write a Good Requirement ........................................................................................ 279\n\nAppendix D: Requirements Verification Matrix ......................................................................................... 282\n\nAppendix E: Creating the Validation Plan (Including Validation Requirements Matrix) ........................ 284\n\nAppendix F: Functional, Timing, and State Analysis  ................................................................................. 285\n\nAppendix G: Technology Assessment/Insertion ........................................................................................ 293\n\nAppendix H: Integration Plan Outline ........................................................................................................ 299\n\nAppendix I: Verification and Validation Plan Sample Outline .................................................................. 301\n\nAppendix J: SEMP Content Outline ............................................................................................................. 303\n\nAppendix K: Plans ........................................................................................................................................ 308\n\nAppendix L: Interface Requirements Document Outline .......................................................................... 309\n\nAppendix M: CM Plan Outline ..................................................................................................................... 311\n\nAppendix N: Guidance on Technical Peer Reviews/Inspections  .............................................................. 312\n\nAppendix O: Tradeoff Examples .................................................................................................................. 316\n\nAppendix P: SOW Review Checklist ............................................................................................................ 317\n\nAppendix Q: Project Protection Plan Outline ............................................................................................ 321\n\nReferences ...................................................................................................................................................... 323\n\nBibliography .................................................................................................................................................. 327\n\nIndex ............................................................................................................................................................... 332\n\n\n\nTable of Contents\n\nNASA Systems Engineering Handbook ? ix\n\nFigures\n2.0-1 SE in context of overall project management ...................................................................................................... 4\n2.1-1 The systems engineering engine ............................................................................................................................ 5\n2.2-1 A miniaturized conceptualization of the poster-size NASA project life-cycle process flow for \n flight and ground systems accompanying this handbook ................................................................................. 6\n2.3-1 SE engine tracking icon .......................................................................................................................................... 8\n2.3-2 Product hierarchy, tier 1: first pass through the SE engine ................................................................................ 9\n2.3-3 Product hierarchy, tier 2: external tank .............................................................................................................. 10\n2.3-4 Product hierarchy, tier 2: orbiter ......................................................................................................................... 10\n2.3-5 Product hierarchy, tier 3: avionics system .......................................................................................................... 11\n2.3-6 Product hierarchy: complete pass through system design processes side of the SE engine ........................ 11\n2.3-7 Model of typical activities during operational phase (Phase E) of a product ............................................... 14\n2.3-8 New products or upgrades reentering the SE engine ....................................................................................... 15\n2.5-1 The enveloping surface of nondominated designs ............................................................................................ 16\n2.5-2 Estimates of outcomes to be obtained from several design concepts including uncertainty ...................... 17\n3.0-1 NASA program life cycle ...................................................................................................................................... 20\n3.0-2 NASA project life cycle ......................................................................................................................................... 20\n3.10-1 Typical NASA budget cycle  .............................................................................................................................. 29\n4.0-1 Interrelationships among the system design processes .................................................................................... 31\n4.1-1 Stakeholder Expectations Definition Process .................................................................................................... 33\n4.1-2 Product flow for stakeholder expectations ........................................................................................................ 34\n4.1-3 Typical ConOps development for a science mission ........................................................................................ 36\n4.1-4 Example of an associated end-to-end operational architecture  ..................................................................... 36\n4.1-5a Example of a lunar sortie timeline developed early in the life cycle............................................................. 37\n4.1-5b Example of a lunar sortie DRM early in the life cycle .................................................................................... 37\n4.1-6 Example of a more detailed, integrated timeline later in the life cycle for a science mission ..................... 38\n4.2-1 Technical Requirements Definition Process ...................................................................................................... 40\n4.2-2 Characteristics of functional, operational, reliability, safety, and specialty requirements ........................... 43\n4.2-3 The flowdown of requirements ............................................................................................................................ 46\n4.2-4 Allocation and flowdown of science pointing requirements ........................................................................... 47\n4.3-1 Logical Decomposition Process .......................................................................................................................... 49\n4.3-2 Example of a PBS ................................................................................................................................................... 52\n4.3-3 Example of a functional flow block diagram ..................................................................................................... 53\n4.3-4 Example of an N2 diagram .................................................................................................................................. 54\n4.4-1 Design Solution Definition Process  .................................................................................................................... 55\n4.4-2 The doctrine of successive refinement ................................................................................................................. 56\n4.4-3 A quantitative objective function, dependent on life-cycle cost and all aspects of effectiveness ................ 58\n5.0-1 Product realization ................................................................................................................................................ 71\n5.1-1 Product Implementation Process ........................................................................................................................ 73\n5.2-1 Product Integration Process ................................................................................................................................ 78\n5.3-1 Product Verification Process ............................................................................................................................... 84\n5.3-2 Bottom-up realization process ............................................................................................................................ 90\n5.3-3 Example of end-to-end data flow for a scientific satellite mission ................................................................. 94\n5.4-1 Product Validation Process .................................................................................................................................. 99\n5.5-1 Product Transition Process ................................................................................................................................ 106\n6.1-1 Technical Planning Process ............................................................................................................................... 112\n6.1-2 Activity-on-arrow and precedence diagrams for network schedules........................................................... 116\n6.1-3 Gantt chart ........................................................................................................................................................... 118\n6.1-4 Relationship between a system, a PBS, and a WBS ........................................................................................ 123\n6.1-5 Examples of WBS development errors ............................................................................................................. 125\n6.2-1 Requirements Management Process ................................................................................................................. 131\n6.3-1 Interface Management Process .......................................................................................................................... 136\n\n\n\nx ? NASA Systems Engineering Handbook\n\nTable of Contents\n\n6.4-1 Technical Risk Management Process ................................................................................................................ 140\n6.4-2 Scenario-based modeling of hazards ................................................................................................................ 141\n6.4-3 Risk as a set of triplets  ........................................................................................................................................ 141\n6.4-4 Continuous risk management ........................................................................................................................... 142\n6.4-5 The interface between CRM and risk-informed decision analysis ............................................................... 143\n6.4-6 Risk analysis of decision alternatives ................................................................................................................ 144\n6.4-7 Risk matrix ........................................................................................................................................................... 145\n6.4-8 Example of a fault tree ........................................................................................................................................ 146\n6.4-9 Deliberation ......................................................................................................................................................... 147\n6.4-10 Performance monitoring and control of deviations ..................................................................................... 149\n6.4-11 Margin management method .......................................................................................................................... 150\n6.5-1 CM Process .......................................................................................................................................................... 151\n6.5-2 Five elements of configuration management .................................................................................................. 152\n6.5-3 Evolution of technical baseline .......................................................................................................................... 153\n6.5-4 Typical change control process .......................................................................................................................... 155\n6.6-1 Technical Data Management Process ............................................................................................................... 158\n6.7-1 Technical Assessment Process ........................................................................................................................... 166\n6.7-2 Planning and status reportingfeedback loop ................................................................................................... 167\n6.7-3 Cost and schedule variances .............................................................................................................................. 190\n6.7-4 Relationships of MOEs, MOPs,and TPMs ....................................................................................................... 192\n6.7-5 Use of the planned profile method for the weight TPM with rebaseline in Chandra Project .................. 194\n6.7-6 Use of the margin management method for the mass TPM in Sojourner .................................................. 194\n6.8-1 Decision Analysis Process .................................................................................................................................. 198\n6.8-2 Example of a decision matrix  ........................................................................................................................... 201\n6.8-3 Systems analysis across the life cycle ................................................................................................................ 203\n6.8-4 Simulation model analysis techniques ............................................................................................................. 204\n6.8-5 Trade study process ............................................................................................................................................. 205\n6.8-6 Influence diagrams .............................................................................................................................................. 210\n6.8-7 Decision tree ........................................................................................................................................................ 211\n6.8-8 Utility function for a \u201cvolume\u201d performance measure ................................................................................... 213\n6.8-9 Risk-informed Decision Analysis Process ....................................................................................................... 214\n6.8-10 Example of an objectives hierarchy ................................................................................................................ 215\n7.1-1 Acquisition life cycle  .......................................................................................................................................... 218\n7.1-2 Contract requirements development process .................................................................................................. 223\n7.2-1 CACE people/process/tools/facility paradigm ................................................................................................ 234\n7.4-1 Human factors interaction model ..................................................................................................................... 247\n7.4-2 HF engineering process and its links to the NASA program/project life cycle .......................................... 248\nF-1 FFBD flowdown ..................................................................................................................................................... 286\nF-2 FFBD: example 1 .................................................................................................................................................... 287\nF-3 FFBD showing additional control constructs: example 2 ................................................................................. 287\nF-4 Enhanced FFBD: example 3 .................................................................................................................................. 288\nF-5 Requirements allocation sheet.............................................................................................................................. 289\nF-6 N2 diagram for orbital equipment  ...................................................................................................................... 289\nF-7 Timing diagram example ...................................................................................................................................... 290\nF-8 Slew command status state diagram .................................................................................................................... 291\nG-1 PBS example ........................................................................................................................................................... 294\nG-2 Technology assessment process ........................................................................................................................... 295\nG-3 Architectural studies and technology development ......................................................................................... 296\nG-4 Technology readiness levels ................................................................................................................................. 296\nG-5 The TMA thought process ................................................................................................................................... 297\nG-6 TRL assessment matrix ......................................................................................................................................... 298\nN-1 The peer review/inspection process .................................................................................................................... 312\n\n\n\nTable of Contents\n\nNASA Systems Engineering Handbook ? xi\n\nN-2 Peer reviews/inspections quick reference guide ............................................................................................... 315\n\nTables\n2.3-1 Project Life-Cycle Phases ....................................................................................................................................... 7\n4.1-1 Typical Operational Phases for a NASA Mission ............................................................................................. 39\n4.2-1 Benefits of Well-Written Requirements ............................................................................................................. 42\n4.2-2 Requirements Metadata ....................................................................................................................................... 48\n4.4-1 ILS Technical Disciplines ...................................................................................................................................... 66\n6.6-1 Technical Data Tasks .......................................................................................................................................... 163\n6.7-1 Program Technical Reviews ............................................................................................................................... 170\n6.7-2 P/SRR Entrance and Success Criteria ............................................................................................................... 171\n6.7-3 P/SDR Entrance and Success Criteria .............................................................................................................. 172\n6.7-4 MCR Entrance and Success Criteria ................................................................................................................. 173\n6.7-5 SRR Entrance and Success Criteria ................................................................................................................... 174\n6.7-6 MDR Entrance and Success Criteria ................................................................................................................ 175\n6.7-7 SDR Entrance and Success Criteria .................................................................................................................. 176\n6.7-8 PDR Entrance and Success Criteria .................................................................................................................. 177\n6.7-9 CDR Entrance and Success Criteria ................................................................................................................. 178\n6.7-10 PRR Entrance and Success Criteria ................................................................................................................ 179\n6.7-11 SIR Entrance and Success Criteria .................................................................................................................. 180\n6.7-12 TRR Entrance and Success Criteria ................................................................................................................ 181\n6.7-13 SAR Entrance and Success Criteria ................................................................................................................ 182\n6.7-14 ORR Entrance and Success Criteria  .............................................................................................................. 183\n6.7-15 FRR Entrance and Success Criteria  ............................................................................................................... 184\n6.7-16 PLAR Entrance and Success Criteria  ............................................................................................................ 185\n6.7-17 CERR Entrance and Success Criteria ............................................................................................................. 186\n6.7-18 PFAR Entrance and Success Criteria .............................................................................................................. 186\n6.7-19 DR Entrance and Success Criteria .................................................................................................................. 187\n6.7-20 Functional and Physical Configuration Audits  ............................................................................................ 189\n6.7-21 Systems Engineering Process Metrics ............................................................................................................ 196\n6.8-1 Consequence Table ............................................................................................................................................. 199\n6.8-2 Typical Information to Capture in a Decision Report  .................................................................................. 202\n7.1-1 Applying the Technical Processes on Contract ............................................................................................... 220\n7.1-2 Steps in the Requirements Development Process  .......................................................................................... 224\n7.1-3 Proposal Evaluation Criteria ............................................................................................................................. 227\n7.1-4 Risks in Acquisition ............................................................................................................................................ 228\n7.1-5 Typical Work Product Documents ................................................................................................................... 230\n7.1-6 Contract-Subcontract Issues .............................................................................................................................. 231\n7.4-1 Human and Organizational Analysis Techniques  ......................................................................................... 249\n7.5-1 Planetary Protection Mission Categories ......................................................................................................... 259\n7.5-2 Summarized Planetary Protection Requirements .......................................................................................... 259\nD-1 Requirements Verification Matrix ...................................................................................................................... 283\nE-1 Validation Requirements Matrix ......................................................................................................................... 284\nG-1 Products Provided by the TA as a Function of Program/Project Phase ........................................................ 294\nH-1 Integration Plan Contents .................................................................................................................................... 300\nM-1 CM Plan Outline .................................................................................................................................................. 311\nO-1 Typical Tradeoffs for Space Systems ................................................................................................................... 316\nO-2 Typical Tradeoffs in the Acquisition Process..................................................................................................... 316\nO-3 Typical Tradeoffs Throughout the Project Life Cycle ....................................................................................... 316\n\n\n\nxii ? NASA Systems Engineering Handbook\n\nTable of Contents\n\nBoxes\nSystem Cost, Effectiveness, and Cost-Effectiveness ..................................................................................................... 16\nThe Systems Engineer\u2019s Dilemma .................................................................................................................................. 17\nProgram Formulation ...................................................................................................................................................... 21\nProgram Implementation ................................................................................................................................................ 21\nPre-Phase A: Concept Studies ........................................................................................................................................ 22\nPhase A: Concept and Technology Development ........................................................................................................ 23\nPhase B: Preliminary Design and Technology Completion ....................................................................................... 24\nPhase C: Final Design and Fabrication ......................................................................................................................... 26\nPhase D: System Assembly, Integration and Test, Launch .......................................................................................... 27\nPhase E: Operations and Sustainment ........................................................................................................................... 28\nPhase F: Closeout ............................................................................................................................................................. 28\nSystem Design Keys ......................................................................................................................................................... 32\nExample of Functional and Performance Requirements ............................................................................................ 43\nRationale ............................................................................................................................................................................ 48\nDOD Architecture Framework ...................................................................................................................................... 51\nPrototypes ......................................................................................................................................................................... 67\nProduct Realization Keys ................................................................................................................................................ 72\nDifferences Between Verification and Validation Testing ........................................................................................... 83\nTypes of Testing ................................................................................................................................................................ 85\nTypes of Verification ........................................................................................................................................................ 86\nDifferences Between Verification and Validation Testing ........................................................................................... 98\nTypes of Validation......................................................................................................................................................... 100\nExamples of Enabling Products and Support Resources for Preparing to Conduct Validation .......................... 102\nModel Verification and Validation ............................................................................................................................... 104\nCrosscutting Technical Management Keys ................................................................................................................. 111\nGantt Chart Features ...................................................................................................................................................... 117\nWBS Hierarchies for Systems ....................................................................................................................................... 126\nDefinitions ....................................................................................................................................................................... 132\nTypical Interface Management Checklist .................................................................................................................... 138\nKey Concepts in Technical Risk Management  .......................................................................................................... 139\nExample Sources of Risk ............................................................................................................................................... 145\nLimitations of Risk Matrices ......................................................................................................................................... 145\nTypes of Configuration Change Management Changes ........................................................................................... 154\nWarning Signs/Red Flags (How Do You Know When You\u2019re in Trouble?) ............................................................ 156\nRedlines Were identified as One of the Major Causes of the NOAA N-Prime Mishap ....................................... 157\nInappropriate Uses of Technical Data .......................................................................................................................... 160\nData Collection Checklist ............................................................................................................................................. 162\nTermination Review ....................................................................................................................................................... 169\nAnalyzing the Estimate at Completion........................................................................................................................ 191\nExamples of Technical Performance Measures  ......................................................................................................... 193\nAn Example of a Trade Tree for a Mars Rover ........................................................................................................... 207\nTrade Study Reports ....................................................................................................................................................... 208\nSolicitations ..................................................................................................................................................................... 219\nSource Evaluation Board ............................................................................................................................................... 226\nContext Diagrams .......................................................................................................................................................... 292\n\n\n\nNASA Systems Engineering Handbook ? xiii\n\nPreface\n\nSince the writing of NASA/SP-6105 in 1995, systems \nengineering at the National Aeronautics and Space Ad-\nministration (NASA), within national and international \nstandard bodies, and as a discipline has undergone rapid \nevolution. Changes include implementing standards \nin the International Organization for Standardization \n(ISO) 9000, the use of Carnegie Mellon Software Engi-\nneering Institute\u2019s Capability Maturity Model\u00ae Integra-\ntion (CMMI\u00ae) to improve development and delivery of \nproducts, and the impacts of mission failures. Lessons \nlearned on systems engineering were documented in re-\nports such as those by the NASA Integrated Action Team \n(NIAT), the Columbia Accident Investigation Board \n(CAIB), and the follow-on Diaz Report. Out of these \nefforts came the NASA Office of the Chief Engineer \n(OCE) initiative to improve the overall Agency systems \nengineering infrastructure and capability for the efficient \nand effective engineering of NASA systems, to produce \nquality products, and to achieve mission success. In ad-\ndition, Agency policy and requirements for systems en-\ngineering have been established. This handbook update \nis a part of the OCE-sponsored Agencywide systems en-\ngineering initiative.\n\nIn 1995, SP-6105 was initially published to bring the \nfundamental concepts and techniques of systems engi-\nneering to NASA personnel in a way that recognizes the \nnature of NASA systems and the NASA environment. \nThis revision of SP-6105 maintains that original philos-\nophy while updating the Agency\u2019s systems engineering \nbody of knowledge, providing guidance for insight into \ncurrent best Agency practices, and aligning the hand-\nbook with the new Agency systems engineering policy.\n\nThe update of this handbook was twofold: a top-down \ncompatibility with higher level Agency policy and a \n\nbottom-up infusion of guidance from the NASA prac-\ntitioners in the field. The approach provided the oppor-\ntunity to obtain best practices from across NASA and \nbridge the information to the established NASA sys-\ntems engineering process. The attempt is to commu-\nnicate principles of good practice as well as alternative \napproaches rather than specify a particular way to ac-\ncomplish a task. The result embodied in this handbook is \na top-level implementation approach on the practice of \nsystems engineering unique to NASA. The material for \nupdating this handbook was drawn from many different \nsources, including NASA procedural requirements, field \ncenter systems engineering handbooks and processes, as \nwell as non-NASA systems engineering textbooks and \nguides.\n\nThis handbook consists of six core chapters: (1) systems \nengineering fundamentals discussion, (2) the NASA \nprogram/project life cycles, (3) systems engineering pro-\ncesses to get from a concept to a design, (4) systems engi-\nneering processes to get from a design to a final product, \n(5) crosscutting management processes in systems en-\ngineering, and (6) special topics relative to systems en-\ngineering. These core chapters are supplemented by ap-\npendices that provide outlines, examples, and further \ninformation to illustrate topics in the core chapters. The \nhandbook makes extensive use of boxes and figures to \ndefine, refine, illustrate, and extend concepts in the core \nchapters without diverting the reader from the main in-\nformation.\n\nThe handbook provides top-level guidelines for good \nsystems engineering practices; it is not intended in any \nway to be a directive.\n\nNASA/SP-2007-6105 Rev1 supersedes SP-6105, dated \nJune 1995.\n\n\n\n\n\nNASA Systems Engineering Handbook ? xv\n\nAcknowledgments\n\nPrimary points of contact: Stephen J. Kapurch, Office \nof the Chief Engineer, NASA Headquarters, and Neil E. \nRainwater, Marshall Space Flight Center.\n\nThe following individuals are recognized as contributing \npractitioners to the content of this handbook revision:\n? Core Team Member (or Representative) from Center, \n\nDirectorate, or Office\n? Integration Team Member\n\u2022 Subject Matter Expert Team Champion\n? Subject Matter Expert\n\nArden Acord, NASA/Jet Propulsion Laboratory ?\nDanette Allen, NASA/Langley Research Center ?\nDeborah Amato, NASA/Goddard Space Flight Center ?\u2022\nJim Andary, NASA/Goddard Space Flight Center ??\nTim Beard, NASA/Ames Research Center ?\nJim Bilbro, NASA/Marshall Space Flight Center ?\nMike Blythe, NASA/Headquarters ?\nLinda Bromley, NASA/Johnson Space Center ?\u2022??\nDave Brown, Defense Acquisition University ?\nJohn Brunson, NASA/Marshall Space Flight Center ?\u2022\nJoe Burt, NASA/Goddard Space Flight Center ?\nGlenn Campbell, NASA/Headquarters ?\nJoyce Carpenter, NASA/Johnson Space Center ?\u2022\nKeith Chamberlin, NASA/Goddard Space Flight Center ?\nPeggy Chun, NASA/NASA Engineering and Safety \n\nCenter ?\u2022??\nCindy Coker, NASA/Marshall Space Flight Center ?\nNita Congress, Graphic Designer ?\nCatharine Conley, NASA/Headquarters ?\nShelley Delay, NASA/Marshall Space Flight Center ?\nRebecca Deschamp, NASA/Stennis Space Center ?\nHomayoon Dezfuli, NASA/Headquarters ?\u2022\nOlga Dominguez, NASA/Headquarters ?\nRajiv Doreswamy, NASA/Headquarters ?\nLarry Dyer, NASA/Johnson Space Center ?\nNelson Eng, NASA/Johnson Space Center ?\nPatricia Eng, NASA/Headquarters ?\n\nAmy Epps, NASA/Marshall Space Flight Center ?\nChester Everline, NASA/Jet Propulsion Laboratory ?\nKaren Fashimpaur, Arctic Slope Regional Corporation ??\nOrlando Figueroa, NASA/Goddard Space Flight Center ?\nStanley Fishkind, NASA/Headquarters ?\nBrad Flick, NASA/Dryden Flight Research Center ?\nMarton Forkosh, NASA/Glenn Research Center ?\nDan Freund, NASA/Johnson Space Center ?\nGreg Galbreath, NASA/Johnson Space Center ?\nLouie Galland, NASA/Langley Research Center ?\nYuri Gawdiak, NASA/Headquarters ?\u2022?\nTheresa Gibson, NASA/Glenn Research Center ?\nRonnie Gillian, NASA/Langley Research Center ?\nJulius Giriunas, NASA/Glenn Research Center ?\nEd Gollop, NASA/Marshall Space Flight Center ?\nLee Graham, NASA/Johnson Space Center ?\nLarry Green, NASA/Langley Research Center ?\nOwen Greulich, NASA/Headquarters ?\nBen Hanel, NASA/Ames Research Center ?\nGena Henderson, NASA/Kennedy Space Center \u2022?\nAmy Hemken, NASA/Marshall Space Flight Center ?\nBob Hennessy, NASA/NASA Engineering and Safety \n\nCenter ?\nEllen Herring, NASA/Goddard Space Flight Center \u2022?\nRenee Hugger, NASA/Johnson Space Center ?\nBrian Hughitt, NASA/Headquarters ?\nEric Isaac, NASA/Goddard Space Flight Center ?\nTom Jacks, NASA/Stennis Space Center ?\nKen Johnson, NASA/NASA Engineering and Safety \n\nCenter ?\nRoss Jones, NASA/Jet Propulsion Laboratory ?\nJohn Juhasz, NASA/Johnson Space Center ?\nStephen Kapurch, NASA/Headquarters ??\u2022\nJason Kastner, NASA/Jet Propulsion Laboratory ?\nKristen Kehrer, NASA/Kennedy Space Center ?\nJohn Kelly, NASA/Headquarters ?\nKriss Kennedy, NASA/Johnson Space Center ?\n\n\n\nxvi ? NASA Systems Engineering Handbook\n\nAcknowledgments\n\nSteven Kennedy, NASA/Kennedy Space Center ? \nTracey Kickbusch, NASA/Kennedy Space Center ?\nCasey Kirchner, NASA/Stennis Space Center ?\nKenneth Kumor, NASA/Headquarters ?\nJanne Lady, SAITECH/CSC ?\nJerry Lake, Systems Management international ?\nKenneth W. Ledbetter, NASA/Headquarters ?\nSteve Leete, NASA/Goddard Space Flight Center ?\nWilliam Lincoln, NASA/Jet Propulsion Laboratory ?\nDave Littman, NASA/Goddard Space Flight Center ?\nJohn Lucero, NASA/Glenn Research Center ?\nPaul Luz, NASA/Marshall Space Flight Center ?\nTodd MacLeod, NASA/Marshall Space Flight Center ?\nRoger Mathews, NASA/Kennedy Space Center ?\u2022\nBryon Maynard, NASA/Stennis Space Center ?\nPatrick McDuffee, NASA/Marshall Space Flight Center ?\nMark McElyea, NASA/Marshall Space Flight Center ?\nWilliam McGovern, Defense Acquisition University ?\nColleen McGraw, NASA/Goddard Space Flight \n\nCenter ??\u2022\nMelissa McGuire, NASA/Glenn Research Center ?\nDon Mendoza, NASA/Ames Research Center ?\nLeila Meshkat, NASA/Jet Propulsion Laboratory ?\nElizabeth Messer, NASA/Stennis Space Center ?\u2022\nChuck Miller, NASA/Headquarters ?\nScott Mimbs, NASA/Kennedy Space Center ?\nSteve Newton, NASA/Marshall Space Flight Center ?\nTri Nguyen, NASA/Johnson Space Center ?\nChuck Niles, NASA/Langley Research Center ?\u2022\nCynthia Null, NASA/NASA Engineering and Safety \n\nCenter ?\nJohn Olson, NASA/Headquarters ?\nTim Olson, QIC, Inc. ?\nSam Padgett, NASA/Johnson Space Center ?\nChristine Powell, NASA/Stennis Space Center ?\u2022??\nSteve Prahst, NASA/Glenn Research Center ?\nPete Prassinos, NASA/Headquarters ?\nMark Prill, NASA/Marshall Space Flight Center ?\nNeil Rainwater, NASA/Marshall Space Flight Center ? ?\nRon Ray, NASA/Dryden Flight Research Center ?\nGary Rawitscher, NASA/Headquarters ?\nJoshua Reinert, ISL Inc. ?\nNorman Rioux, NASA/Goddard Space Flight Center ?\n\nSteve Robbins, NASA/Marshall Space Flight Center ?\u2022\nDennis Rohn, NASA/Glenn Research Center ??\nJim Rose, NASA/Jet Propulsion Laboratory ?\nArnie Ruskin,* NASA/Jet Propulsion Laboratory ?\u2022\nHarry Ryan, NASA/Stennis Space Center ?\nGeorge Salazar, NASA/Johnson Space Center ?\nNina Scheller, NASA/Ames Research Center ?\nPat Schuler, NASA/Langley Research Center ?\u2022\nRandy Seftas, NASA/Goddard Space Flight Center ?\nJoey Shelton, NASA/Marshall Space Flight Center ?\u2022 \nRobert Shishko, NASA/Jet Propulsion Laboratory ??\nBurton Sigal, NASA/Jet Propulsion Laboratory ?\nSandra Smalley, NASA/Headquarters ?\nRichard Smith, NASA/Kennedy Space Center ?\nJohn Snoderly, Defense Acquisition University ?\nRichard Sorge, NASA/Glenn Research Center ?\nMichael Stamatelatos, NASA/Headquarters ? \nTom Sutliff, NASA/Glenn Research Center ?\u2022\nTodd Tofil, NASA/Glenn Research Center ?\nJohn Tinsley, NASA/Headquarters ?\nRob Traister, Graphic Designer ?\nClayton Turner, NASA/Langley Research Center ?\nPaul VanDamme, NASA/Jet Propulsion Laboratory ?\nKaren Vaner, NASA/Stennis Space Center ?\nLynn Vernon, NASA/Johnson Space Center ? \nLinda Voss, Technical Writer ?\nBritt Walters, NASA/Johnson Space Center ?\nTommy Watts, NASA/Marshall Space Flight Center ?\nRichard Weinstein, NASA/Headquarters ?\nKatie Weiss, NASA/Jet Propulsion Laboratory ?\u2022\nMartha Wetherholt, NASA/Headquarters ?\nBecky Wheeler, NASA/Jet Propulsion Laboratory ?\nCathy White, NASA/Marshall Space Flight Center ?\nReed Wilcox, NASA/Jet Propulsion Laboratory ?\nBarbara Woolford, NASA/Johnson Space Center ?\u2022\nFelicia Wright, NASA/Langley Research Center ?\nRobert Youngblood, ISL Inc. ?\nTom Zang, NASA/Langley Research Center ?\n\n*In memory of.\n\n\n\nNASA Systems Engineering Handbook ? 1\n\n1.1 Purpose\nThis handbook is intended to provide general guidance \nand information on systems engineering that will be \nuseful to the NASA community. It provides a generic de-\nscription of Systems Engineering (SE) as it should be ap-\nplied throughout NASA. A goal of the handbook is to in-\ncrease awareness and consistency across the Agency and \nadvance the practice of SE. This handbook provides per-\nspectives relevant to NASA and data particular to NASA.\n\nThis handbook should be used as a companion for im-\nplementing NPR 7123.1, Systems Engineering Processes \nand Requirements, as well as the Center-specific hand-\nbooks and directives developed for implementing sys-\ntems engineering at NASA. It provides a companion ref-\nerence book for the various systems engineering related \ncourses being offered under NASA\u2019s auspices.\n\n1.2 Scope and Depth\nThe coverage in this handbook is limited to general \nconcepts and generic descriptions of processes, tools, \nand techniques. It provides information on systems en-\ngineering best practices and pitfalls to avoid. There are \nmany Center-specific handbooks and directives as well as \ntextbooks that can be consulted for in-depth tutorials.\n\nThis handbook describes systems engineering as it should \nbe applied to the development and implementation of \n\nlarge and small NASA programs and projects. NASA \nhas defined different life cycles that specifically address \nthe major project categories, or product lines, which \nare: Flight Systems and Ground Support (FS&GS), Re-\nsearch and Technology (R&T), Construction of Facili-\nties (CoF), and Environmental Compliance and Resto-\nration (ECR). The technical content of the handbook \nprovides systems engineering best practices that should \nbe incorporated into all NASA product lines. (Check \nthe NASA On-Line Directives Information System \n(NODIS) electronic document library for applicable \nNASA directives on topics such as product lines.) For \nsimplicity this handbook uses the FS&GS product line \nas an example. The specifics of FS&GS can be seen in \nthe description of the life cycle and the details of the \nmilestone reviews. Each product line will vary in these \ntwo areas; therefore, the reader should refer to the ap-\nplicable NASA procedural requirements for the specific \nrequirements for their life cycle and reviews. The en-\ngineering of NASA systems requires a systematic and \ndisciplined set of processes that are applied recursively \nand iteratively for the design, development, operation, \nmaintenance, and closeout of systems throughout the \nlife cycle of the programs and projects.\n\nThe handbook\u2019s scope properly includes systems engi-\nneering functions regardless of whether they are per-\nformed by a manager or an engineer, in-house, or by a \ncontractor.\n\n1.0 Introduction\n\n\n\n\n\nNASA Systems Engineering Handbook ? 3\n\nSystems engineering is a methodical, disciplined ap-\nproach for the design, realization, technical manage-\nment, operations, and retirement of a system. A \u201csystem\u201d \nis a construct or collection of different elements that to-\ngether produce results not obtainable by the elements \nalone. The elements, or parts, can include people, hard-\nware, software, facilities, policies, and documents; that is, \nall things required to produce system-level results. The \nresults include system-level qualities, properties, char-\nacteristics, functions, behavior, and performance. The \nvalue added by the system as a whole, beyond that con-\ntributed independently by the parts, is primarily created \nby the relationship among the parts; that is, how they are \ninterconnected.1 It is a way of looking at the \u201cbig picture\u201d \nwhen making technical decisions. It is a way of achieving \nstakeholder functional, physical, and operational perfor-\nmance requirements in the intended use environment \nover the planned life of the systems. In other words, sys-\ntems engineering is a logical way of thinking.\n\nSystems engineering is the art and science of devel-\noping an operable system capable of meeting require-\nments within often opposed constraints. Systems engi-\nneering is a holistic, integrative discipline, wherein the \ncontributions of structural engineers, electrical engi-\nneers, mechanism designers, power engineers, human \nfactors engineers, and many more disciplines are evalu-\nated and balanced, one against another, to produce a co-\nherent whole that is not dominated by the perspective of \na single discipline.2 \n\nSystems engineering seeks a safe and balanced design in \nthe face of opposing interests and multiple, sometimes \nconflicting constraints. The systems engineer must de-\nvelop the skill and instinct for identifying and focusing \nefforts on assessments to optimize the overall design \n\n1Rechtin, Systems Architecting of Organizations: Why Eagles \nCan\u2019t Swim.\n\n2Comments on systems engineering throughout Chapter 2.0 \nare extracted from the speech \u201cSystem Engineering and the \nTwo Cultures of Engineering\u201d by Michael D. Griffin, NASA \nAdministrator. \n\nand not favor one system/subsystem at the expense \nof another. The art is in knowing when and where to \nprobe. Personnel with these skills are usually tagged as \n\u201csystems engineers.\u201d They may have other titles\u2014lead \nsystems engineer, technical manager, chief engineer\u2014\nbut for this document, we will use the term systems en-\ngineer.\n\nThe exact role and responsibility of the systems engi-\nneer may change from project to project depending on \nthe size and complexity of the project and from phase \nto phase of the life cycle. For large projects, there may \nbe one or more systems engineers. For small projects, \nsometimes the project manager may perform these \npractices. But, whoever assumes those responsibilities, \nthe systems engineering functions must be performed. \nThe actual assignment of the roles and responsibilities \nof the named systems engineer may also therefore vary. \nThe lead systems engineer ensures that the system tech-\nnically fulfills the defined needs and requirements and \nthat a proper systems engineering approach is being fol-\nlowed. The systems engineer oversees the project\u2019s sys-\ntems engineering activities as performed by the tech-\nnical team and directs, communicates, monitors, and \ncoordinates tasks. The systems engineer reviews and \nevaluates the technical aspects of the project to ensure \nthat the systems/subsystems engineering processes are \nfunctioning properly and evolves the system from con-\ncept to product. The entire technical team is involved in \nthe systems engineering process.\n\nThe systems engineer will usually play the key role in \nleading the development of the system architecture, de-\nfining and allocating requirements, evaluating design \ntradeoffs, balancing technical risk between systems, de-\nfining and assessing interfaces, providing oversight of \nverification and validation activities, as well as many \nother tasks. The systems engineer will usually have the \nprime responsibility in developing many of the project \ndocuments, including the Systems Engineering Manage-\nment Plan (SEMP), requirements/specification docu-\nments, verification and validation documents, certifica-\ntion packages, and other technical documentation. \n\n2.0 Fundamentals of Systems Engineering\n\n\n\n4 ? NASA Systems Engineering Handbook\n\n2.0 Fundamentals of Systems Engineering\n\nIn summary, the systems engineer is skilled in the art \nand science of balancing organizational and technical in-\nteractions in complex systems. However, since the entire \nteam is involved in the systems engineering approach, \nin some ways everyone is a systems engineer. Systems \nengineering is about tradeoffs and compromises, about \ngeneralists rather than specialists. Systems engineering is \nabout looking at the \u201cbig picture\u201d and not only ensuring \nthat they get the design right (meet requirements) but \nthat they get the right design.\n\nTo explore this further, put SE in the context of project \nmanagement. As discussed in NPR 7120.5, NASA Space \nFlight Program and Project Management Requirements, \nproject management is the function of planning, over-\nseeing, and directing the numerous activities required \nto achieve the requirements, goals, and objectives of the \ncustomer and other stakeholders within specified cost, \nquality, and schedule constraints. Project management \ncan be thought of as having two major areas of emphasis, \nboth of equal weight and importance. These areas are \nsystems engineering and project control. Figure 2.0-1 is \na notional graphic depicting this concept. Note that there \nare areas where the two cornerstones of project manage-\nment overlap. In these areas, SE provides the technical \naspects or inputs; whereas project control provides the \nprogrammatic, cost, and schedule inputs.\n\nThis document will focus on the SE side of the diagram. \nThese practices/processes are taken from NPR 7123.1, \n\nNASA Systems Engineering Processes and Requirements. \nEach will be described in much greater detail in sub-\nsequent chapters of this document, but an overview is \ngiven below.\n\n2.1 The Common Technical Processes \nand the SE Engine\n\nThere are three sets of common technical processes in \nNPR 7123.1, NASA Systems Engineering Processes and \nRequirements: system design, product realization, and \ntechnical management. The processes in each set and \ntheir interactions and flows are illustrated by the NPR \nsystems engineering \u201cengine\u201d shown in Figure 2.1-1. The \nprocesses of the SE engine are used to develop and realize \nthe end products. This chapter provides the application \ncontext of the 17 common technical processes required \nin NPR 7123.1. The system design processes, the prod-\nuct realization processes, and the technical management \nprocesses are discussed in more details in Chapters 4.0, \n5.0, and 6.0, respectively. Steps 1 through 9 indicated in \nFigure 2.1-1 represent the tasks in execution of a project. \nSteps 10 through 17 are crosscutting tools for carrying \nout the processes.\n\nSystem Design Processes: ?  The four system design \nprocesses shown in Figure 2.1-1 are used to define \nand baseline stakeholder expectations, generate \nand baseline technical requirements, and convert \nthe technical requirements into a design solution \n\nthat will satisfy the base-\nlined stakeholder expecta-\ntions. These processes are \napplied to each product of \nthe system structure from \nthe top of the structure to \nthe bottom until the lowest \nproducts in any system \nstructure branch are de-\nfined to the point where \nthey can be built, bought, \nor reused. All other prod-\nucts in the system structure \nare realized by integration. \nDesigners not only develop \nthe design solutions to the \nproducts intended to per-\nform the operational func-\ntions of the system, but also \n\nPROJECT CONTROLSYSTEMS ENGINEERING\n\n? System Design\n \u2013 Requirements De?nition\n \u2013 Technical Solution De?nition\n? Product Realization\n \u2013 Design Realization\n \u2013 Evaluation\n \u2013 Product Transition\n? Technical Management\n \u2013 Technical Planning\n \u2013 Technical Control\n   \u2013 Technical Assessment\n   \u2013 Technical Decision Analysis\n\n? Management Planning\n? Integrated Assessment\n? Schedule Management\n? Con?guration Management\n? Resource Management \n? Documentation and Data \n\nManagement\n? Acquisition Management \n\n? Planning\n? Risk Management\n? Con?guration \n\nManagement\n? Data Management \n? Assessment\n? Decision Analysis \n\nFigure 2.0?1 SE in context of overall project management\n\n\n\n2.1 The Common Technical Processes and the SE Engine\n\nNASA Systems Engineering Handbook ? 5\n\nestablish requirements for the products and services \nthat enable each operational/mission product in the \nsystem structure.\nProduct Realization Processes: ?  The product realiza-\ntion processes are applied to each operational/mis-\nsion product in the system structure starting from \nthe lowest level product and working up to higher \nlevel integrated products. These processes are used to \ncreate the design solution for each product (e.g., by \nthe Product Implementation or Product Integration \nProcess) and to verify, validate, and transition up to \nthe next hierarchical level products that satisfy their \ndesign solutions and meet stakeholder expectations as \na function of the applicable life-cycle phase.\nTechnical Management Processes: ?  The technical \nmanagement processes are used to establish and \nevolve technical plans for the project, to manage \ncommunication across interfaces, to assess progress \nagainst the plans and requirements for the system \nproducts or services, to control technical execution of \n\nthe project through to completion, and to aid in the \ndecisionmaking process.\n\nThe processes within the SE engine are used both itera-\ntively and recursively. As defined in NPR 7123.1, \u201citera-\ntive\u201d is the \u201capplication of a process to the same product \nor set of products to correct a discovered discrepancy \nor other variation from requirements,\u201d whereas \u201crecur-\nsive\u201d is defined as adding value to the system \u201cby the \nrepeated application of processes to design next lower \nlayer system products or to realize next upper layer end \nproducts within the system structure. This also applies to \nrepeating application of the same processes to the system \nstructure in the next life-cycle phase to mature the \nsystem definition and satisfy phase success criteria.\u201d The \nexample used in Section 2.3 will further explain these \nconcepts. The technical processes are applied recursively \nand iteratively to break down the initializing concepts of \nthe system to a level of detail concrete enough that the \ntechnical team can implement a product from the infor-\nmation. Then the processes are applied recursively and \n\nFigure 2.1?1 The systems engineering engine\n\nRequirements flow down\nfrom level above\n\nRequirements flow down\nto level below\n\nRealized products\nfrom level below \n\nPRODUCT\nREALIZATION\nPROCESSES\n\nTECHNICAL MANAGEMENT\nPROCESSES\n\nTechnical Decision Analysis\nProcess\n\n17. Decision Analysis\n\nTechnical Planning\nProcess\n\n10. Technical Planning\n\nTechnical Control\nProcesses\n\n11. Requirements Management\n12. Interface Management\n\n15. Technical Data Management\n14. Configuration Management\n13. Technical Risk Management\n\nTechnical Assessment\nProcess\n\n16. Technical Assessment\n\nRealized products\nto level above \n\nSystem design processes\napplied to each work breakdown \n\nstructure model down and \nacross system structure  \n\nProduct realization processes\napplied to each product\n\nup and across\nsystem structure\n\nSYSTEM\nDESIGN\n\nPROCESSES\n\nRequirements Definition\nProcesses\n\n1. Stakeholder Expectations \nDefinition\n\n2. Technical Requirements \nDefinition\n\nTechnical Solution\nDefinition Processes \n\n3. Logical Decomposition\n4. Design Solution Definition\n\nProduct Transition Process\n9. Product Transition\n\nEvaluation Processes\n7. Product Verification\n8. Product Validation\n\nDesign Realization\nProcesses\n\n5. Product Implementation\n6. Product Integration\n\n\n\n6 ? NASA Systems Engineering Handbook\n\n2.0 Fundamentals of Systems Engineering\n\niteratively to integrate the smallest product into greater \nand larger systems until the whole of the system has been \nassembled, verified, validated, and transitioned.\n\n2.2 An Overview of the SE Engine by \nProject Phase\n\nFigure 2.2-1 conceptually illustrates how the SE engine \nis used during each of the seven phases of a project. \nFigure 2.2-1 is a conceptual diagram. For all of the de-\ntails, refer to the poster version of this figure, which ac-\ncompanies this handbook.\n\nThe uppermost horizontal portion of this chart is used as \na reference to project system maturity, as the project pro-\ngresses from a feasible concept to an as-deployed system; \nphase activities; Key Decision Points (KDPs); and major \nproject reviews.\n\nThe next major horizontal band shows the technical de-\nvelopment processes (steps 1 through 9) in each project \nphase. The systems engineering engine cycles five times \n\nfrom Pre-Phase A through Phase D. Please note that \nNASA\u2019s management has structured Phases C and D \nto \u201csplit\u201d the technical development processes in half in \nPhases C and D to ensure closer management control. The \nengine is bound by a dashed line in Phases C and D.\n\nOnce a project enters into its operational state (Phase E) \nand closes with a closeout phase (Phase F), the technical \nwork shifts to activities commensurate with these last \ntwo project phases.\n\nThe next major horizontal band shows the eight tech-\nnical management processes (steps 10 through 17) in \neach project phase. The SE engine cycles the technical \nmanagement processes seven times from Pre-Phase A \nthrough Phase F. \n\nEach of the engine entries is given a 6105 paragraph label \nthat is keyed to Chapters 4.0, 5.0, and 6.0 in this hand-\nbook. For example, in the technical development pro-\ncesses, \u201cGet Stakeholder Expectations\u201d discussions and \ndetails are in Section 4.1. \n\nPre-Phase A:\nConcept Studies \n\nImplementationApproval\n\nTe\nch\n\nni\nca\n\nl D\nev\n\nel\nop\n\nm\nen\n\nt\nTe\n\nch\nni\n\nca\nl M\n\nan\nag\n\nem\nen\n\nt\n\n5.2\n\nKey Decision Points:\n\nMajor Reviews:\n\nFeasible Concept Top-Level Architecture Functional Baseline AllocatedBaseline As-Deployed  Baseline\n\n6.1\n\n6.8\n\n6.1 \n\n6.8 \n\n6.1 \n\n6.8 \n\n6.1 \n\n6.8 \n\n6.1 \n\n6.8 \n\n6.1 \n\n6.8 \n\n6.1 \n\n6.8 \n\n?\n\n?\n\n?\n\n5.3\n\n5.4\n\n5.5\n\n5.1\n\n4.2\n\n4.1\n\n4.3\n\n4.4\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n4.2\n\n4.1\n\n4.3\n\n4.4\n\n5.1\n\n5.3\n\n5.4\n\n5.5\n\n5.2\n\nProduct\nBaseline\n\nPhase F:\nCloseout \n\nPhase E:\nOperations &\nSustainment \n\nPhase D:\nSystem Assembly, \n\nIntegration & Test, Launch\n\nPhase B:\nPreliminary Design &\n\nTechnology Completion\n\nPhase A:\nConcept & Technology\n\nDevelopment \n\nPhase C:\nFinal Design &\n\nFabrication\n\n6.2\n\n6.7\n\n6.6\n\n6.5\n\n6.4\n\n6.3\n\nFormulation\n\nFigure 2.2?1  A miniaturized conceptualization of the poster?size NASA project life?cycle process flow for \nflight and ground systems accompanying this handbook\n\n\n\n2.3 Example of Using the SE Engine\n\nNASA Systems Engineering Handbook ? 7\n\n2.3 Example of Using the SE Engine\nTo help in understanding how the SE engine is applied, \nan example will be posed and walked through the pro-\ncesses. Pertinent to this discussion are the phases of the \nprogram and project life cycles, which will be discussed \nin greater depth in Chapter 3.0 of this document. As de-\nscribed in Chapter 3.0, NPR 7120.5 defines the life cycle \nused for NASA programs and projects. The life-cycle \nphases are described in Table 2.3-1.\n\nUse of the different phases of a life cycle allows the var-\nious products of a project to be gradually developed and \nmatured from initial concepts through the fielding of the \nproduct and to its final retirement. The SE engine shown \nin Figure 2.1-1 is used throughout all phases. \n\nIn Pre-Phase A, the SE engine is used to develop the \ninitial concepts; develop a preliminary/draft set of key \nhigh-level requirements; realize these concepts through \nmodeling, mockups, simulation, or other means; and \nverify and validate that these concepts and products \nwould be able to meet the key high-level requirements. \nNote that this is not the formal verification and valida-\ntion program that will be performed on the final product \nbut is a methodical runthrough ensuring that the con-\ncepts that are being developed in this Pre-Phase A would \nbe able to meet the likely requirements and expectations \nof the stakeholders. Concepts would be developed to the \nlowest level necessary to ensure that the concepts are fea-\nsible and to a level that will reduce the risk low enough to \nsatisfy the project. Academically, this process could pro-\nceed down to the circuit board level for every system. \n\nTable 2.3?1 Project Life?Cycle Phases\n\nPhase Purpose Typical Output\n\nFo\nrm\n\nul\nat\n\nio\nn\n\nPre-Phase A \nConcept \nStudies\n\nTo produce a broad spectrum of ideas and alternatives for missions \nfrom which new programs/projects can be selected. Determine feasi-\nbility of desired system, develop mission concepts, draft system-level \nrequirements, identify potential technology needs.\n\nFeasible system concepts \nin the form of simulations, \nanalysis, study reports, \nmodels, and mockups\n\nPhase A \nConcept and \nTechnology \nDevelopment\n\nTo determine the feasibility and desirability of a suggested new major \nsystem and establish an initial baseline compatibility with NASA\u2019s stra-\ntegic plans. Develop final mission concept, system-level requirements, \nand needed system structure technology developments.\n\nSystem concept definition \nin the form of simulations, \nanalysis, engineering \nmodels, and mockups and \ntrade study definition\n\nPhase B \nPreliminary \nDesign and \nTechnology \nCompletion\n\nTo define the project in enough detail to establish an initial baseline \ncapable of meeting mission needs. Develop system structure end \nproduct (and enabling product) requirements and generate a prelimi-\nnary design for each system structure end product.\n\nEnd products in the form \nof mockups, trade study \nresults, specification and \ninterface documents, and \nprototypes\n\nIm\npl\n\nem\nen\n\nta\ntio\n\nn\n\nPhase C \nFinal Design \nand Fabrication\n\nTo complete the detailed design of the system (and its associated \nsubsystems, including its operations systems), fabricate hardware, and \ncode software. Generate final designs for each system structure end \nproduct.\n\nEnd product detailed \ndesigns, end product \ncomponent fabrication, \nand software development\n\nPhase D \nSystem \nAssembly, \nIntegration and \nTest, Launch\n\nTo assemble and integrate the products to create the system, mean-\nwhile developing confidence that it will be able to meet the system \nrequirements. Launch and prepare for operations. Perform system \nend product implementation, assembly, integration and test, and \ntransition to use.\n\nOperations-ready system \nend product with sup-\nporting related enabling \nproducts\n\nPhase E \nOperations and \nSustainment\n\nTo conduct the mission and meet the initially identified need and \nmaintain support for that need. Implement the mission operations \nplan.\n\nDesired system\n\nPhase F \nCloseout\n\nTo implement the systems decommissioning/disposal plan developed \nin Phase E and perform analyses of the returned data and any \nreturned samples.\n\nProduct closeout \n\n\n\n8 ? NASA Systems Engineering Handbook\n\n2.0 Fundamentals of Systems Engineering\n\nHowever, that would involve a great deal of time and \nmoney. There may be a higher level or tier of product \nthan circuit board level that would enable designers to \naccurately determine the feasibility of accomplishing the \nproject (purpose of Pre-Phase A).\n\nDuring Phase A, the recursive use of the SE engine is \ncontinued, this time taking the concepts and draft key \nrequirements that were developed and validated during \nPre-Phase A and fleshing them out to become the set of \nbaseline system requirements and Concept of Opera-\ntions (ConOps). During this phase, key areas of high risk \nmight be simulated or prototyped to ensure that the con-\ncepts and requirements being developed are good ones \nand to identify verification and validation tools and tech-\nniques that will be needed in later phases.\n\nDuring Phase B, the SE engine is applied recursively to \nfurther mature requirements for all products in the de-\nveloping product tree, develop ConOps preliminary de-\nsigns, and perform feasibility analysis of the verification \nand validation concepts to ensure the designs will likely \nbe able to meet their requirements.\n\nPhase C again uses the left side of the SE engine to fi-\nnalize all requirement updates, finalize ConOps, develop \nthe final designs to the lowest level of the product tree, \nand begin fabrication. Phase D uses the right side of the \nSE engine to recursively perform the final implementa-\ntion, integration, verification, and validation of the end \nproduct, and at the final pass, transition the end product \nto the user. The technical management processes of the \nSE engine are used in Phases E and F to monitor perfor-\nmance; control configuration; and make decisions asso-\nciated with the operations, sustaining engineering, and \ncloseout of the system. Any new capabilities or upgrades \nof the existing system would reenter the SE engine as \nnew developments.\n\n2.3.1 Detailed Example\nSince it is already well known, the NASA Space Trans-\nportation System (STS) will be used as an example to \nlook at how the SE engine would be used in Phase A. \nThis example will be simplified to illustrate the applica-\ntion of the SE processes in the engine, but will in no way \nbe as detailed as necessary to actually build the highly \ncomplex vehicle. The SE engine is used recursively to \ndrive out more and more detail with each pass. The icon \nshown in Figure 2.3-1 will be used to keep track of the ap-\nplicable place in the SE engine. The numbers in the icon \n\ncorrespond to the num-\nbered processes within \nthe SE engine as shown \nin Figure 2.1-1. The var-\nious layers of the product \nhierarchy will be called \n\u201ctiers.\u201d Tiers are also \ncalled \u201clayers,\u201d or \u201clevels.\u201d \nBut basically, the higher \nthe number of the tier or level, the lower in the product \nhierarchy the product is going and the more detailed the \nproduct is becoming (e.g., going from boxes, to circuit \nboards, to components).\n\n2.3.2 Example Premise\nNASA decides that there is a need for a transportation \nsystem that will act like a \u201ctruck\u201d to carry large pieces \nof equipment and crew into Low Earth Orbit (LEO). \nReferring back to the project life cycle, the project first \nenters the Pre-Phase A. During this phase, several con-\ncept studies are performed, and it is determined that it \nis feasible to develop such a \u201cspace truck.\u201d This is deter-\nmined through combinations of simulations, mockups, \nanalyses, or other like means. For simplicity, assume fea-\nsibility will be proven through concept models. The pro-\ncesses and framework of the SE engine will be used to \ndesign and implement these models. The project would \nthen enter the Phase A activities to take the Pre-Phase A \nconcepts and refine them and define the system require-\nments for the end product. The detailed example will \nbegin in Phase A and show how the SE engine is used. \nAs described in the overview, a similar process is used \nfor the other project phases.\n\n2.3.2.1 Example Phase A System Design Passes\n\nFirst Pass\nTaking the preliminary concepts and drafting key system \nrequirements developed during the Pre-Phase A activi-\n\nties, the SE engine is en-\ntered at the first process \nand used to determine \nwho the product (i.e., \nthe STS) stakeholders \nare and what they want. \n\nDuring Pre-Phase A these needs and expectations were \npretty general ideas, probably just saying the Agency \nneeds a \u201cspace truck\u201d that will carry X tons of payload \ninto LEO, accommodate a payload of so-and-so size, \n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n16\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\nFigure 2.3?1 SE engine \ntracking icon\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n\n\n2.3 Example of Using the SE Engine\n\nNASA Systems Engineering Handbook ? 9\n\ncarry a crew of seven, etc. During this Phase A pass, \nthese general concepts are detailed out and agreed to. \nThe ConOps (sometimes referred to as operational con-\ncept) generated in Pre-Phase A is also detailed out and \nagreed to to ensure all stakeholders are in agreement as \nto what is really expected of the product\u2014in this case \nthe transportation system. The detailed expectations are \nthen converted into good requirement statements. (For \nmore information on what constitutes a good require-\nment, see Appendix C.) Subsequent passes and subse-\nquent phases will refine these requirements into specifi-\ncations that can actually be built. Also note that all of the \ntechnical management processes (SE engine processes \nnumbered 10 through 17) are also used during this and \nall subsequent passes and activities. These ensure that all \nthe proper planning, control, assessment, and decisions \nare used and maintained. Although for simplification \nthey will not be mentioned in the rest of this example, \nthey will always be in effect.\n\nNext, using the require-\nments and the ConOps \npreviously developed, \nlogical decomposition \nmodels/diagrams are \nbuilt up to help bring the \n\nrequirements into perspective and to show their relation-\nship. Finally, these diagrams, requirements, and ConOps \ndocuments are used to develop one or more feasible de-\nsign solutions. Note that at this point, since this is only the \nfirst pass through the SE engine, these design solutions \nare not detailed enough to actually build anything. Con-\nsequently, the design solutions might be summarized as, \n\u201cTo accomplish this transportation system, the best op-\ntion in our trade studies is a three-part system: a reus-\nable orbiter for the crew and cargo, a large external tank \nto hold the propellants, and two solid rocket boosters to \ngive extra power for liftoff that can be recovered, refur-\nbished, and reused.\u201d (Of course, the actual design solu-\ntion would be much more descriptive and detailed). So, \nfor this first pass, the first tier of the product hierarchy \nmight look like Figure 2.3-2. There would also be other \nenabling products that might appear in the product tree, \nbut for simplicity only, the main products are shown in \nthis example.\n\nNow, obviously design solution is not yet at a detailed \nenough level to actually build the prototypes or models \nof any of these products. The requirements, ConOps, \nfunctional diagrams, and design solutions are still at a \n\npretty high, general level. Note that the SE processes on \nthe right side (i.e., the product realization processes) of \nthe SE engine have yet to be addressed. The design must \nfirst be at a level that something can actually be built, \ncoded, or reused before that side of the SE engine can be \nused. So, a second pass of the left side of the SE engine \nwill be started.\n\nSecond Pass\nThe SE engine is completely recursive. That is, each of \nthe three elements shown in the tier 1 diagram can now \n\nbe considered a product \nof its own and the SE en-\ngine is therefore applied \nto each of the three ele-\nments separately. For ex-\nample, the external tank \n\nis considered an end product and the SE engine resets \nback to the first processes. So now, just focusing on the \nexternal tank, who are the stakeholders and what they \nexpect of the external tank is determined. Of course, one \nof the main stakeholders will be the owners of the tier 1 \nrequirements and the STS as an end product, but there \nwill also be other new stakeholders. A new ConOps for \nhow the external tank would operate is generated. The \ntier 1 requirements that are applicable (allocated) to the \nexternal tank would be \u201cflowed down\u201d and validated. \nUsually, some of these will be too general to implement \ninto a design, so the requirements will have to be de-\ntailed out. To these derived requirements, there will also \nbe added new requirements that are generated from the \nstakeholder expectations, and other applicable standards \nfor workmanship, safety, quality, etc.\n\nNext, the external tank requirements and the external \ntank ConOps are established, and functional diagrams \nare developed as was done in the first pass with the STS \nproduct. Finally, these diagrams, requirements, and \nConOps documents are used to develop some feasible \ndesign solutions for the external tank. At this pass, there \n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\nExternal\nTank\n\nOrbiterTier 1\n\nTier 0\n\nSolid\nRocket Booster\n\nSpace\nTransportation System \n\nFigure 2.3?2 Product hierarchy, tier 1: first pass \nthrough the SE engine\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n\n\n10 ? NASA Systems Engineering Handbook\n\n2.0 Fundamentals of Systems Engineering\n\nwill also not be enough \ndetail to actually build \nor prototype the external \ntank. The design solution \nmight be summarized as, \n\u201cTo build this external \n\ntank, since our trade studies showed the best option was \nto use cryogenic propellants, a tank for the liquid hy-\ndrogen will be needed as will another tank for the liquid \noxygen, instrumentation, and an outer structure of alu-\nminum coated with foam.\u201d Thus, the tier 2 product tree \nfor the external tank might look like Figure 2.3-3.\n\nIn a similar manner, the \norbiter would also take \nanother pass through the \nSE engine starting with \nidentifying the stake-\nholders and their expec-\n\ntations, and generating a ConOps for the orbiter element. \nThe tier 1 requirements that are applicable (allocated) to \nthe orbiter would be \u201cflowed down\u201d and validated; new \nrequirements derived from them and any additional \nrequirements (including interfaces with the other ele-\nments) would be added.\n\nNext, the orbiter require-\nments and the ConOps \nare taken, functional di-\nagrams are developed, \nand one or more feasible \ndesign solutions for the \n\norbiter are generated. As with the external tank, at this \npass, there will not be enough detail to actually build or \ndo a complex model of the orbiter. The orbiter design \n\nsolution might be summarized as, \u201cTo build this orbiter \nwill require a winged vehicle with a thermal protection \nsystem; an avionics system; a guidance, navigation, and \ncontrol system; a propulsion system; an environmental \ncontrol system; etc.\u201d So the tier 2 product tree for the \norbiter element might look like Figure 2.3-4.\n\nLikewise, the solid rocket booster would also be consid-\nered an end product, and a pass through the SE engine \nwould generate a tier 2 design concept, just as was done \nwith the external tank and the orbiter.\n\nThird Pass\nEach of the tier 2 elements is also considered an end \nproduct, and each undergoes another pass through \n\nthe SE engine, defining \nstakeholders, generating \nConOps, flowing down \nallocated requirements, \ngenerating new and de-\nrived requirements, and \n\ndeveloping functional diagrams and design solution \nconcepts. As an example of just the avionics system el-\nement, the tier 3 product hierarchy tree might look like \nFigure 2.3-5.\n\nPasses 4 Through n\nFor this Phase A set of passes, this recursive process is \ncontinued for each product (model) on each tier down \nto the lowest level in the product tree. Note that in some \nprojects it may not be feasible, given an estimated project \ncost and schedule, to perform this recursive process com-\npletely down to the smallest component during Phase A. \n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\nFigure 2.3?3 Product hierarchy, tier 2:  \nexternal tank\n\nExternal\nTank\n\nOrbiter Solid\nRocket\nBooster\n\nSpace\nTransportation\n\nSystem \n\nTier 1\n\nTier 0\n\nTier 2 Hydrogen\nTank\n\nInstru-\nmentation\n\nExternal\nStructure\n\nOxygen\nTank\n\nExternal\nTank\n\nOrbiter Solid\nRocket\nBooster\n\nSpace\nTransportation\n\nSystem \n\nTier 1\n\nTier 0\n\nTier 2\n\nExternal\nStructure\n\nEnviron-\nmental\nControl\nSystem\n\nAvionics\nSystem\n\nThermal\nProtection\n\nSystem\n\nEtc.\n\nFigure 2.3?4 Product hierarchy, tier 2: orbiter\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n\n\n2.3 Example of Using the SE Engine\n\nNASA Systems Engineering Handbook ? 11\n\nIn these cases, engi-\nneering judgment must \nbe used to determine \nwhat level of the product \ntier is feasible. Note that \nthe lowest feasible level \n\nmay occur at different tiers depending on the product-\nline complexity. For example, for one product line it may \noccur at tier 2; whereas, for a more complex product, it \ncould occur at tier 8. This also means that it will take dif-\n\nferent amounts of time to reach the bottom. Thus, for \nany given program or project, products will be at var-\nious stages of development. For this Phase A example, \nFigure 2.3-6 depicts the STS product hierarchy after com-\npletely passing through the system design processes side \nof the SE engine. At the end of this set of passes, system \nrequirements, ConOps, and high-level conceptual func-\ntional and physical architectures for each product in the \ntree would exist. Note that these would not yet be the \ndetailed or even preliminary designs for the end prod-\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\nFigure 2.3?5 Product hierarchy, tier 3: avionics system\n\nExternal\nTank\n\nOrbiter Solid\nRocket Booster\n\nSpace\nTransportation System \n\nTier 1\n\nTier 0\n\nTier 2\nExternal\n\nStructure\nEnvironmental\nControl System\n\nAvionics\nSystem\n\nThermal\nProtection\n\nSystem\n\nEtc.\n\nTier 3\nCommunication\n\nSystem\nDisplays &\nControls\n\nCommand & Data\nHandling System\n\nInstrumentation\nSystem\n\nEtc.\n\nFigure 2.3?6 Product hierarchy: complete pass through system design processes side of the SE engine\nNote: The unshaded boxes represent bottom-level phase products.\n\nExternal\nTank\n\nOrbiter Solid\nRocket Booster\n\nSpace\nTransportation System \n\nTier 1\n\nTier 0\n\nTier 2\n\nTier 3\n\nTier 4\n\nTier 5\n\nTier 6\n\nBAA nCB A nCB\n\nAbAa BbBaAbAa BbBa CnCa\n\nAabAaa BabBaa CabCaa\n\nBaabBaaa CabbCaba\n\nBaabbBaaba\n\nAbAa BbBa CnCa\n\nAbbAba CabCaa\n\nCabbCaba\n\nAabAaa BbbBba\n\nBbbbBbaa\n\n\n\n12 ? NASA Systems Engineering Handbook\n\n2.0 Fundamentals of Systems Engineering\n\nucts. These will come later in the life cycle. At this point, \nenough conceptual design work has been done to ensure \nthat at least the high-risk requirements are achievable as \nwill be shown in the following passes.\n\n2.3.2.2 Example Product Realization Passes\nSo now that the requirements and conceptual designs for \nthe principal Phase A products have been developed, they \nneed to be checked to ensure they are achievable. Note that \nthere are two types of products. The first product is the \n\u201cend product\u201d\u2014the one that will actually be delivered to \nthe final user. The second type of product will be called \na \u201cphase product.\u201d A phase product is generated within a \nparticular life-cycle phase that helps move the project to-\nward delivering a final product. For example, while in Pre-\nPhase A, a foam-core mockup might be built to help visu-\nalize some of the concepts. Those mockups would not be \nthe final \u201cend product,\u201d but would be the \u201cphase product.\u201d \nFor this Phase A example, assume some computer models \nwill be created and simulations performed of these key \nconcepts to show that they are achievable. These will be \nthe phase product for our example. \n\nNow the focus shifts to the right side (i.e., product real-\nization processes) of the SE engine, which will be applied \nrecursively, starting at the bottom of the product hier-\narchy and moving upwards.\n\nFirst Pass \nEach of the phase products (i.e., our computer models) \nfor the bottom-level product tier (ones that are unshaded \n\nin Figure 2.3-6) is taken \nindividually and real-\nized\u2014that is, it is either \nbought, built, coded, or \nreused. For our example, \nassume the external tank \n\nproduct model Aa is a standard Commercial-Off-the-\nShelf (COTS) product that is bought. Aba is a model that \ncan be reused from another project, and product Abb is \na model that will have to be developed with an in-house \ndesign that is to be built. Note that these models are \nparts of a larger model product that will be assembled \nor integrated on a subsequent runthrough of the SE en-\ngine. That is, to realize the model for product Ab of the \nexternal tank, models for products Aba and Abb must \nbe first implemented and then later integrated together. \nThis pass of the SE engine will be the realizing part. Like-\nwise, each of the unshaded bottom-level model products \n\nis realized in this first pass. The models will help us un-\nderstand and plan the method to implement the final \nend product and will ensure the feasibility of the imple-\nmented method.\n\nNext, each of the realized \nmodels (phase products) \nare used to verify that the \nend product would likely \nmeet the requirements as \ndefined in the Technical \n\nRequirements Definition Process during the system de-\nsign pass for this product. This shows the product would \nlikely meet the \u201cshall\u201d statements that were allocated, \nderived, or generated for it by method of test, analysis, \ninspection, or demonstration\u2014that it was \u201cbuilt right.\u201d \nVerification is performed for each of the unshaded \nbottom-level model products. Note that during this \nPhase A pass, this process is not the formal verification \nof the final end product. However, using analysis, simu-\nlation, models, or other means shows that the require-\nments are good (verifiable) and that the concepts will \nmost likely satisfy them. This also allows draft verifica-\ntion procedures of key areas to be developed. What can \nbe formally verified, however, is that the phase product \n(the model) meets the requirements for the model.\n\nAfter the phase product \n(models) has been veri-\nfied and used for planning \nthe end product verifica-\ntion, the models are then \nused for validation. That \n\nis, additional test, analysis, inspection, or demonstrations \nare conducted to ensure that the proposed conceptual de-\nsigns will likely meet the expectations of the stakeholders \nfor this phase product and for the end product. This will \ntrack back to the ConOps that was mutually developed \nwith the stakeholders during the Stakeholder Expecta-\ntions Definition Process of the system design pass for \nthis product. This will help ensure that the project has \n\u201cbuilt the right\u201d product at this level.\n\nAfter verification and \nvalidation of the phase \nproduct (models) and \nusing it for planning the \nverification and valida-\ntion of the end product, \n\nit is time to prepare the model for transition to the next \n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n\n\n2.3 Example of Using the SE Engine\n\nNASA Systems Engineering Handbook ? 13\n\nlevel up. Depending on complexity, where the model will \nbe transitioned, security requirements, etc., transition \nmay involve crating and shipment, transmitting over a \nnetwork, or hand carrying over to the next lab. Whatever \nis appropriate, each model for the bottom-level product \nis prepared and handed to the next level up for further \nintegration.\n\nSecond Pass\nNow that all the models (phase products) for the bottom-\nlevel end products are realized, verified, validated, and \n\ntransitioned, it is time \nto start integrating them \ninto the next higher level \nproduct. For example, for \nthe external tank, realized \ntier 4 models for product \n\nAba and Abb are integrated to form the model for the \ntier 3 product Ab. Note that the Product Implementation \nProcess only occurs at the bottommost product. All sub-\nsequent passes of the SE engine will employ the Product \nIntegration Process since already realized products will \nbe integrated to form the new higher level products. In-\ntegrating the lower tier phase products will result in the \nnext-higher-tier phase product. This integration process \ncan also be used for planning the integration of the final \nend products.\n\nAfter the new integrated \nphase product (model) \nhas been formed (tier 3 \nproduct Ab for example), \nit must now be proven \nthat it meets its require-\n\nments. These will be the allocated, derived, or generated \nrequirements developed during the Technical Require-\nments Definition Process during the system design pass \nfor the model for this integrated product. This ensures that \nthe integrated product was built (assembled) right. Note \nthat just verifying the component parts (i.e., the individual \nmodels) that were used in the integration is not sufficient \nto assume that the integrated product will work right. \nThere are many sources of problems that could occur\u2014\nincomplete requirements at the interfaces, wrong assump-\ntions during design, etc. The only sure way of knowing if \nan integrated product is good is to perform verification \nand validation at each stage. The knowledge gained from \nverifying this integrated phase product can also be used \nfor planning the verification of the final end products.\n\nLikewise, after the inte-\ngrated phase product is \nverified, it needs to be \nvalidated to show that it \nmeets the expectations \nas documented in the \n\nConOps for the model of the product at this level. Even \nthough the component parts making up the integrated \nproduct will have been validated at this point, the only \nway to know that the project has built the \u201cright\u201d inte-\ngrated product is to perform validation on the integrated \nproduct itself. Again, this information will help in the \nplanning for the validation of the end products.\n\nThe model for the inte-\ngrated phase product at \nthis level (tier 3 product \nAb for example) is now \nready to be transitioned \nto the next higher level \n\n(tier 2 for the example). As with the products in the first \npass, the integrated phase product is prepared according \nto its needs/requirements and shipped or handed over. \nIn the example, the model for the external tank tier 3 in-\ntegrated product Ab is transitioned to the owners of the \nmodel for the tier 2 product A. This effort with the phase \nproducts will be useful in planning for the transition of \nthe end products.\n\nPasses 3 Through n\nIn a similar manner as the second pass, the tier 3 models \nfor the products are integrated together, realized, veri-\n\nfied, validated, and transi-\ntioned to the next higher \ntier. For the example, \nthe realized model for \nexternal tank tier 3 in-\ntegrated phase product \n\nAb is integrated with the model for tier 3 realized phase \nproduct Aa to form the tier 2 phase product A. Note that \ntier 3 product Aa is a bottom-tier product that has yet \nto go through the integration process. It may also have \nbeen realized some time ago and has been waiting for the \nAb product line to become realized. Part of its transition \nmight have been to place it in secure storage until the \nAb product line became available. Or it could be that Aa \nwas the long-lead item and product Ab had been com-\npleted some time ago and was waiting for the Aa pur-\nchase to arrive before they could be integrated together. \n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n\n\n14 ? NASA Systems Engineering Handbook\n\n2.0 Fundamentals of Systems Engineering\n\nThe length of the branch of the product tree does not nec-\nessarily translate to a corresponding length of time. This \nis why good planning in the first part of a project is so \ncritical.\n\nFinal Pass\nAt some point, all the models for the tier 1 phase prod-\nucts will each have been used to ensure the system re-\n\nquirements and con-\ncepts developed during \nthis Phase A cycle can be \nimplemented, integrated, \nverified, validated, and \ntransitioned. The ele-\n\nments are now defined as the external tank, the orbiter, \nand the solid rocket boosters. One final pass through \nthe SE engine will show that they will likely be success-\nfully implemented, integrated, verified, and validated. \nThe final of these products\u2014in the form of the base-\nlined system requirements, ConOps, conceptual func-\ntional and physical designs\u2014are made to provide inputs \ninto the next life-cycle phase (B) where they will be fur-\nther matured. In later phases, the products will actually \nbe built into physical form. At this stage of the project, \nthe key characteristics of each product are passed down-\nstream in key SE documentation, as noted.\n\n2.3.2.3 Example Use of the SE Engine in \nPhases B Through D\n\nPhase B begins the preliminary design of the final end \nproduct. The recursive passes through the SE engine are \nrepeated in a similar manner to that discussed in the de-\ntailed Phase A example. At this phase, the phase product \nmight be a prototype of the product(s). Prototypes could \nbe developed and then put through the planned verifica-\ntion and validation processes to ensure the design will \nlikely meet all the requirements and expectations prior \nto the build of the final flight units. Any mistakes found \non prototypes are much easier and less costly to correct \nthan if not found until the flight units are built and un-\ndergoing the certification process.\n\nWhereas the previous phases dealt with the final product \nin the form of analysis, concepts, or prototypes, Phases \nC and D work with the final end product itself. During \nPhase C, we recursively use the left side of the SE engine \nto develop the final design. In Phase D, we recursively use \nthe right side of the SE engine to realize the final product \nand conduct the formal verification and validation of the \n\nfinal product. As we come out of the last pass of the SE \nengine in Phase D, we have the final fully realized end \nproduct, the STS, ready to be delivered for launch.\n\n2.3.2.4 Example Use of the SE Engine in \nPhases E and F\n\nEven in Phase E (Operations and Sustainment) and \nPhase F (Closeout) of the life cycle, the technical man-\n\nagement processes in the \nSE engine are still being \nused. During the opera-\ntions phase of a project, \na number of activities are \nstill going on. In addi-\n\ntion to the day-to-day use of the product, there is a need \nto monitor or manage various aspects of the system. \nThis is where the key Technical Performance Measures \n(TPMs) that were defined in the early stages of devel-\nopment continue to play a part. (TPMs are described in \nSubsection 6.7.2.) These are great measures to monitor \nto ensure the product continues to perform as designed \nand expected. Configurations are still under control, still \nexecuting the Configuration Management Process. De-\ncisions are still being made using the Decision Analysis \nProcess. Indeed, all of the technical management pro-\ncesses still apply. For this discussion, the term \u201csystems \nmanagement\u201d will be used for this aspect of operations. \nIn addition to systems management and systems oper-\nation, there may also be a need for periodic refurbish-\nment, repairing broken parts, cleaning, sparing, logis-\ntics, or other activities. Although other terms are used, \nfor the purposes of this discussion the term \u201csustaining \nengineering\u201d will be used for these activities. Again, all of \nthe technical management processes still apply to these \n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n\n16\n\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n16\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\nFigure 2.3?7 Model of typical activities during \noperational phase (Phase E) of a product\n\n    \n    \n\n S\nus\n\nta\nin\n\nin\ng \n\nEn\ngin\n\neer\ning\n\n           Systems M\nanagem\n\nent\n\nPhase E\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n16\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\nOperation\n\n\n\n2.3 Example of Using the SE Engine\n\nNASA Systems Engineering Handbook ? 15\n\nactivities. Figure 2.3-7 represents these three activities \noccurring simultaneously and continuously throughout \nthe operational lifetime of the final product. Some por-\ntions of the SE processes need to continue even after the \nsystem becomes nonoperational to handle retirement, \ndecommissioning, and disposal. This is consistent with \nthe basic SE principle of handling the full system life \ncycle from \u201ccradle to grave.\u201d\n\nHowever, if at any point in this phase a new product, a \nchange that affects the design or certification of a product, \nor an upgrade to an existing product is needed, the de-\nvelopment processes of the SE engine are reentered at the \ntop. That is, the first thing that is done for an upgrade is \nto determine who the stakeholders are and what they ex-\npect. The entire SE engine is used just as for a newly de-\nveloped product. This might be pictorially portrayed as in \nFigure 2.3-8. Note that in the figure although the SE engine \nis shown only once, it is used recursively down through \nthe product hierarchy for upgraded products, just as de-\nscribed in our detailed example for the initial product.\n\n2.4 Distinctions Between Product \nVerification and Product \nValidation\n\nFrom a process perspective, the Product Verification and \nProduct Validation Processes may be similar in nature, \nbut the objectives are fundamentally different. Verifica-\n\ntion of a product shows proof of compliance with require-\nments\u2014that the product can meet each \u201cshall\u201d statement \nas proven though performance of a test, analysis, inspec-\ntion, or demonstration. Validation of a product shows that \nthe product accomplishes the intended purpose in the in-\ntended environment\u2014that it meets the expectations of the \ncustomer and other stakeholders as shown through per-\nformance of a test, analysis, inspection, or demonstration.\n\nVerification testing relates back to the approved require-\nments set and can be performed at different stages in the \nproduct life cycle. The approved specifications, draw-\nings, parts lists, and other configuration documenta-\ntion establish the configuration baseline of that product, \nwhich may have to be modified at a later time. Without a \nverified baseline and appropriate configuration controls, \nlater modifications could be costly or cause major per-\nformance problems. \n\nValidation relates back to the ConOps document. Vali-\ndation testing is conducted under realistic conditions (or \nsimulated conditions) on end products for the purpose \nof determining the effectiveness and suitability of the \nproduct for use in mission operations by typical users. \n\nThe selection of the verification or validation method is \nbased on engineering judgment as to which is the most \neffective way to reliably show the product\u2019s conformance \nto requirements or that it will operate as intended and \ndescribed in the ConOps. \n\nFigure 2.3?8 New products or upgrades reentering the SE engine\n\n     \n    \n\nSu\nst\n\nai\nni\n\nng\n En\n\ngi\nne\n\neri\nng\n\n           Systems Managem\nent\n\n \n\n \n   P\n\nha\nse\n\n B  \n         \n\n          Phase C                    Phase D\n                    \n\n1\n2\n\n3\n4\n\n10\n9\n\n16\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\nFinal Deployment to End User\n\nInitial Idea\nPhase F \nCloseout\n\nUpgrades/Changes Reenter SE Engine\nat Stakeholder Expectations De?nition\n\nPhase E\n\n1\n2\n\n3\n4\n\n10\n9\n\n17\n16\n15\n14\n13\n12\n11\n\n5\n6\n\n7\n8\n\n               Pre-Phas\ne A\n\n     \n    \n\n    \n    \n\n    \n    \n\n    \n    \n\n    \n \n\n                 Operatio\nn   \n\n     \n    \n\n    \n    \n\n    \n    \n\n    \n    \n\n  \nPhase A                                    \n\n  \n\n\n\n16 ? NASA Systems Engineering Handbook\n\n2.0 Fundamentals of Systems Engineering\n\n2.5 Cost Aspect of Systems \nEngineering\n\nThe objective of systems engineering is to see that the \nsystem is designed, built, and operated so that it accom-\nplishes its purpose safely in the most cost-effective way \npossible considering performance, cost, schedule, and \nrisk.\n\nA cost-effective and safe system must provide a partic-\nular kind of balance between effectiveness and cost: the \nsystem must provide the most effectiveness for the re-\nsources expended, or equivalently, it must be the least \nexpensive for the effectiveness it provides. This condition \nis a weak one because there are usually many designs \nthat meet the condition. Think of each possible design \nas a point in the tradeoff space between effectiveness and \ncost. A graph plotting the maximum achievable effec-\ntiveness of designs available with current technology as \na function of cost would, in general, yield a curved line \nsuch as the one shown in Figure 2.5-1. (In the figure, all \nthe dimensions of effectiveness are represented by the \nordinate (y axis) and all the dimensions of cost by the \nabscissa (x axis).) In other words, the curved line repre-\nsents the envelope of the currently available technology \nin terms of cost-effectiveness. \n\nPoints above the line cannot be achieved with currently \navailable technology; that is, they do not represent fea-\nsible designs. (Some of those points may be feasible in \nthe future when further technological advances have \nbeen made.) Points inside the envelope are feasible, but \nare said to be dominated by designs whose combined \ncost and effectiveness lie on the envelope line. Designs \nrepresented by points on the envelope line are called \ncost-effective (or efficient or nondominated) solutions. \n\nDesign trade studies, an important part of the systems \nengineering process, often attempt to find designs that \nprovide a better combination of the various dimensions \nof cost and effectiveness. When the starting point for a \ndesign trade study is inside the envelope, there are alter-\nnatives that either reduce costs with change to the overall \neffectiveness or alternatives that improve effectiveness \nwithout a cost increase (i.e., moving closer to the enve-\nlope curve). Then, the systems engineer\u2019s decision is easy. \nOther than in the sizing of subsystems, such \u201cwin-win\u201d \ndesign trades are uncommon, but by no means rare. \nWhen the alternatives in a design trade study require \ntrading cost for effectiveness, or even one dimension of \neffectiveness for another at the same cost (i.e., moving \nparallel to the envelope curve), the decisions become \nharder.\n\nSystem Cost, Effectiveness, and  \nCost?Effectiveness\n\nCost: ?  The cost of a system is the value of the re-\nsources needed to design, build, operate, and \ndispose of it. Because resources come in many \nforms\u2014work performed by NASA personnel and \ncontractors; materials; energy; and the use of facili-\nties and equipment such as wind tunnels, factories, \noffices, and computers\u2014it is convenient to express \nthese values in common terms by using monetary \nunits (such as dollars of a specified year).\n\nEffectiveness: ?  The effectiveness of a system is a \nquantitative measure of the degree to which the \nsystem\u2019s purpose is achieved. Effectiveness mea-\nsures are usually very dependent upon system per-\nformance. For example, launch vehicle effective-\nness depends on the probability of successfully \ninjecting a payload onto a usable trajectory. The \nassociated system performance attributes include \nthe mass that can be put into a specified nominal \norbit, the trade between injected mass and launch \nvelocity, and launch availability.\n\nCost?Effectiveness: ?  The cost-effectiveness of a sys-\ntem combines both the cost and the effectiveness \nof the system in the context of its objectives. While \nit may be necessary to measure either or both of \nthose in terms of several numbers, it is sometimes \npossible to combine the components into a mean-\ningful, single-valued objective function for use in de-\nsign optimization. Even without knowing how to \ntrade effectiveness for cost, designs that have lower \ncost and higher effectiveness are always preferred.\n\nFigure 2.5?1 The enveloping surface of \nnondominated designs\n\nCost\n\nE?\nec\n\nti\nve\n\nne\nss\n\nThere are no designs\nthat produce results in\nthis portion of the\ntrade space \n\nAll possible designs with\ncurrently known technology\nproduce results somewhere \nin this portion of the trade\nspace \n\n\n\n2.5 Cost Aspect of Systems Engineering\n\nNASA Systems Engineering Handbook ? 17\n\nThe process of finding the most cost-effective design is \nfurther complicated by uncertainty, which is shown in \nFigure 2.5-2. Exactly what outcomes will be realized by \na particular system design cannot be known in advance \nwith certainty, so the projected cost and effectiveness of a \ndesign are better described by a probability distribution \nthan by a point. This distribution can be thought of as a \ncloud that is thickest at the most likely value and thinnest \nfarthest away from the most likely point, as is shown for \ndesign concept A in the figure. Distributions resulting \nfrom designs that have little uncertainty are dense and \nhighly compact, as is shown for concept B. Distributions \nassociated with risky designs may have significant prob-\nabilities of producing highly undesirable outcomes, as is \nsuggested by the presence of an additional low-effective-\nness/high-cost cloud for concept C. (Of course, the en-\nvelope of such clouds cannot be a sharp line such as is \nshown in the figure, but must itself be rather fuzzy. The \nline can now be thought of as representing the envelope \nat some fixed confidence level, that is, a specific, numer-\nical probability of achieving that effectiveness.) \n\nBoth effectiveness and cost may require several descrip-\ntors. Even the Echo balloons (circa 1960), in addition \nto their primary mission as communications satellites, \nobtained scientific data on the electromagnetic environ-\nment and atmospheric drag. Furthermore, Echo was \nthe first satellite visible to the naked eye, an unquanti-\n\nfiable\u2014but not unrecognized at the beginning of the \nspace race\u2014aspect of its effectiveness. Sputnik (circa \n1957), for example, drew much of its effectiveness from \nthe fact that it was a \u201cfirst.\u201d Costs, the expenditure of lim-\nited resources, may be measured in the several dimen-\nsions of funding, personnel, use of facilities, and so on. \nSchedule may appear as an attribute of effectiveness or \ncost, or as a constraint. A mission to Mars that misses its \nlaunch window has to wait about two years for another \nopportunity\u2014a clear schedule constraint.\n\nIn some contexts, it is appropriate to seek the most effec-\ntiveness possible within a fixed budget and with a fixed \nrisk; in other contexts, it is more appropriate to seek the \nleast cost possible with specified effectiveness and risk. \nIn these cases, there is the question of what level of effec-\ntiveness to specify or what level of costs to fix. In prac-\ntice, these may be mandated in the form of performance \nor cost requirements. It then becomes appropriate to ask \nwhether a slight relaxation of requirements could pro-\nduce a significantly cheaper system or whether a few \nmore resources could produce a significantly more ef-\nfective system. \n\nThe technical team must choose among designs that \ndiffer in terms of numerous attributes. A variety of \nmethods have been developed that can be used to help \nuncover preferences between attributes and to quantify \nsubjective assessments of relative value. When this can \nbe done, trades between attributes can be assessed quan-\ntitatively. Often, however, the attributes seem to be truly \nincommensurate: decisions need to be made in spite of \nthis multiplicity.\n\nFigure 2.5?2 Estimates of outcomes to be \nobtained from several design concepts including \n\nuncertainty\nNote: A, B, and C are design concepts with different risk patterns.\n\nB\n\nC\n\nA\n\nCost\n\nE?\nec\n\nti\nve\n\nne\nss\n\nThe Systems Engineer\u2019s Dilemma\n\nAt each cost-effective solution:\n\nTo reduce cost at constant risk, performance must  ?\nbe reduced.\n\nTo reduce risk at constant cost, performance must  ?\nbe reduced.\n\nTo reduce cost at constant performance, higher  ?\nrisks must be accepted.\n\nTo reduce risk at constant performance, higher  ?\ncosts must be accepted.\n\nIn this context, time in the schedule is often a critical \nresource, so that schedule behaves like a kind of cost.\n\n\n\n\n\nNASA Systems Engineering Handbook ? 19\n\n3.0 NASA Program/Project Life Cycle\n\nOne of the fundamental concepts used within NASA for \nthe management of major systems is the program/project \nlife cycle, which consists of a categorization of everything \nthat should be done to accomplish a program or project \ninto distinct phases, separated by Key Decision Points \n(KDPs). KDPs are the events at which the decision au-\nthority determines the readiness of a program/project to \nprogress to the next phase of the life cycle (or to the next \nKDP). Phase boundaries are defined so that they provide \nmore or less natural points for Go or No-Go decisions. \nDecisions to proceed may be qualified by liens that must \nbe removed within an agreed to time period. A program \nor project that fails to pass a KDP may be allowed to \u201cgo \nback to the drawing board\u201d to try again later\u2014or it may \nbe terminated.\n\nAll systems start with the recognition of a need or the \ndiscovery of an opportunity and proceed through var-\nious stages of development to a final disposition. While \nthe most dramatic impacts of the analysis and optimi-\nzation activities associated with systems engineering are \nobtained in the early stages, decisions that affect millions \nof dollars of value or cost continue to be amenable to the \nsystems approach even as the end of the system lifetime \napproaches.\n\nDecomposing the program/project life cycle into phases \norganizes the entire process into more manageable pieces. \nThe program/project life cycle should provide managers \nwith incremental visibility into the progress being made \nat points in time that fit with the management and bud-\ngetary environments.\n\nNPR 7120.5, NASA Space Flight Program and Project \nManagement Requirements defines the major NASA \nlife-cycle phases as Formulation and Implementation. \nFor Flight Systems and Ground Support (FS&GS) \nprojects, the NASA life-cycle phases of Formulation \nand Implementation divide into the following seven \nincremental pieces. The phases of the project life cycle \nare:\n\nPre-Phase A: Concept Studies (i.e., identify feasible  ?\nalternatives)\n\nPhase A: Concept and Technology Development (i.e.,  ?\ndefine the project and identify and initiate necessary \ntechnology)\nPhase B: Preliminary Design and Technology Com- ?\npletion (i.e., establish a preliminary design and de-\nvelop necessary technology)\nPhase C: Final Design and Fabrication (i.e., complete  ?\nthe system design and build/code the components)\nPhase D: System Assembly, Integration and Test,  ?\nLaunch (i.e., integrate components, and verify the \nsystem, prepare for operations, and launch)\nPhase E: Operations and Sustainment (i.e., operate  ?\nand maintain the system)\nPhase F: Closeout (i.e., disposal of systems and anal- ?\nysis of data)\n\nFigure 3.0-1 (NASA program life cycle) and Figure 3.0-2 \n(NASA project life cycle) identify the KDPs and re-\nviews that characterize the phases. Sections 3.1 and 3.2 \ncontain narrative descriptions of the purposes, major \nactivities, products, and KDPs of the NASA program \nlife-cycle phases. Sections 3.3 to 3.9 contain narrative \ndescriptions of the purposes, major activities, prod-\nucts, and KDPs of the NASA project life-cycle phases. \nSection 3.10 describes the NASA budget cycle within \nwhich program/project managers and systems engi-\nneers must operate.\n\n3.1 Program Formulation\nThe program Formulation phase establishes a cost-ef-\nfective program that is demonstrably capable of meeting \nAgency and mission directorate goals and objectives. \nThe program Formulation Authorization Document \n(FAD) authorizes a Program Manager (PM) to initiate \nthe planning of a new program and to perform the anal-\nyses required to formulate a sound program plan. Major \nreviews leading to approval at KDP I are the P/SRR, \nP/SDR, PAR, and governing Program Management \nCouncil (PMC) review. (See full list of reviews in the \nprogram and project life cycle figures on the next page.) \nA summary of the required gate products for the pro-\n\n\n\n20 ? NASA Systems Engineering Handbook\n\n3.0 NASA Program/Project Life Cycle\n\nFormulation Implementation\n\nKDP 0 KDP I KDP II KDP IVKDP III\n\nNASA Life-\nCycle Phases\n\nApproval\n\nMajor Program\nReviews\n\nKey Decision\nPoints\n\nKDP n\n\nP/SRR P/SDR\n\nPSR\n\nPDR CDR ORR\n\nUncoupled & Loosely Coupled Programs\n\nOr\n\nSingle-Project & Tightly Coupled Programs\nSIR CERRTRR\n\nPSRs, PIRs, and KDPs are conducted ~ every 2 years\n\nOr\n\nPSRFRR PLAR\n\nFigure 3.0?1 NASA program life cycle\n\nFormulation Implementation\n\nMCR PDR SAR FRR DRPLAR CERR\n\nMCR\n\nSRR SDR\n\nMDRSRR PDR\n\nCDR/PRR\n\nCDR/PRR FRR\n\nORR\n\nORR DR\n\nSIR\n\nSIR PLAR CERR\n\nPFAR\n\nKDP A KDP B KDP C KDP EKDP D\n\nLaunch\n\nNASA Life-\nCycle Phases\n\nApproval\n\nProject Life-\nCycle Phases\n\nSupporting\nReviews\n\nRobotic Mission\nReviews\n\nHuman Space\nFlight Reviews\n\nKey Decision\nPoints\n\nPhase F:\nCloseout \n\nPhase E:\nOperations &\nSustainment \n\nPhase D:\nSystem Assembly,\nIntegration & Test,\n\nLaunch\n\nPre-Phase A:\nConcept\nStudies\n\nPhase B:\nPreliminary\n\nDesign &\nTechnology\nCompletion\n\nPhase A:\nConcept & Technology\n\nDevelopment \n\nPhase C:\nFinal Design &\n\nFabrication\n\nKDP F\n\nPeer Reviews, Subsystem Reviews, and System Reviews\n\nTRR\n\nTRR\n\nFigure 3.0?2 NASA project life cycle\n\nCDR Critical Design Review\nCERR Critical Events Readiness Review\nDR Decommissioning Review\nFRR Flight Readiness Review\nKDP Key Decision Point\nMCR Mission Concept Review\nMDR Mission Definition Review\nORR Operational Readiness Review\nPDR Preliminary Design Review\nPFAR Post-Flight Assessment Review\nPIR Program Implementation Review\n\nPLAR Post-Launch Assessment Review\nPRR Production Readiness Review\nP/SDR Program/System Definition Review\nP/SRR Program/System Requirements Review\nPSR Program Status Review\nSAR System Acceptance Review\nSDR System Definition Review\nSIR System Integration Review\nSRR System Requirements Review\nTRR Test Readiness Review\n\n\n\n3.1 Program Formulation\n\nNASA Systems Engineering Handbook ? 21\n\ngram Formulation phase can be found in NPR 7120.5. \nFormulation for all program types is the same, involving \none or more program reviews followed by KDP I where \na decision is made approving a program to begin imple-\nmentation. Typically, there is no incentive to move a pro-\ngram into implementation until its first project is ready \nfor implementation.\n\n3.2 Program Implementation\nDuring the program Implementation phase, the PM \nworks with the Mission Directorate Associate Admin-\nistrator (MDAA) and the constituent project man-\nagers to execute the program plan cost effectively. \nProgram reviews ensure that the program continues \nto contribute to Agency and mission directorate goals \nand objectives within funding constraints. A sum-\nmary of the required gate products for the program \nImplementation phase can be found in NPR 7120.5. \nThe program life cycle has two different implementa-\ntion paths, depending on program type. Each imple-\nmentation path has different types of major reviews. \n\nFor uncoupled and loosely coupled programs, the \nImplementation phase only requires PSRs and PIRs \nto assess the program\u2019s performance and make a rec-\nommendation on its authorization at KDPs approx-\nimately every two years. Single-project and tightly \ncoupled programs are more complex. For single-\nproject programs, the Implementation phase program \nreviews shown in Figure 3.0-1 are synonymous (not \nduplicative) with the project reviews in the project life \ncycle (see Figure 3.0-2) through Phase D. Once in op-\nerations, these programs usually have biennial KDPs \npreceded by attendant PSRs/PIRs. Tightly coupled \nprograms during implementation have program re-\nviews tied to the project reviews to ensure the proper \nintegration of projects into the larger system. Once in \noperations, tightly coupled programs also have bien-\nnial PSRs/PIRs/KDPs to assess the program\u2019s perfor-\nmance and authorize its continuation.\n\nProgram Formulation\n\nPurpose\nTo establish a cost-effective program that is demon-\nstrably capable of meeting Agency and mission direc-\ntorate goals and objectives\n\nTypical Activities and Their Products\nDevelop program requirements and allocate them  ?\nto initial projects \n\nDefine and approve program acquisition strategies ?\n\nDevelop interfaces to other programs ?\n\nStart development of technologies that cut across  ?\nmultiple projects within the program\n\nDerive initial cost estimates and approve a program  ?\nbudget\n\nPerform required program Formulation technical  ?\nactivities defined in NPR 7120.5\n\nSatisfy program Formulation reviews\u2019 entrance/suc- ?\ncess criteria detailed in NPR 7123.1\n\nReviews\nP/SRR  ?\nP/SDR ? Program Implementation\n\nPurpose\nTo execute the program and constituent projects \nand ensure the program continues to contribute to \nAgency goals and objectives within funding con-\nstraints\n\nTypical Activities and Their Products\nInitiate projects through direct assignment or com- ?\npetitive process (e.g., Request for Proposal (RFP), \nAnnouncement of Opportunity (AO)\n\nMonitor project\u2019s formulation, approval, implemen- ?\ntation, integration, operation, and ultimate decom-\nmissioning\n\nAdjust program as resources and requirements  ?\nchange\n\nPerform required program Implementation techni- ?\ncal activities from NPR 7120.5\n\nSatisfy program Implementation reviews\u2019 entrance/ ?\nsuccess criteria from NPR 7123.1\n\nReviews\nPSR/PIR (uncoupled and loosely coupled programs  ?\nonly)\n\nReviews synonymous (not duplicative) with the  ?\nproject reviews in the project life cycle (see Fig-\nure 3.0-2) through Phase D (single-project and \ntightly coupled programs only)\n\n\n\n22 ? NASA Systems Engineering Handbook\n\n3.0 NASA Program/Project Life Cycle\n\n3.3 Project Pre-Phase A: Concept \nStudies\n\nThe purpose of this phase, which is usually performed \nmore or less continually by concept study groups, is to \ndevise various feasible concepts from which new proj-\nects (programs) can be selected. Typically, this activity \nconsists of loosely structured examinations of new ideas, \n\nusually without central control and mostly oriented to-\nward small studies. Its major product is a list of sug-\ngested projects, based on the identification of needs and \nthe discovery of opportunities that are potentially con-\nsistent with NASA\u2019s mission, capabilities, priorities, and \nresources.\n\nAdvanced studies may extend for several years and may \nbe a sequence of papers that are only loosely connected. \nThese studies typically focus on establishing mission \ngoals and formulating top-level system requirements \nand ConOps. Conceptual designs are often offered to \ndemonstrate feasibility and support programmatic es-\ntimates. The emphasis is on establishing feasibility and \ndesirability rather than optimality. Analyses and designs \nare accordingly limited in both depth and number of op-\ntions.\n\n3.4 Project Phase A: Concept and \nTechnology Development\n\nDuring Phase A, activities are performed to fully develop \na baseline mission concept and begin or assume respon-\nsibility for the development of needed technologies. This \nwork, along with interactions with stakeholders, helps \nestablish a mission concept and the program require-\nments on the project. \n\nIn Phase A, a team\u2014often associated with a program or \ninformal project office\u2014readdresses the mission con-\ncept to ensure that the project justification and practi-\ncality are sufficient to warrant a place in NASA\u2019s budget. \nThe team\u2019s effort focuses on analyzing mission require-\nments and establishing a mission architecture. Activi-\nties become formal, and the emphasis shifts toward es-\ntablishing optimality rather than feasibility. The effort \naddresses more depth and considers many alternatives. \nGoals and objectives are solidified, and the project de-\nvelops more definition in the system requirements, top-\nlevel system architecture, and ConOps. Conceptual de-\nsigns are developed and exhibit more engineering detail \nthan in advanced studies. Technical risks are identified \nin more detail, and technology development needs be-\ncome focused.\n\nIn Phase A, the effort focuses on allocating functions to \nparticular items of hardware, software, personnel, etc. \nSystem functional and performance requirements, along \nwith architectures and designs, become firm as system \ntradeoffs and subsystem tradeoffs iterate back and forth \n\nPre?Phase A: Concept Studies\n\nPurpose\nTo produce a broad spectrum of ideas and alterna-\ntives for missions from which new programs/projects \ncan be selected\n\nTypical Activities and Products\n(Note: AO projects will have defined the deliverable \nproducts.)\n\nIdentify missions and architecture consistent with  ?\ncharter\n\nIdentify and involve users and other stakeholders ?\n\nIdentify and perform tradeoffs and analyses  ?\n\nIdentify requirements, which include: ?\n\nMission, ?\nScience, and ?\nTop-level system. ?\n\nDefine measures of effectiveness and measures of  ?\nperformance\n\nIdentify top-level technical performance measures  ?\n\nPerform preliminary evaluations of possible mis- ?\nsions\n\nPrepare program/project proposals, which may in- ?\nclude:\n\nMission justification and objectives; ?\nPossible ConOps; ?\nHigh-level WBSs; ?\nCost, schedule, and risk estimates; and ?\nTechnology assessment and maturation strate- ?\ngies.\n\nPrepare preliminary mission concept report ?\n\nPerform required Pre-Phase A technical activities  ?\nfrom NPR 7120.5\n\nSatisfy MCR entrance/success criteria from NPR 7123.1 ?\n\nReviews\nMCR ?\n\nInformal proposal review ?\n\n\n\n3.4 Project Phase A: Concept and Technology Development\n\nNASA Systems Engineering Handbook ? 23\n\nPhase A: Concept and Technology Development\n\nPurpose\nTo determine the feasibility and desirability of a suggested new major system and establish an initial baseline compat-\nibility with NASA\u2019s strategic plans\n\nTypical Activities and Their Products\nPrepare and initiate a project plan ?\n\nDevelop top-level requirements and constraints ?\n\nDefine and document system requirements (hardware and software) ?\n\nAllocate preliminary system requirements to next lower level ?\n\nDefine system software functionality description and requirements ?\n\nDefine and document internal and external interface requirements ?\n\nIdentify integrated logistics support requirements ?\n\nDevelop corresponding evaluation criteria and metrics ?\n\nDocument the ConOps ?\n\nBaseline the mission concept report ?\n\nDemonstrate that credible, feasible design(s) exist ?\n\nPerform and archive trade studies ?\n\nDevelop mission architecture ?\n\nInitiate environmental evaluation/National Environmental Policy Act process ?\n\nDevelop initial orbital debris assessment (NASA Safety Standard 1740.14) ?\n\nEstablish technical resource estimates ?\n\nDefine life-cycle cost estimates and develop system-level cost-effectiveness model ?\n\nDefine the WBS ?\n\nDevelop SOWs ?\n\nAcquire systems engineering tools and models ?\n\nBaseline the SEMP ?\n\nDevelop system risk analyses ?\n\nPrepare and initiate a risk management plan ?\n\nPrepare and Initiate a configuration management plan ?\n\nPrepare and initiate a data management plan ?\n\nPrepare engineering specialty plans (e.g., contamination control plan, electromagnetic interference/electromagnetic  ?\ncompatibility control plan, reliability plan, quality control plan, parts management plan)\n\nPrepare a safety and mission assurance plan ?\n\nPrepare a software development or management plan (see NPR 7150.2) ?\n\nPrepare a technology development plan and initiate advanced technology development ?\n\nEstablish human rating plan ?\n\nDefine verification and validation approach and document it in verification and validation plans ?\n\nPerform required Phase A technical activities from NPR 7120.5 ?\n\nSatisfy Phase A reviews\u2019 entrance/success criteria from NPR 7123.1 ?\n\nReviews\nSRR ?\nMDR (robotic mission only) ?\nSDR (human space flight only) ?\n\n\n\n24 ? NASA Systems Engineering Handbook\n\n3.0 NASA Program/Project Life Cycle\n\nin the effort to seek out more cost-effective designs. \n(Trade studies should precede\u2014rather than follow\u2014\nsystem design decisions.) Major products to this point \ninclude an accepted functional baseline for the system \nand its major end items. The effort also produces var-\nious engineering and management plans to prepare for \nmanaging the project\u2019s downstream processes, such as \nverification and operations, and for implementing engi-\nneering specialty programs.\n\n3.5 Project Phase B: Preliminary \nDesign and Technology \nCompletion\n\nDuring Phase B, activities are performed to establish \nan initial project baseline, which (according to NPR \n7120.5 and NPR 7123.1) includes \u201ca formal flow down \nof the project-level performance requirements to a \ncomplete set of system and subsystem design speci-\nfications for both flight and ground elements\u201d and \n\u201ccorresponding preliminary designs.\u201d The technical \nrequirements should be sufficiently detailed to estab-\nlish firm schedule and cost estimates for the project. \nIt also should be noted, especially for AO-driven proj-\nects, that Phase B is where the top-level requirements \nand the requirements flowed down to the next level \nare finalized and placed under configuration con-\ntrol. While the requirements should be baselined in \nPhase A, there are just enough changes resulting from \nthe trade studies and analyses in late Phase A and \nearly Phase B that changes are inevitable. However, by \nmid-Phase B, the top-level requirements should be fi-\nnalized.\n\nActually, the Phase B baseline consists of a collection \nof evolving baselines covering technical and business \naspects of the project: system (and subsystem) re-\nquirements and specifications, designs, verification \nand operations plans, and so on in the technical por-\ntion of the baseline, and schedules, cost projections, \nand management plans in the business portion. Es-\ntablishment of baselines implies the implementation \nof configuration management procedures. (See Sec-\ntion 6.5.)\n\nIn Phase B, the effort shifts to establishing a function-\nally complete preliminary design solution (i.e., a func-\ntional baseline) that meets mission goals and objec-\ntives. Trade studies continue. Interfaces among the \n\nPhase B: Preliminary Design and \nTechnology Completion\n\nPurpose\n\nTo define the project in enough detail to establish an \ninitial baseline capable of meeting mission needs\n\nTypical Activities and Their Products\nBaseline the project plan ?\nReview and update documents developed and  ?\nbaselined in Phase A\n\nDevelop science/exploration operations plan based  ?\non matured ConOps\n\nUpdate engineering specialty plans (e.g., contami- ?\nnation control plan, electromagnetic interference/\nelectromagnetic compatibility control plan, reliabil-\nity plan, quality control plan, parts management \nplan)\n\nUpdate technology maturation planning ?\n\nReport technology development results ?\n\nUpdate risk management plan ?\n\nUpdate cost and schedule data ?\n\nFinalize and approve top-level requirements and  ?\nflowdown to the next level of requirements\n\nEstablish and baseline design-to specifications  ?\n(hardware and software) and drawings, verification \nand validation plans, and interface documents at \nlower levels\nPerform and archive trade studies\u2019 results ?\n\nPerform design analyses and report results ?\n\nConduct engineering development tests and re- ?\nport results\n\nSelect a baseline design solution  ?\n\nBaseline a preliminary design report ?\n\nDefine internal and external interface design solu- ?\ntions (e.g., interface control documents)\n\nDefine system operations as well as PI/contract pro- ?\nposal management, review, and access and contin-\ngency planning\n\nDevelop appropriate level safety data package ?\n\nDevelop preliminary orbital debris assessment ?\n\nPerform required Phase B technical activities from  ?\nNPR 7120.5\n\nSatisfy Phase B reviews\u2019 entrance/success criteria  ?\nfrom NPR 7123.1\n\nReviews\nPDR ?\nSafety review ?\n\n\n\n3.5 Project Phase B: Preliminary Design and Technology Completion\n\nNASA Systems Engineering Handbook ? 25\n\nmajor end items are defined. Engineering test items \nmay be developed and used to derive data for further \ndesign work, and project risks are reduced by suc-\ncessful technology developments and demonstrations. \nPhase B culminates in a series of PDRs, containing the \nsystem-level PDR and PDRs for lower level end items \nas appropriate. The PDRs reflect the successive refine-\nment of requirements into designs. (See the doctrine \nof successive refinement in Subsection 4.4.1.2 and \nFigure 4.4-2.) Design issues uncovered in the PDRs \nshould be resolved so that final design can begin with \nunambiguous design-to specifications. From this point \non, almost all changes to the baseline are expected to \nrepresent successive refinements, not fundamental \nchanges. Prior to baselining, the system architecture, \npreliminary design, and ConOps must have been vali-\ndated by enough technical analysis and design work \nto establish a credible, feasible design in greater detail \nthan was sufficient for Phase A.\n\n3.6 Project Phase C: Final Design and \nFabrication\n\nDuring Phase C, activities are performed to establish a \ncomplete design (allocated baseline), fabricate or pro-\nduce hardware, and code software in preparation for \nintegration. Trade studies continue. Engineering test \nunits more closely resembling actual hardware are built \nand tested to establish confidence that the design will \nfunction in the expected environments. Engineering \nspecialty analysis results are integrated into the de-\nsign, and the manufacturing process and controls are \ndefined and validated. All the planning initiated back \nin Phase A for the testing and operational equipment, \nprocesses and analysis, integration of the engineering \nspecialty analysis, and manufacturing processes and \ncontrols is implemented. Configuration management \ncontinues to track and control design changes as de-\ntailed interfaces are defined. At each step in the succes-\nsive refinement of the final design, corresponding inte-\ngration and verification activities are planned in greater \ndetail. During this phase, technical parameters, sched-\nules, and budgets are closely tracked to ensure that \nundesirable trends (such as an unexpected growth in \n\nspacecraft mass or increase in its cost) are recognized \nearly enough to take corrective action. These activities \nfocus on preparing for the CDR, PRR (if required), and \nthe SIR.\n\nPhase C contains a series of CDRs containing the \nsystem-level CDR and CDRs corresponding to the dif-\nferent levels of the system hierarchy. A CDR for each \nend item should be held prior to the start of fabrica-\ntion/production for hardware and prior to the start \nof coding of deliverable software products. Typically, \nthe sequence of CDRs reflects the integration process \nthat will occur in the next phase\u2014that is, from lower \nlevel CDRs to the system-level CDR. Projects, how-\never, should tailor the sequencing of the reviews to \nmeet the needs of the project. If there is a production \nrun of products, a PRR will be performed to ensure the \nproduction plans, facilities, and personnel are ready to \nbegin production. Phase C culminates with an SIR. The \nfinal product of this phase is a product ready for inte-\ngration.\n\n3.7 Project Phase D: System \nAssembly, Integration and Test, \nLaunch\n\nDuring Phase D, activities are performed to assemble, \nintegrate, test, and launch the system. These activities \nfocus on preparing for the FRR. Activities include as-\nsembly, integration, verification, and validation of the \nsystem, including testing the flight system to expected \nenvironments within margin. Other activities include \nthe initial training of operating personnel and imple-\nmentation of the logistics and spares planning. For flight \nprojects, the focus of activities then shifts to prelaunch \nintegration and launch. Although all these activities are \nconducted in this phase of a project, the planning for \nthese activities was initiated in Phase A. The planning \nfor the activities cannot be delayed until Phase D be-\ngins because the design of the project is too advanced \nto incorporate requirements for testing and operations. \nPhase D concludes with a system that has been shown \nto be capable of accomplishing the purpose for which \nit was created.\n\n\n\n26 ? NASA Systems Engineering Handbook\n\n3.0 NASA Program/Project Life Cycle\n\nPhase C: Final Design and Fabrication\n\nPurpose\nTo complete the detailed design of the system (and its associated subsystems, including its operations systems), fabri-\ncate hardware, and code software\n\nTypical Activities and Their Products\nUpdate documents developed and baselined in Phase B ?\n\nUpdate interface documents ?\n\nUpdate  ? mission operations plan based on matured ConOps\n\nUpdate engineering specialty plans (e.g., contamination control plan, electromagnetic interference/electromagnetic  ?\ncompatibility control plan, reliability plan, quality control plan, parts management plan)\n\nAugment baselined documents to reflect the growing maturity of the system, including the system architecture, WBS,  ?\nand project plans\n\nUpdate and baseline production plans ?\n\nRefine integration procedures ?\n\nBaseline  ? logistics support plan\n\nAdd remaining lower level design specifications to the system architecture ?\n\nComplete manufacturing and assembly plans and procedures ?\n\nEstablish and baseline build-to specifications (hardware and software) and drawings, verification and validation plans,  ?\nand interface documents at all levels\n\nBaseline detailed design report ?\n\nMaintain requirements documents ?\n\nMaintain verification and validation plans ?\nMonitor project progress against project plans ?\nDevelop verification and validation procedures ?\nDevelop hardware and software detailed designs ?\nDevelop the system integration plan and the system operation plan ?\nDevelop the end-to-end information system design ?\nDevelop spares planning ?\nDevelop command and telemetry list ?\nPrepare launch site checkout and operations plans ?\nPrepare operations and activation plan ?\n\nPrepare system decommissioning/disposal plan, including human capital transition, for use in Phase F ?\nFinalize appropriate level safety data package ?\n\nDevelop preliminary operations handbook ?\nPerform and archive trade studies ?\n\nFabricate (or code) the product ?\nPerform testing at the component or subsystem level ?\n\nIdentify opportunities for preplanned product improvement ?\n\nBaseline orbital debris assessment ?\n\nPerform required Phase C technical activities from NPR 7120.5 ?\n\nSatisfy Phase C reviews\u2019 entrance/success criteria from NPR 7123.1 ?\n\nReviews\nCDR ?\nPRR ?\nSIR ?\nSafety review ?\n\n\n\n3.7 Project Phase D: System Assembly, Integration and Test, Launch\n\nNASA Systems Engineering Handbook ? 27\n\nPhase D: System Assembly, Integration and Test, Launch\n\nPurpose\nTo assemble and integrate the products and create the system, meanwhile developing confidence that it will be able to \nmeet the system requirements; conduct launch and prepare for operations\n\nTypical Activities and Their Products\nIntegrate and verify items according to the integration and verification plans, yielding verified components and (sub- ?\nsystems)\n\nMonitor project progress against project plans ?\n\nRefine verification and validation procedures at all levels ?\n\nPerform system qualification verifications ?\n\nPerform system acceptance verifications and validation(s) (e.g., end-to-end tests encompassing all elements (i.e.,  ?\nspace element, ground system, data processing system)\n\nPerform system environmental testing ?\n\nAssess and approve verification and validation results ?\n\nResolve verification and validation discrepancies ?\n\nArchive documentation for verifications and validations performed ?\n\nBaseline verification and validation report ?\n\nBaseline \u201cas-built\u201d hardware and software documentation ?\n\nUpdate logistics support plan ?\n\nDocument lessons learned ?\n\nPrepare and baseline operator\u2019s manuals ?\n\nPrepare and baseline maintenance manuals ?\n\nApprove and baseline operations handbook ?\n\nTrain initial system operators and maintainers ?\n\nTrain on contingency planning ?\n\nFinalize and implement spares planning ?\n\nConfirm telemetry validation and ground data processing ?\n\nConfirm system and support elements are ready for flight ?\n\nIntegrate with launch vehicle(s) and launch, perform orbit insertion, etc., to achieve a deployed system ?\n\nPerform initial operational verification(s) and validation(s) ?\n\nPerform required Phase D technical activities from NPR 7120.5 ?\n\nSatisfy Phase D reviews\u2019 entrance/success criteria from NPR 7123.1 ?\n\nReviews\nTRR (at all levels) ?\nSAR (human space flight only) ?\nORR ?\nFRR ?\nSystem functional and physical configuration audits ?\nSafety review ?\n\n\n\n28 ? NASA Systems Engineering Handbook\n\n3.0 NASA Program/Project Life Cycle\n\n3.8 Project Phase E: Operations and \nSustainment\n\nDuring Phase E, activities are performed to conduct the \nprime mission and meet the initially identified need and \nmaintain support for that need. The products of the phase \nare the results of the mission. This phase encompasses \nthe evolution of the system only insofar as that evolution \ndoes not involve major changes to the system architec-\nture. Changes of that scope constitute new \u201cneeds,\u201d and \n\nthe project life cycle starts over. For large flight projects, \nthere may be an extended period of cruise, orbit inser-\ntion, on-orbit assembly, and initial shakedown opera-\ntions. Near the end of the prime mission, the project may \napply for a mission extension to continue mission activi-\nties or attempt to perform additional mission objectives.\n\n3.9 Project Phase F: Closeout\nDuring Phase F, activities are performed to implement \nthe systems decommissioning disposal planning and an-\nalyze any returned data and samples. The products of the \nphase are the results of the mission.\n\nPhase F deals with the final closeout of the system when \nit has completed its mission; the time at which this oc-\ncurs depends on many factors. For a flight system that \nreturns to Earth with a short mission duration, closeout \nmay require little more than deintegration of the hard-\nware and its return to its owner. On flight projects of long \nduration, closeout may proceed according to established \nplans or may begin as a result of unplanned events, such \nas failures. Refer to NPD 8010.3, Notification of Intent to \nDecommission or Terminate Operating Space Systems and \nTerminate Missions for terminating an operating mis-\nsion. Alternatively, technological advances may make it \nuneconomical to continue operating the system either in \nits current configuration or an improved one.\n\nPhase E: Operations and Sustainment\n\nPurpose\nTo conduct the mission and meet the initially identi-\nfied need and maintain support for that need\n\nTypical Activities and Their Products\nConduct launch vehicle performance assessment ?\n\nConduct in-orbit spacecraft checkout ?\n\nCommission and activate science instruments ?\n\nConduct the intended prime mission(s) ?\n\nCollect engineering and science data ?\n\nTrain replacement operators and maintainers ?\n\nTrain the flight team for future mission phases (e.g.,  ?\nplanetary landed operations)\n\nMaintain and approve operations and mainte- ?\nnance logs\n\nMaintain and upgrade the system ?\n\nAddress problem/failure reports ?\n\nProcess and analyze mission data ?\n\nApply for mission extensions, if warranted, and con- ?\nduct mission activities if awarded\n\nPrepare for deactivation, disassembly, decommis- ?\nsioning as planned (subject to mission extension)\n\nComplete post-flight evaluation reports ?\n\nComplete final mission report ?\n\nPerform required Phase E technical activities from  ?\nNPR 7120.5\n\nSatisfy Phase E reviews\u2019 entrance/success criteria  ?\nfrom NPR 7123.1\n\nReviews\nPLAR ?\nCERR ?\nPFAR (human space flight only) ?\nSystem upgrade review ?\nSafety review ?\n\nPhase F: Closeout\n\nPurpose\nTo implement the systems decommissioning/dis-\nposal plan developed in Phase C and analyze any re-\nturned data and samples \n\nTypical Activities and Their Products\nDispose of the system and supporting processes ?\n\nDocument lessons learned ?\n\nBaseline mission final report ?\n\nArchive data ?\n\nBegin transition of human capital (if applicable) ?\n\nPerform required Phase F technical activities from  ?\nNPR 7120.5\n\nSatisfy Phase F reviews\u2019 entrance/success criteria  ?\nfrom NPR 7123.1\n\nReviews\nDR ?\n\n\n\n3.9 Project Phase F: Closeout\n\nNASA Systems Engineering Handbook ? 29\n\nTo limit space debris, NPR 8715.6, NASA Proce-\ndural Requirements for Limiting Orbital Debris pro-\nvides guidelines for removing Earth-orbiting robotic \nsatellites from their operational orbits at the end of \ntheir useful life. For Low Earth Orbiting (LEO) mis-\nsions, the satellite is usually deorbited. For small sat-\nellites, this is accomplished by allowing the orbit to \nslowly decay until the satellite eventually burns up \nin the Earth\u2019s atmosphere. Larger, more massive sat-\nellites and observatories must be designed to demise \nor deorbited in a controlled manner so that they can \nbe safely targeted for impact in a remote area of the \nocean. The Geostationary (GEO) satellites at 35,790 \nkm above the Earth cannot be practically deorbited, \nso they are boosted to a higher orbit well beyond the \ncrowded operational GEO orbit.\n\nIn addition to uncertainty as to when this part of the \nphase begins, the activities associated with safe closeout \nof a system may be long and complex and may affect \nthe system design. Consequently, different options and \nstrategies should be considered during the project\u2019s ear-\nlier phases along with the costs and risks associated with \nthe different options. \n\n3.10 Funding: The Budget Cycle\nNASA operates with annual funding from Congress. \nThis funding results, however, from a continuous rolling \nprocess of budget formulation, budget enactment, and \nfinally, budget execution. NASA\u2019s Financial Manage-\nment Requirements (FMR) Volume 4 provides the con-\ncepts, the goals, and an overview of NASA\u2019s budget \nsystem of resource alignment referred to as Planning, \nProgramming, Budgeting, and Execution (PPBE) and \nestablishes guidance on the programming and bud-\ngeting phases of the PPBE process, which are critical to \nbudget formulation for NASA. Volume 4 includes stra-\ntegic budget planning and resources guidance, program \nreview, budget development, budget presentation, and \njustification of estimates to the Office of Management \nand Budget (OMB) and to Congress. It also provides \ndetailed descriptions of the roles and responsibilities \nfor key players in each step of the process. It consoli-\ndates current legal, regulatory, and administrative poli-\ncies and procedures applicable to NASA. A highly sim-\nplified representation of the typical NASA budget cycle \nis shown in Figure 3.10-1.\n\nEXECUTION\n\nInternal/External\nStudies and\n\nAnalysis\n\nPLANNING\n\nNASA\nStrategic\n\nPlan\n\nAnnual\nPerformance\n\nGoals\n\nImplementation\nPlanning\n\nStrategic\nPlanning\nGuidance\n\nPerformance and\nAccountability\n\nReport\n\nCloseout\n\nAnalysis of\nPerformance/\nExpenditures\n\nMonthly\nPhasing\n\nPlans\n\nOperating Plan\nand\n\nReprogramming\n\n(Pdm)\n\nPROGRAMMING\n\nProgram\nDecision\n\nMemorandum\n\nProgram\nReview/Issues\n\nBook\n\nInstitutional\nInfrastructure\n\nAnalysis\n\nProgram\nAnalysis and\nAlignment\n\nProgram and\nResource\nGuidance\n\nBUDGETING\n\nAppropriation\n\nPresident\u2019s\nBudget\n\nOMB Budget\n\nProgrammatic\nand Institutional\n\nGuidance\n\nFigure 3.10?1 Typical NASA budget cycle \n\n\n\n30 ? NASA Systems Engineering Handbook\n\n3.0 NASA Program/Project Life Cycle\n\nNASA typically starts developing its budget each Feb-\nruary with economic forecasts and general guidelines as \nidentified in the most recent President\u2019s budget. By late \nAugust, NASA has completed the planning, program-\nming, and budgeting phases of the PPBE process and \nprepares for submittal of a preliminary NASA budget \nto the OMB. A final NASA budget is submitted to the \nOMB in September for incorporation into the Pres-\nident\u2019s budget transmittal to Congress, which gener-\nally occurs in January. This proposed budget is then \nsubjected to congressional review and approval, cul-\nminating in the passage of bills authorizing NASA to \nobligate funds in accordance with congressional stip-\nulations and appropriating those funds. The congres-\n\nsional process generally lasts through the summer. In \nrecent years, however, final bills have often been de-\nlayed past the start of the fiscal year on October 1. In \nthose years, NASA has operated on continuing resolu-\ntion by Congress.\n\nWith annual funding, there is an implicit funding con-\ntrol gate at the beginning of every fiscal year. While these \ngates place planning requirements on the project and \ncan make significant replanning necessary, they are not \npart of an orderly systems engineering process. Rather, \nthey constitute one of the sources of uncertainty that af-\nfect project risks, and they are essential to consider in \nproject planning.\n\n\n\nNASA Systems Engineering Handbook ? 31\n\n4.0 System Design\n\nThis chapter describes the activities in the system de-\nsign processes listed in Figure 2.1-1. The chapter is sepa-\nrated into sections corresponding to steps 1 to 4 listed \nin Figure 2.1-1. The processes within each step are dis-\ncussed in terms of inputs, activities, and outputs. Addi-\ntional guidance is provided using examples that are rel-\nevant to NASA projects. The system design processes are \nfour interdependent, highly iterative and recursive pro-\ncesses, resulting in a validated set of requirements and a \nvalidated design solution that satisfies a set of stakeholder \nexpectations. The four system design processes are to de-\nvelop stakeholder expectations, technical requirements, \nlogical decompositions, and design solutions. \n\nFigure 4.0-1 illustrates the recursive relationship among \nthe four system design processes. These processes start \n\nwith a study team collecting and clarifying the stake-\nholder expectations, including the mission objectives, \nconstraints, design drivers, operational objectives, and \ncriteria for defining mission success. This set of stake-\nholder expectations and high-level requirements is used \nto drive an iterative design loop where a strawman ar-\nchitecture/design, the concept of operations, and de-\nrived requirements are developed. These three products \nmust be consistent with each other and will require it-\nerations and design decisions to achieve this consistency. \nOnce consistency is achieved, analyses allow the project \nteam to validate the design against the stakeholder ex-\npectations. A simplified validation asks the questions: \nDoes the system work? Is the system safe and reliable? Is \nthe system achievable within budget and schedule con-\nstraints? If the answer to any of these questions is no, \n\nFigure 4.0?1 Interrelationships among the system design processes\n\nYesNoRebaseline\nrequirements?\n\nNo\n\nMission\nAuthority\n\nYes\n\nFunctional &\nPerformance\n\nAnalysis \nYes\n\nHigh-Level\nRequirements\n\nSelect\nBaseline\n\nStart\n\nStakeholder\nExpectations \n\nMission\nObjectives &\nConstraints\n\nOperational\nObjectives\n\nMission\nSuccess\nCriteria\n\nTrade Studies and Iterative Design Loop \n\nConOps \n\nDesign and\nProduct\n\nBreakdown\nStructure \n\nDerived and\nAllocated\n\nRequirements\n? Functional\n? Performance\n? Interface\n? Operational\n? \u201cIlities\u201d\n\nWork?\nSafe & reliable?\n\nA?ordable?\n\nSu?cient\ndepth?\n\nNo ? Next Level\n\nStakeholder Expectations De?nition\n\nTechnical Requirements De?nition\n\nLogical Decomposition\n\nDesign Solution De?nition\n\nDecision Analysis\n\nLegend:\n\nFunctional\nand Logical\n\nDecomposition\n\n\n\n32 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nthen changes to the design or stakeholder expectations \nwill be required, and the process started again. This pro-\ncess continues until the system\u2014architecture, ConOps, \nand requirements\u2014meets the stakeholder expectations.\n\nThe depth of the design effort must be sufficient to allow \nanalytical verification of the design to the requirements. \nThe design must be feasible and credible when judged \nby a knowledgeable independent review team and must \nhave sufficient depth to support cost modeling. \n\nOnce the system meets the stakeholder expectations, the \nstudy team baselines the products and prepares for the \nnext phase. Often, intermediate levels of decomposition \nare validated as part of the process. In the next level of \ndecomposition, the baselined derived (and allocated) re-\nquirements become the set of high-level requirements \nfor the decomposed elements and the process begins \nagain. These system design processes are primarily ap-\nplied in Pre-Phase A and continue through Phase C.\n\nThe system design processes during Pre-Phase A focus \non producing a feasible design that will lead to Formula-\ntion approval. During Phase A, alternative designs and \nadditional analytical maturity are pursued to optimize \nthe design architecture. Phase B results in a prelimi-\nnary design that satisfies the approval criteria. During \nPhase C, detailed, build-to designs are completed. \n\nThis has been a simplified description intended to dem-\nonstrate the recursive relationship among the system de-\nsign processes. These processes should be used as guid-\nance and tailored for each study team depending on the \nsize of the project and the hierarchical level of the study \n\nteam. The next sections describe each of the four system \ndesign processes and their associated products for a \ngiven NASA mission.\n\nSystem Design Keys\n\nSuccessfully understanding and defining the mis- ?\nsion objectives and operational concepts are keys \nto capturing the stakeholder expectations, which \nwill translate into quality requirements over the life \ncycle of the project.\n\nComplete and thorough requirements traceability  ?\nis a critical factor in successful validation of require-\nments.\n\nClear and unambiguous requirements will help  ?\navoid misunderstanding when developing the \noverall system and when making major or minor \nchanges.\n\nDocument all decisions made during the develop- ?\nment of the original design concept in the techni-\ncal data package. This will make the original design \nphilosophy and negotiation results available to  \nassess future proposed changes and modifications \nagainst.\n\nThe design solution verification occurs when an  ?\nacceptable design solution has been selected and \ndocumented in a technical data package. The de-\nsign solution is verified against the system require-\nments and constraints. However, the validation of \na design solution is a continuing recursive and it-\nerative process during which the design solution is \nevaluated against stakeholder expectations.\n\n\n\nNASA Systems Engineering Handbook ? 33\n\nThe Stakeholder Expectations Definition Process is the ini-\ntial process within the SE engine that establishes the foun-\ndation from which the system is designed and the product \nis realized. The main purpose of this process is to identify \nwho the stakeholders are and how they intend to use the \nproduct. This is usually accomplished through use-case sce-\nnarios, Design Reference Missions (DRMs), and ConOps.\n\n4.1.1 Process Description\nFigure 4.1-1 provides a typical flow diagram for the \nStakeholder Expectations Definition Process and identi-\nfies typical inputs, outputs, and activities to consider in \naddressing stakeholder expectations definition.\n\n4.1.1.1 Inputs\nTypical inputs needed for the Stakeholder Expectations \nDefinition Process would include the following:\n\nUpper Level Requirements and Expectations: ?  These \nwould be the requirements and expectations (e.g., \nneeds, wants, desires, capabilities, constraints, ex-\nternal interfaces) that are being flowed down to a par-\nticular system of interest from a higher level (e.g., pro-\ngram, project, etc.).\nIdentified Customers and Stakeholders: ?  The organi-\nzation or individual who has requested the product(s) \nand those who are affected by or are in some way ac-\ncountable for the product\u2019s outcome.\n\n4.1.1.2 Process Activities\n\nIdentifying Stakeholders\n\nAdvocacy for new programs and projects may originate in \nmany organizations. These include Presidential directives, \nCongress, NASA Headquarters (HQ), the NASA Centers, \nNASA advisory committees, the National Academy of Sci-\n\n4.1 Stakeholder Expectations Definition\n\nCustomer Flowdown \nRequirements\n\nInitial Customer\nExpectations \n\nEnabling Product\nSupport Strategies\n\nConcept of Operations\n\nMeasures of Effectiveness \n\nValidated Stakeholder\nExpectations\n\nOther Stakeholder\nExpectations \n\nFrom project\n\nFrom Design Solution\nDe?nition (recursive loop) and\n\nRequirements Management and \nInterface Management Processes   \n\nTo Technical\nRequirements De?nition and\n\nRequirements Management and \nInterface Management Processes\n\nTo Technical Requirements\nDe?nition and Technical Data \n\nManagement Processes \n\nTo Technical\nRequirements De?nition\n\nand Con?guration\nManagement Processes \n\nEstablish list of stakeholders\n\nDefine stakeholder expectations in acceptable\nstatements \n\nValidate that defined expectation statements \nreflect bidirectional traceability\n\nElicit stakeholder expectations\n\nEstablish operations concept and support\nstrategies\n\nAnalyze expectation statements for measures \nof effectiveness\n\nBaseline stakeholder expectations\n\nObtain stakeholder commitments to the \nvalidated set of expectations\n\nFigure 4.1?1 Stakeholder Expectations Definition Process\n\n\n\n34 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nences, the National Space Council, and many other groups \nin the science and space communities. These organizations \nare commonly referred to as stakeholders. A stakeholder is \na group or individual who is affected by or is in some way \naccountable for the outcome of an undertaking.\n\nStakeholders can be classified as customers and other \ninterested parties. Customers are those who will receive \nthe goods or services and are the direct beneficiaries of \nthe work. Examples of customers are scientists, project \nmanagers, and subsystems engineers.\n\nOther interested parties are those who affect the project \nby providing broad, overarching constraints within \nwhich the customers\u2019 needs must be achieved. These par-\nties may be affected by the resulting product, the manner \nin which the product is used, or have a responsibility for \nproviding life-cycle support services. Examples include \nCongress, advisory planning teams, program managers, \nusers, operators, maintainers, mission partners, and \nNASA contractors. It is important that the list of stake-\nholders be identified early in the process, as well as the \nprimary stakeholders who will have the most significant \ninfluence over the project.\n\nIdentifying Stakeholder Expectations\nStakeholder expectations, the vision of a particular stake-\nholder individual or group, result when they specify what is \ndesired as an end state or as an item to be produced and put \nbounds upon the achievement of the goals. These bounds \nmay encompass expenditures (resources), time to deliver, \n\nperformance objectives, or other less obvious quantities \nsuch as organizational needs or geopolitical goals.\n\nFigure 4.1-2 shows the type of information needed when \ndefining stakeholder expectations and depicts how the \ninformation evolves into a set of high-level require-\nments. The yellow paths depict validation paths. Exam-\nples of the types of information that would be defined \nduring each step are also provided.\n\nDefining stakeholder expectations begins with the mis-\nsion authority and strategic objectives that the mission is \nmeant to achieve. Mission authority changes depending \non the category of the mission. For example, science mis-\nsions are usually driven by NASA Science Mission Di-\nrectorate strategic plans; whereas the exploration mis-\nsions may be driven by a Presidential directive. \n\nAn early task in defining stakeholder expectations is \nunderstanding the objectives of the mission. Clearly de-\nscribing and documenting them helps ensure that the \nproject team is working toward a common goal. These \nobjectives form the basis for developing the mission, so \nthey need to be clearly defined and articulated. \n\nDefining the objectives is done by eliciting the needs, \nwants, desires, capabilities, external interfaces, assump-\ntions, and constraints from the stakeholders. Arriving \nat an agreed-to set of objectives can be a long and ar-\nduous task. The proactive iteration with the stakeholders \nthroughout the systems engineering process is the way \n\nMission\nAuthority\n\nMission\nObjectives \n\nSuccess\nCriteria \n\nDesign\nDrivers \n\n? Agency Strategic\nPlans\n? Announcements of\n\nOpportunity\n? Road Maps\n? Directed Missions\n\n? Science Objectives\n\n? Exploration \nObjectives \n? Technology \n\nDemonstration\nObjectives\n? Technology \n\nDevelopment\nObjectives\n\n? Programmatic\nObjectives \n\nOperational\nObjectives \n\nOperational Drivers\n\n? What measurements?\n? How well?   \n\nMeasurements\n\nExplorations\n\nMission Drivers\n\n? Integration and Test\n? Launch\n? On-Orbit\n? Transfer\n? Surface\n? Science Data\n\nDistribution\n? . . . ? What explorations?\n\n? What goals?   \n\n? Launch Date\n? Mission Duration\n? Orbit\n? . . .\n\nFigure 4.1?2 Product flow for stakeholder expectations\n\n\n\n4.1 Stakeholder Expectations Definition\n\nNASA Systems Engineering Handbook ? 35\n\nthat all parties can come to a true understanding of what \nshould be done and what it takes to do the job. It is im-\nportant to know who the primary stakeholders are and \nwho has the decision authority to help resolve conflicts.\n\nThe project team should also identify the constraints \nthat may apply. A constraint is a condition that must be \nmet. Sometimes a constraint is dictated by external fac-\ntors such as orbital mechanics or the state of technology; \nsometimes constraints are the result of the overall budget \nenvironment. It is important to document the constraints \nand assumptions along with the mission objectives. \n\nOperational objectives also need to be included in de-\nfining the stakeholder expectations. The operational ob-\njectives identify how the mission must be operated to \nachieve the mission objectives.\n\nThe mission and operational success criteria define what \nthe mission must accomplish to be successful. This will \nbe in the form of a measurement concept for science \nmissions and exploration concept for human explora-\ntion missions. The success criteria also define how well \nthe concept measurements or exploration activities must \nbe accomplished. The success criteria capture the stake-\nholder expectations and, along with programmatic re-\nquirements and constraints, are used within the high-\nlevel requirements. \n\nThe design drivers will be strongly dependent upon the \nConOps, including the operational environment, orbit, \nand mission duration requirements. For science mis-\nsions, the design drivers may include, at a minimum, the \nmission launch date, duration, and orbit. If alternative \norbits are to be considered, a separate concept is needed \nfor each orbit. Exploration missions must consider the \ndestination, the duration, the operational sequence (and \nsystem configuration changes), and the in situ explora-\ntion activities that allow the exploration to succeed.\n\nThe end result of this step is the discovery and delineation \nof the system\u2019s goals, which generally express the agree-\n\nments, desires, and requirements of the eventual users of \nthe system. The high-level requirements and success cri-\nteria are examples of the products representing the con-\nsensus of the stakeholders. \n\n4.1.1.3 Outputs\nTypical outputs for capturing stakeholder expectations \nwould include the following:\n\nTop-Level Requirements and Expectations: ?  These \nwould be the top-level requirements and expectations \n(e.g., needs, wants, desires, capabilities, constraints, \nand external interfaces) for the product(s) to be de-\nveloped.\nConOps: ?  This describes how the system will be oper-\nated during the life-cycle phases to meet stakeholder \nexpectations. It describes the system characteris-\ntics from an operational perspective and helps facili-\ntate an understanding of the system goals. Examples \nwould be the ConOps document or a DRM.\n\n4.1.2 Stakeholder Expectations Definition \nGuidance\n\n4.1.2.1 Concept of Operations\nThe ConOps is an important component in capturing \nstakeholder expectations, requirements, and the archi-\ntecture of a project. It stimulates the development of \nthe requirements and architecture related to the user \nelements of the system. It serves as the basis for subse-\nquent definition documents such as the operations plan, \nlaunch and early orbit plan, and operations handbook \nand provides the foundation for the long-range opera-\ntional planning activities such as operational facilities, \nstaffing, and network scheduling. \n\nThe ConOps is an important driver in the system re-\nquirements and therefore must be considered early \nin the system design processes. Thinking through the \nConOps and use cases often reveals requirements and \ndesign functions that might otherwise be overlooked. A \nsimple example to illustrate this point is adding system \nrequirements to allow for communication during a par-\nticular phase of a mission. This may require an additional \nantenna in a specific location that may not be required \nduring the nominal mission.\n\nThe ConOps is important for all projects. For science \nprojects, the ConOps describes how the systems will be \noperated to achieve the measurement set required for a \n\nNote: It is extremely important to involve stakehold-\ners in all phases of a project. Such involvement should \nbe built in as a self-correcting feedback loop that will \nsignificantly enhance the chances of mission success. \nInvolving stakeholders in a project builds confidence \nin the end product and serves as a validation and ac-\nceptance with the target audience.\n\n\n\n36 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nsuccessful mission. They are usually driven by the data \nvolume of the measurement set. The ConOps for explo-\nration projects is likely to be more complex. There are \ntypically more operational phases, more configuration \nchanges, and additional communication links required \nfor human interaction. For human spaceflight, functions \nand objectives must be clearly allocated between human \noperators and systems early in the project. \n\nThe ConOps should consider all aspects of operations \nincluding integration, test, and launch through disposal. \nTypical information contained in the ConOps includes \na description of the major phases; operation timelines; \noperational scenarios and/or DRM; end-to-end commu-\nnications strategy; command and data architecture; op-\nerational facilities; integrated logistic support (resupply, \nmaintenance, and assembly); and critical events. The op-\nerational scenarios describe the dynamic view of the sys-\ntems\u2019 operations and include how the system is perceived \nto function throughout the various modes and mode \ntransitions, including interactions with external inter-\n\nfaces. For exploration missions, multiple DRMs make up \na ConOps. The design and performance analysis leading \nto the requirements must satisfy all of them. Figure 4.1-3 \n\nMission Operations Center\n\nInstrument \nSOC\n\nAcquisition Data \nObservatory Commands \n\nTracking Data \nStation Status \n\nInstrument Commands/Leads\n\nData Distribution\nSystem\n\nStation Control \n\nSame Interfaces\nas Prime Ground Site \n\nExternal\nTracking Station\n\nAcquisition\nData\nCmd\n\nTrending\nGround Station\nControl System\n\nInstrument \nSOC\n\nS-Band: \nTRK, Cmd, and HK Tlm\n\nDDS\nControl System\n\nAlert Noti?cation\nSystem\n\nFlight Software\nMaintenance Lab\n\nFlight Software\nMemory Dumps Simulated\n\nCommands\n\nInstrument \nSOC\n\nInstrument\n#1\n\nScience Data\n55 Mbps\n\nScience Planning\nand FDS Products\n\n R/T Housekeeping Telemetry \n R/T Housekeeping Telemetry \n R/T Housekeeping Telemetry \n\nMission Planning\nahd Scheduling\n\nPlan daily/periodic events\nCreate engineering plan\nGenerate daily loads \n\nFlight Dynamics\nSystem\n\nOrbit Determination\nManuever Planning\nProduct Generation\nR/T Attitude Determination\nSensor/Actuator Calibration \n\nTelemetry and\nCommand System\n\nASIST/FEDS\nTelemetry Monitoring\nCommand Management\nHK Data Archival\nHK Level-0 Processing\nAutomated Operations\nAnomaly Detection  DDS Control \n\n DDS Status (Including Short-\nTerm Science\nData Storage)\n\n Ground Site #1\nS-Band\n\nGround System\n(Including Short-Term\nS-Band Data Storage)\n\nKa-Band\nGround System\n\nKa\nScience\n\nData\n\nObservatory Housekeeping Telemetry \n\n Ground Site #2\nS-Band\n\nGround System\n(Including Short-Term\nS-Band Data Storage)\n\nKa-Band\nGround System\n\nS-Band: \nTRK, Cmd, and HK Tlm\n\nKa-Band: \n150 Mbps\n\nScience Data\n\nKa-Band: \n150 Mbps\n\nScience Data\n\nS-Band:\nHK Tlm, TRK Data\n\nInstrument\n#2\n\nScience Data\n2 Mbps\n\nInstrument\n#3\n\nScience Data\n58 Mbps\n\nS-Band: \nTRK, Cmd,\n\nand HK Tlm\n\nFigure 4.1?4 Example of an associated end?to?end operational architecture \n\nFigure 4.1?3 Typical ConOps development for a \nscience mission\n\nDevelop\n operational\n\nrequire-\nments\n\nDefine\nend-to-end\ncommuni-\n\ncation\nlinks\n\nIdentify\noperational\n\nfacilities\n\nDevelop\norgani-\nzational\n\nresponsi-\nbilities\n\nDevelop\nproject\n\noperations\ntimeline\n\nDevelop\ncritical\nevents\n\nDevelop\noperational\nconfigura-\n\ntions\n\nDefine\noperational\n\nlaunch\nsegment\ndrivers\n\nDefine\noperational\n\nground\nsegment\ndrivers\n\nDefine\noperational\n\nflight\nsegment\ndrivers\n\n\n\n4.1 Stakeholder Expectations Definition\n\nNASA Systems Engineering Handbook ? 37\n\nillustrates typical information included in the ConOps \nfor a science mission, and Figure 4.1-4 is an example of \nan end-to-end operational architecture. For more infor-\nmation about developing the ConOps, see ANSI/AIAA \nG-043-1992, Guide for the Preparation of Operational \nConcept Documents. \n\nThe operation timelines provide the basis for defining \nsystem configurations, operational activities, and other \nsequenced related elements necessary to achieve the \nmission objectives for each operational phase. It de-\nscribes the activities, tasks, and other sequenced related \nelements necessary to achieve the mission objectives in \neach of the phases. Depending on the type of project \n(science, exploration, operational), the timeline could \nbecome quite complex.\n\nThe timeline matures along with the design. It starts as \na simple time-sequenced order of the major events and \nmatures into a detailed description of subsystem oper-\nations during all major mission modes or transitions. \nAn example of a lunar sortie timeline and DRM early in \nthe life cycle are shown in Figures 4.1-5a and b, respec-\ntively. An example of a more detailed, integrated time-\n\nFigure 4.1?5a Example of a lunar sortie timeline \ndeveloped early in the life cycle\n\nLaunch Operations\n\nLEO Operations\n\nLunar Orbit Operations\n\nReentry and Landing Operations\n\nLunar Surface Operations\n\nLunar Transfer Operations\n\nEarth Transfer Operations\n\nIntegration and Test\n\n0 1 2 3 4\nElapsed Time (Weeks)\n\nFigure 4.1?5b Example of a lunar sortie DRM early in the life cycle\n\n100 km\nLow Lunar Orbit\n\nLow Earth\nOrbit\n\nAscent Stage\n Expended\n\nLunar\nSurface\nAccess\nModule\n(LSAM) Crew Exploration Vehicle\n\nEarth Departure \nStage Expended\n\nDirect or Skip \nLand Entry\n\nEarth\n\nMoon\n\nLSAM Performs Lunar Orbit Injection\n\nEarth Departure Stage\n\nline later in the life cycle for a science mission is shown \nin Figure 4.1-6.\n\nAn important part of the ConOps is defining the op-\nerational phases, which will span project Phases D, E, \nand F. The operational phases provide a time-sequenced \n\n\n\n38 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nCoverage\n\nAtlas EELV\n\nGN&C\n\nPropulsion\n\nC&DH/RF\n\nPower/\nElectricity\n\nDeployables\n\nThermal\n\nInstruments\n\nGround\n\nUSN Sites\n\nTDRs\n\nControl Mode\n\nRecorder\nS-Downlink\n\nIRUs On\nACE-A On\nRWAs O?\nSTs O?\nDSS O?\nACS thr ISo Valves Open\n\nS/C Load\n\nS-XMTR\n\nMOC\n\nL-0.5 Launch Sep L+1 L+1.5\n\nUpdate on-board EPV\n\nDecontamination Heaters On\n\nGCE Powered On\n\nFDF Receives MECO State\nVector From EELV\n\n1 Hour Launch Window\n\nHMI EB & OP O?\nAIA EB & Op O?\nEVE EB & Op O?\n\nSurvival Heaters Enabled\n\nSADS & HGADS Damper Heaters On Solar Array Deployment\n\nDetect Separation Then:\n- Initiate SA Deployment\n- Power on RWAs\n\nXMTR on Coherent\n\nSun Acquisition\nComplete\n\n616 W 656 W\n804 W\n\n846 W\n\n551 W\n\n1095 W\n\n293 W258 W\n\nRecord 16 kbps\n\n64 kbps\nHardline\n\n2 kbps\nHardline\n\n100% SOC\n\nAcquire Sun\n\nSun Acquisition Mode\n\n(If High Tipo? Rates,\nDelta-H If Needed\nby Ground Command Only)\n\nRWAs Powered On\nNull Rates\n(Wheels)\n\nStart XMTR On\nSequencer (RTS)\n\nFairing\nJettison\n\nMECO SECO-1\nCoast\n\n& 1RPM Roll\nSECO-2\n\n12:30Z12:00Z\nSeparation:\n11:30:58Z10:59:13Z\n\n4/18/08\n9:59:13Z\n\n(Hrs)\nLaunch\n\nOverburg, S. Africa - 25 mins (11:30:46Z - 11:56:04Z)\n\nDongara, Australia - 9.4 hrs (11:45:38Z - 21:00:23z)\n\nDump Recorder\n\n240 kbps on Carrier\n\nPrep Lines & HGA Survival\nHeaters Come On\n\n>95% SOC\n\nL-5 mins\nSAS O?\nGo to Internal Power\n\n>95% Battery SOC Launch Criteria\n- Once on internal Power, >95% Allows \n>15 minutes Before Launch or Scrub\n\nXMTR on Noncoherent\nvia Stored Command\n\n2 kbps on Carrier\n\n2 kbps Command Uplink\n\nLV to Sep\nAttitude\n\nSeparation\nL-31 mins 45 secs\nAltitude = 300 km\n\nTDRS: Prime at Sep Then Backup/Contingency Only\n\nNote: Nominally Acquire TDRS Pre-Sep and Stay on Until Acquisition at Dongara\n\nACE-B Powered On\n\n64 kbps on Subcarrier/Ranging on Carrier\n\nPower Positive\nCharge Battery\n@ 10 Amps\n\nFigure 4.1?6 Example of a more detailed, integrated timeline later in the life cycle for a science mission\n\nstructure for defining the configuration changes and op-\nerational activities needed to be carried out to meet the \ngoals of the mission. For each of the operational phases, \n\nfacilities, equipment, and critical events should also be \nincluded. Table 4.1-1 identifies some common examples \nof operational phases for a NASA mission.\n\n\n\n4.1 Stakeholder Expectations Definition\n\nNASA Systems Engineering Handbook ? 39\n\nTable 4.1?1 Typical Operational Phases for a NASA Mission\n\nOperational Phase Description\n\nIntegration and test \noperations\n\nProject Integration and Test: During the latter period of project integration and test, the system \nis tested by performing operational simulations during functional and environmental testing. The \nsimulations typically exercise the end-to-end command and data system to provide a complete veri-\nfication of system functionality and performance against simulated project operational scenarios.\n\nLaunch Integration: The launch integration phase may repeat integration and test operational and \nfunctional verification in the launch-integrated configuration.\n\nLaunch operations Launch: Launch operation occurs during the launch countdown, launch ascent, and orbit injection. \nCritical event telemetry is an important driver during this phase.\n\nDeployment: Following orbit injection, spacecraft deployment operations reconfigure the space-\ncraft to its orbital configuration. Typically, critical events covering solar array, antenna, and other \ndeployments and orbit trim maneuvers occur during this phase.\n\nIn?Orbit Checkout: In-orbit checkout is used to perform a verification that all systems are healthy. \nThis is followed by on-orbit alignment, calibration, and parameterization of the flight systems to \nprepare for science operations.\n\nScience operations The majority of the operational lifetime is used to perform science operations.\n\nSafe-hold  \noperations\n\nAs a result of on-board fault detection or by ground command, the spacecraft may transition to a \nsafe-hold mode. This mode is designed to maintain the spacecraft in a power positive, thermally \nstable state until the fault is resolved and science operations can resume.\n\nAnomaly resolution \nand maintenance \noperations\n\nAnomaly resolution and maintenance operations occur throughout the mission. They may require \nresources beyond established operational resources. \n\nDisposal operations Disposal operations occur at the end of project life. These operations are used to either provide a \ncontrolled reentry of the spacecraft or a repositioning of the spacecraft to a disposal orbit. In the \nlatter case, the dissipation of stored fuel and electrical energy is required.\n\n\n\n40 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nThe Technical Requirements Definition Process trans-\nforms the stakeholder expectations into a definition of \nthe problem and then into a complete set of validated \ntechnical requirements expressed as \u201cshall\u201d statements \nthat can be used for defining a design solution for the \nProduct Breakdown Structure (PBS) model and related \nenabling products. The process of requirements definition \nis a recursive and iterative one that develops the stake-\nholders\u2019 requirements, product requirements, and lower \n\nlevel product/component requirements (e.g., PBS model \nproducts such as systems or subsystems and related en-\nabling products such as external systems that provide or \nconsume data). The requirements should enable the de-\nscription of all inputs, outputs, and required relationships \nbetween inputs and outputs. The requirements documents \norganize and communicate requirements to the customer \nand other stakeholders and the technical community.\n\nTechnical requirements definition activities apply to the \ndefinition of all technical requirements from the pro-\ngram, project, and system levels down to the lowest level \nproduct/component requirements document. \n\n4.2.1 Process Description\nFigure 4.2-1 provides a typical flow diagram for the \nTechnical Requirements Definition Process and identi-\nfies typical inputs, outputs, and activities to consider in \naddressing technical requirements definition.\n\n4.2 Technical Requirements Definition\n\nIt is important to note that the team must not rely \nsolely on the requirements received to design and \nbuild the system. Communication and iteration with \nthe relevant stakeholders are essential to ensure a \nmutual understanding of each requirement. Other-\nwise, the designers run the risk of misunderstanding \nand implementing an unwanted solution to a differ-\nent interpretation of the requirements. \n\nTechnical Performance\nMeasures \n\nBaselined Stakeholder \nExpectations Validated Technical\n\nRequirements \n\nBaselined Concept of\nOperations \n\nMeasures of\nEffectiveness \n\nBaselined Enabling\nSupport Strategies \n\nFrom Stakeholder\nExpectations De?nition\n\nand Con?guration \nManagement Processes  \n\nMeasures of\nPerformance \n\nTo Logical Decomposition\nand Technical Data\n\nManagement Processes  \n\nTo Technical\nAssessment Process \n\nTo Logical Decomposition\nand Requirements\n\nManagement and Interface\nManagement Processes   \n\nFrom Stakeholder\nExpectations De?nition\n\nand Technical Data\nManagement Processes\n\nDefine performance\nrequirements for each\ndefined functional and\n\nbehavioral expectation  \n\nDefine technical\nperformance measures \n\nDefine design and\nproduct constraints \n\nValidate technical\nrequirements \n\nEstablish technical\nrequirements baseline  \n\nDefine functional and \nbehavioral expectation in\n\ntechnical terms \n\nAnalyze scope of problem\n\nDefine technical require-\nments in acceptable \n\n\u201cshall\u201d statements \n\nDefine measures of\nperformance for each\n\nmeasure of effectiveness\n\nFigure 4.2?1 Technical Requirements Definition Process\n\n\n\n4.2 Technical Requirements Definition\n\nNASA Systems Engineering Handbook ? 41\n\n4.2.1.1 Inputs\nTypical inputs needed for the requirements process \nwould include the following:\n\nTop-Level Requirements and Expectations: ?  These \nwould be the agreed-to top-level requirements and \nexpectations (e.g., needs, wants, desires, capabilities, \nconstraints, external interfaces) for the product(s) to \nbe developed coming from the customer and other \nstakeholders.\nConcept of Operations: ?  This describes how the \nsystem will be operated during the life-cycle phases to \nmeet stakeholder expectations. It describes the system \ncharacteristics from an operational perspective and \nhelps facilitate an understanding of the system goals. \nExamples would be a ConOps document or a DRM.\n\n4.2.1.2 Process Activities\nThe top-level requirements and expectations are initial-\nly assessed to understand the technical problem to be \nsolved and establish the design boundary. This bound-\nary is typically established by performing the following \nactivities:\n\nDefining constraints that the design must adhere to or  ?\nhow the system will be used. The constraints are typically \nnot able to be changed based on tradeoff analyses.\nIdentifying those elements that are already under de- ?\nsign control and cannot be changed. This helps es-\ntablish those areas where further trades will be per-\nformed to narrow potential design solutions.\nEstablishing physical and functional interfaces (e.g.,  ?\nmechanical, electrical, thermal, human, etc.) with \nwhich the system must interact. \nDefining functional and behavioral expectations for  ?\nthe range of anticipated uses of the system as identified \nin the ConOps. The ConOps describes how the system \nwill be operated and the possible use-case scenarios.\n\nWith an overall understanding of the constraints, phys-\nical/functional interfaces, and functional/behavioral ex-\npectations, the requirements can be further defined by \nestablishing performance criteria. The performance is \nexpressed as the quantitative part of the requirement to \nindicate how well each product function is expected to \nbe accomplished.\n\nFinally, the requirements should be defined in accept-\nable \u201cshall\u201d statements, which are complete sentences \nwith a single \u201cshall\u201d per statement. See Appendix C for \n\nguidance on how to write good requirements and Ap-\npendix E for validating requirements. A well-written \nrequirements document provides several specific bene-\nfits to both the stakeholders and the technical team, as \nshown in Table 4.2-1.\n\n4.2.1.3 Outputs\nTypical outputs for the Technical Requirements Defini-\ntion Process would include the following:\n\nTechnical Requirements: ?  This would be the approved \nset of requirements that represents a complete descrip-\ntion of the problem to be solved and requirements that \nhave been validated and approved by the customer and \nstakeholders. Examples of documentation that capture \nthe requirements are a System Requirements Docu-\nment (SRD), Project Requirements Document (PRD), \nInterface Requirements Document (IRD), etc.\nTechnical Measures: ?  An established set of measures \nbased on the expectations and requirements that will \nbe tracked and assessed to determine overall system \nor product effectiveness and customer satisfaction. \nCommon terms for these measures are Measures \nof Effectiveness (MOEs), Measures of Performance \n(MOPs), and Technical Performance Measures \n(TPMs). See Section 6.7 for further details.\n\n4.2.2 Technical Requirements Definition \nGuidance\n\n4.2.2.1 Types of Requirements\nA complete set of project requirements includes the \nfunctional needs requirements (what functions need to \nbe performed), performance requirements (how well \nthese functions must be performed), and interface re-\nquirements (design element interface requirements). For \nspace projects, these requirements are decomposed and \nallocated down to design elements through the PBS.\n\nFunctional, performance, and interface requirements \nare very important but do not constitute the entire set \nof requirements necessary for project success. The space \nsegment design elements must also survive and con-\ntinue to perform in the project environment. These en-\nvironmental drivers include radiation, thermal, acoustic, \nmechanical loads, contamination, radio frequency, and \nothers. In addition, reliability requirements drive design \nchoices in design robustness, failure tolerance, and re-\ndundancy. Safety requirements drive design choices in \nproviding diverse functional redundancy. Other spe-\n\n\n\n42 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\ncialty requirements also may affect design choices. These \nmay include producibility, maintainability, availability, \nupgradeability, human factors, and others. Unlike func-\ntional needs requirements, which are decomposed and \nallocated to design elements, these requirements are \nlevied across major project elements. Designing to meet \nthese requirements requires careful analysis of design \nalternatives. Figure 4.2-2 shows the characteristics of \nfunctional, operational, reliability, safety, and specialty \nrequirements. Top-level mission requirements are gener-\nated from mission objectives, programmatic constraints, \nand assumptions. These are normally grouped into func-\ntion and performance requirements and include the cat-\negories of requirements in Figure 4.2-2.\n\nFunctional Requirements\nThe functional requirements need to be specified for \nall intended uses of the product over its entire lifetime. \nFunctional analysis is used to draw out both functional \nand performance requirements. Requirements are par-\ntitioned into groups, based on established criteria (e.g., \nsimilar functionality, performance, or coupling, etc.), \nto facilitate and focus the requirements analysis. Func-\n\ntional and performance requirements are allocated to \nfunctional partitions and subfunctions, objects, people, \nor processes. Sequencing of time-critical functions is \nconsidered. Each function is identified and described \nin terms of inputs, outputs, and interface requirements \nfrom the top down so that the decomposed functions are \nrecognized as part of larger functional groupings. Func-\ntions are arranged in a logical sequence so that any speci-\nfied operational usage of the system can be traced in an \nend-to-end path to indicate the sequential relationship of \nall functions that must be accomplished by the system.\n\nIt is helpful to walk through the ConOps and scenarios \nasking the following types of questions: what functions \nneed to be performed, where do they need to be per-\nformed, how often, under what operational and environ-\n\nTable 4.2?1 Benefits of Well?Written Requirements\n\nBenefit Rationale\n\nEstablish the basis for agree-\nment between the stakehold-\ners and the developers on \nwhat the product is to do\n\nThe complete description of the functions to be performed by the product specified in the \nrequirements will assist the potential users in determining if the product specified meets \ntheir needs or how the product must be modified to meet their needs. During system \ndesign, requirements are allocated to subsystems (e.g., hardware, software, and other \nmajor components of the system), people, or processes.\n\nReduce the development \neffort because less rework is \nrequired to address poorly \nwritten, missing, and misun-\nderstood requirements\n\nThe Technical Requirements Definition Process activities force the relevant stakeholders \nto consider rigorously all of the requirements before design begins. Careful review of the \nrequirements can reveal omissions, misunderstandings, and inconsistencies early in the \ndevelopment cycle when these problems are easier to correct thereby reducing costly \nredesign, remanufacture, recoding, and retesting in later life-cycle phases.\n\nProvide a basis for estimating \ncosts and schedules \n\nThe description of the product to be developed as given in the requirements is a realistic \nbasis for estimating project costs and can be used to evaluate bids or price estimates.\n\nProvide a baseline for valida-\ntion and verification\n\nOrganizations can develop their validation and verification plans much more productively \nfrom a good requirements document. Both system and subsystem test plans and proce-\ndures are generated from the requirements. As part of the development, the requirements \ndocument provides a baseline against which compliance can be measured. The require-\nments are also used to provide the stakeholders with a basis for acceptance of the system.\n\nFacilitate transfer The requirements make it easier to transfer the product to new users or new machines. \nStakeholders thus find it easier to transfer the product to other parts of their organization, \nand developers find it easier to transfer it to new stakeholders or reuse it.\n\nServe as a basis for enhance-\nment \n\nThe requirements serve as a basis for later enhancement or alteration of the finished \nproduct.\n\nFunctional requirements define what functions need \nto be done to accomplish the objectives.\n\nPerformance requirements define how well the sys-\ntem needs to perform the functions.\n\n\n\n4.2 Technical Requirements Definition\n\nNASA Systems Engineering Handbook ? 43\n\nmental conditions, etc. Thinking through this process \noften reveals additional functional requirements.\n\nPerformance Requirements\nPerformance requirements quantitatively define how \nwell the system needs to perform the functions. Again, \nwalking through the ConOps and the scenarios often \ndraws out the performance requirements by asking the \nfollowing types of questions: how often and how well, \nto what accuracy (e.g., how good does the measure-\nment need to be), what is the quality and quantity of the \noutput, under what stress (maximum simultaneous data \n\nrequests) or environmental conditions, for what dura-\ntion, at what range of values, at what tolerance, and at \nwhat maximum throughput or bandwidth capacity.\n\nOperational Requirements \u2013 \nDrive Functional Requirements \n\nReliability Requirements \u2013 Project Standards \u2013 \nLevied Across Systems \n\nMission Timeline Sequence\nMission Configurations\nCommand and Telemetry Strategy\n\nSpecialty Requirements \u2013 Project Standards \u2013 \nDrive Product Designs \n\nProducibility\nMaintainability\nAsset Protection\n\u2026\n\nSafety Requirements \u2013 Project Standards \u2013 \nLevied Across Systems\n\nOrbital Debris and Reentry\nPlanetary Protection\nToxic Substances\nPressurized Vessels\nRadio Frequency Energy\nSystem Safety\n\u2026\n\nMission Environments\nRobustness, Fault Tolerance, Diverse Redundancy\nVerification\nProcess and Workmanship\n\nFunctional Requirements\nPerformance Requirements\nInterface Requirements\n\nTechnical Requirements \u2013 \nAllocation Hierarchically to PBS\n\nFigure 4.2?2 Characteristics of functional, \noperational, reliability, safety, and specialty \n\nrequirements\n\nExample of Functional and Performance \nRequirements\n\nInitial Function Statement\nThe Thrust Vector Controller (TVC) shall provide vehi-\ncle control about the pitch and yaw axes.\n\nThis statement describes a high-level function that \nthe TVC must perform. The technical team needs to \ntransform this statement into a set of design-to func-\ntional and performance requirements. \n\nFunctional Requirements with Associated \nPerformance Requirements\n\nThe TVC shall gimbal the engine a maximum of  ?\n9 degrees, \u00b1 0.1 degree.\n\nThe TVC shall gimbal the engine at a maximum rate  ?\nof 5 degrees/second \u00b1 0.3 degrees/second.\n\nThe TVC shall provide a force of 40,000 pounds,  ?\n\u00b1 500 pounds.\n\nThe TVC shall have a frequency response of 20 Hz,  ?\n\u00b1 0.1 Hz.\n\nBe careful not to make performance requirements too \nrestrictive. For example, for a system that must be able \nto run on rechargeable batteries, if the performance re-\nquirements specify that the time to recharge should be \nless than 3 hours when a 12-hour recharge time would \nbe sufficient, potential design solutions are eliminated. \nIn the same sense, if the performance requirements \nspecify that a weight must be within \u00b10.5 kg, when \n\u00b12.5 kg is sufficient, metrology cost will increase with-\nout adding value to the product. \n\nWherever possible, define the performance requirements \nin terms of (1) a threshold value (the minimum accept-\nable value needed for the system to carry out its mission) \nand (2) the baseline level of performance desired. Speci-\nfying performance in terms of thresholds and baseline \nrequirements provides the system designers with trade \nspace in which to investigate alternative designs.\n\nAll qualitative performance expectations must be ana-\nlyzed and translated into quantified performance require-\nments. Trade studies often help quantify performance \nrequirements. For example, tradeoffs can show whether \n\n\n\n44 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\na slight relaxation of the performance requirement could \nproduce a significantly cheaper system or whether a few \nmore resources could produce a significantly more effec-\ntive system. The rationale for thresholds and goals should \nbe documented with the requirements to understand the \nreason and origin for the performance requirement in \ncase it must be changed. The performance requirements \nthat can be quantified by or changed by tradeoff analysis \nshould be identified. See Section 6.8, Decision Analysis, \nfor more information on tradeoff analysis.\n\nInterface Requirements\n\nIt is important to define all interface requirements for the \nsystem, including those to enabling systems. The external \ninterfaces form the boundaries between the product and \nthe rest of the world. Types of interfaces include: operational \ncommand and control, computer to computer, mechanical, \nelectrical, thermal, and data. One useful tool in defining in-\nterfaces is the context diagram (see Appendix F), which de-\npicts the product and all of its external interfaces. Once the \nproduct components are defined, a block diagram showing \nthe major components, interconnections, and external in-\nterfaces of the system should be developed to define both \nthe components and their interactions. \n\nInterfaces associated with all product life-cycle phases \nshould also be considered. Examples include interfaces \nwith test equipment; transportation systems; Integrated \nLogistics Support (ILS) systems; and manufacturing fa-\ncilities, operators, users, and maintainers. \n\nAs the technical requirements are defined, the interface \ndiagram should be revisited and the documented inter-\nface requirements refined to include newly identified in-\nterfaces information for requirements both internal and \nexternal. More information regarding interfaces can be \nfound in Section 6.3.\n\nEnvironmental Requirements\n\nEach space mission has a unique set of environmental \nrequirements that apply to the flight segment elements. \nIt is a critical function of systems engineering to identify \nthe external and internal environments for the partic-\nular mission, analyze and quantify the expected environ-\nments, develop design guidance, and establish a margin \nphilosophy against the expected environments. \n\nThe environments envelope should consider what can be \nencountered during ground test, storage, transportation, \n\nlaunch, deployment, and normal operations from begin-\nning of life to end of life. Requirements derived from the \nmission environments should be included in the system \nrequirements.\n\nExternal and internal environment concerns that must \nbe addressed include acceleration, vibration, shock, static \nloads, acoustic, thermal, contamination, crew-induced \nloads, total dose radiation/radiation effects, Single-Event \nEffects (SEEs), surface and internal charging, orbital de-\nbris, atmospheric (atomic oxygen) control and quality, \nattitude control system disturbance (atmospheric drag, \ngravity gradient, and solar pressure), magnetic, pressure \ngradient during launch, microbial growth, and radio fre-\nquency exposure on the ground and on orbit.\n\nThe requirements structure must address the specialty \nengineering disciplines that apply to the mission envi-\nronments across project elements. These discipline areas \nlevy requirements on system elements regarding Electro-\nmagnetic Interference, Electromagnetic Compatibility \n(EMI/EMC), grounding, radiation and other shielding, \ncontamination protection, and reliability. \n\nReliability Requirements\n\nReliability can be defined as the probability that a device, \nproduct, or system will not fail for a given period of time \nunder specified operating conditions. Reliability is an in-\nherent system design characteristic. As a principal con-\ntributing factor in operations and support costs and in \nsystem effectiveness, reliability plays a key role in deter-\nmining the system\u2019s cost-effectiveness.\n\nReliability engineering is a major specialty discipline that \ncontributes to the goal of a cost-effective system. This is \nprimarily accomplished in the systems engineering pro-\ncess through an active role in implementing specific de-\nsign features to ensure that the system can perform in the \npredicted physical environments throughout the mis-\nsion, and by making independent predictions of system \nreliability for design trades and for test program, opera-\ntions, and integrated logistics support planning.\n\nReliability requirements ensure that the system (and \nsubsystems, e.g., software and hardware) can perform in \nthe predicted environments and conditions as expected \nthroughout the mission and that the system has the \nability to withstand certain numbers and types of faults, \nerrors, or failures (e.g., withstand vibration, predicted \ndata rates, command and/or data errors, single-event \n\n\n\n4.2 Technical Requirements Definition\n\nNASA Systems Engineering Handbook ? 45\n\nupsets, and temperature variances to specified limits). \nEnvironments can include ground (transportation and \nhandling), launch, on-orbit (Earth or other), plane-\ntary, reentry, and landing, or they might be for software \nwithin certain modes or states of operation. Reliability \naddresses design and verification requirements to meet \nthe requested level of operation as well as fault and/or \nfailure tolerance for all expected environments and con-\nditions. Reliability requirements cover fault/failure pre-\nvention, detection, isolation, and recovery.\n\nSafety Requirements\nNASA uses the term \u201csafety\u201d broadly to include human \n(public and workforce), environmental, and asset safety. \nThere are two types of safety requirements\u2014determin-\nistic and risk-informed. A deterministic safety require-\nment is the qualitative or quantitative definition of a \nthreshold of action or performance that must be met by \na mission-related design item, system, or activity for that \nitem, system, or activity to be acceptably safe. Examples \nof deterministic safety requirements are incorporation of \nsafety devices (e.g., build physical hardware stops into the \nsystem to prevent the hydraulic lift/arm from extending \npast allowed safety height and length limits); limits on the \nrange of values a system input variable is allowed to take \non; and limit checks on input commands to ensure they \nare within specified safety limits or constraints for that \nmode or state of the system (e.g., the command to re-\ntract the landing gear is only allowed if the airplane is in \nthe airborne state). For those components identified as \n\u201csafety critical,\u201d requirements include functional redun-\ndancy or failure tolerance to allow the system to meet its \nrequirements in the presence of one or more failures or \nto take the system to a safe state with reduced function-\nality (e.g., dual redundant computer processors, safe-state \nbackup processor); detection and automatic system shut-\ndown if specified values (e.g., temperature) exceed pre-\nscribed safety limits; use of only a subset that is approved \nfor safety-critical software of a particular computer lan-\nguage; caution or warning devices; and safety procedures. \nA risk-informed safety requirement is a requirement that \nhas been established, at least in part, on the basis of the \nconsideration of safety-related TPMs and their associ-\nated uncertainty. An example of a risk-informed safety \nrequirement is the Probability of Loss of Crew (P(LOC)) \nnot exceeding a certain value \u201cp\u201d with a certain confi-\ndence level. Meeting safety requirements involves iden-\ntification and elimination of hazards, reducing the likeli-\nhood of the accidents associated with hazards, or reducing \n\nthe impact from the hazard associated with these accidents \nto within acceptable levels. (For additional information \nconcerning safety, see, for example, NPR 8705.2, Human-\nRating Requirements for Space Systems, NPR 8715.3, NASA \nGeneral Safety Program Requirements, and NASA-STD-\n8719.13, Software Safety Standard.)\n\n4.2.2.2 Human Factors Engineering \nRequirements\n\nIn human spaceflight, the human\u2014as operator and as \nmaintainer\u2014is a critical component of the mission and \nsystem design. Human capabilities and limitations must \nenter into designs in the same way that the properties of \nmaterials and characteristics of electronic components \ndo. Human factors engineering is the discipline that \nstudies human-system interfaces and interactions and \nprovides requirements, standards, and guidelines to en-\nsure the entire system can function as designed with ef-\nfective accommodation of the human component. \n\nHumans are initially integrated into systems through \nanalysis of the overall mission. Mission functions are \nallocated to humans as appropriate to the system ar-\nchitecture, technical capabilities, cost factors, and crew \ncapabilities. Once functions are allocated, human fac-\ntors analysts work with system designers to ensure that \nhuman operators and maintainers are provided the \nequipment, tools, and interfaces to perform their as-\nsigned tasks safely and effectively. \n\nNASA-STD-3001, NASA Space Flight Human System \nStandards Volume 1: Crew Health ensures that systems \nare safe and effective for humans. The standards focus \non the human integrated with the system, the measures \nneeded (rest, nutrition, medical care, exercise, etc.) to \nensure that the human stays healthy and effective, the \nworkplace environment, and crew-system physical and \ncognitive interfaces.\n\n4.2.2.3 Requirements Decomposition, \nAllocation, and Validation\n\nRequirements are decomposed in a hierarchical struc-\nture starting with the highest level requirements im-\nposed by Presidential directives, mission directorates, \nprogram, Agency, and customer and other stakeholders. \nThese high-level requirements are decomposed into \nfunctional and performance requirements and allocated \nacross the system. These are then further decomposed \nand allocated among the elements and subsystems. This \n\n\n\n46 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\ndecomposition and allocation process continues until a \ncomplete set of design-to requirements is achieved. At \neach level of decomposition (system, subsystem, compo-\nnent, etc.), the total set of derived requirements must be \nvalidated against the stakeholder expectations or higher \nlevel parent requirements before proceeding to the next \nlevel of decomposition.\n\nThe traceability of requirements to the lowest level en-\nsures that each require ment is necessary to meet the \nstakeholder expectations. Require ments that are not al-\nlocated to lower levels or are not implemented at a lower \nlevel re sult in a design that does not meet objectives and \nis, therefore, not valid. Con versely, lower level require-\nments that are not traceable to higher level requirements \n\nresult in an overdesign that is not justified. This hierar-\nchical flowdown is illustrated in Figure 4.2-3. \n\nFigure 4.2-4 is an example of how science pointing re-\nquirements are successively decomposed and allocated \nfrom the top down for a typical science mission. It is im-\nportant to un derstand and document the relationship be-\ntween requirements. This will reduce the possibility of \nmis in ter pretation and the possibility of an unsatisfactory \ndesign and associated cost increases. \n\nThroughout Phases A and B, changes in requirements and \nconstraints will occur. It is impera tive that all changes be \nthoroughly evaluated to determine the impacts on both \nhigher and lower hierarchical levels. All changes must be \n\nSystem\nPerformance\n\nRequirements\n\nEnvironmental\nand Other Design\n\nRequirements\nand Guidelines \n\nInstitutional\nConstraints\n\nAssumptions\n\nImplementing\nOrganizations\n\nCustomer\n\nProgrammatics:\n? Cost\n? Schedule\n? Constraints\n? Mission Classification\n\nMission\nObjectives \n\nSystem\nFunctional\n\nRequirements \n\nSubsystem A\nFunctional and\n\nPerformance\nRequirements\n\nSubsystem\nC\n\nMission\nRequirements \n\nAllocated\nRequirements\n\nDerived\nRequirements\n\nSubsystem X\nFunctional and\n\nPerformance\nRequirements\n\nAllocated\nRequirements\n\nDerived\nRequirements\n\nSubsystem\nB\n\n...\n\nMission\nAuthority \n\nFigure 4.2?3 The flowdown of requirements\n\n\n\n4.2 Technical Requirements Definition\n\nNASA Systems Engineering Handbook ? 47\n\nsubjected to a review and approval cycle as part of a formal \nchange control process to maintain traceability and to en-\nsure the impacts of any changes are fully assessed for all \nparts of the system. A more formal change control pro-\ncess is re quired if the mission is very large and involves \nmore than one Center or crosses other jurisdic tional or \norganizational boundaries.\n\n4.2.2.4 Capturing Requirements and the \nRequirements Database\n\nAt the time the requirements are written, it is important \nto capture the requirements statements along with the \nmetadata associated with each requirement. The meta-\ndata is the supporting information necessary to help \nclarify and link the requirements. \n\nThe method of verification must also be thought through \nand captured for each requirement at the time it is de-\nveloped. The verification method includes test, inspec-\ntion, analysis, and demonstration. Be sure to document \nany new or derived requirements that are uncovered \nduring determination of the verification method. An \nexample is requiring an additional test port to give \nvisibility to an internal signal during integration and \ntest. If a requirement cannot be verified, then either it \n\nshould not be a requirement or the requirement state-\nment needs to be rewritten. For example, the requirement \nto \u201cminimize noise\u201d is vague and cannot be verified. If the \nrequirement is restated as \u201cthe noise level of the compo-\nnent X shall remain under Y decibels\u201d then it is clearly ver-\nifiable. Examples of the types of metadata are provided in \nTable 4.2-2. \n\nThe requirements database is an extremely useful tool for \ncapturing the requirements and the associated metadata and \nfor showing the bidirectional traceability between require-\nments. The database evolves over time and could be used \nfor tracking status information related to requirements such \nas To Be Determined (TBD)/To Be Resolved (TBR) status, \nresolution date, and verification status. Each project should \ndecide what metadata will be captured. The database is usu-\nally in a central location that is made available to the entire \nproject team. (See Appendix D for a sample requirements \nverification matrix.)\n\n4.2.2.5 Technical Standards\n\nImportance of Standards Application\n\nStandards provide a proven basis for establishing \ncommon technical requirements across a program or \n\nFigure 4.2?4 Allocation and flowdown of science pointing requirements\n\nScience Pointing\nRequirements \n\nSpacecraft\nRequirements \n\nGround\nRequirements \n\nAttitude\nDetermination\nRequirements \n\nScience Axis\nKnowledge\n\nRequirements \n\nTotal Gyro to\nStar Tracker\n\nError \n\nScience Axis to\nAttitude Control\n\nSystem Reference\n\nInstrument\nBoresight to\nScience Axis \n\nAttitude\nEstimation\n\nError \n\nMain\nStructure\nThermal\n\nDeformation\n\nGyro to Star\nTracker\n\nCalibration\nUncertainty\n\nOptical\nBench\n\nThermal\nDeformation\n\nFilter\nEstimation\n\nError \n\nGyro\nBias Rate\n\nDrift \n\nInstrument\nCalibration\n\nError \n\nInstrument\nThermal\n\nDeformation \n\nStar\nCatalog\nLocation\n\nError \n\nVelocity\nAberration \n\n\n\n48 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nproject to avoid incompatibilities and ensure that at least \nminimum requirements are met. Common standards \ncan also lower implementation cost as well as costs for \ninspection, common supplies, etc. Typically, standards \n(and specifications) are used throughout the product life \ncycle to establish design requirements and margins, ma-\nterials and process specifications, test methods, and in-\nterface specifications. Standards are used as requirements \n(and guidelines) for design, fabrication, verification, val-\nidation, acceptance, operations, and maintenance.\n\nSelection of Standards\n\nNASA policy for technical standards is provided in NPD \n8070.6, Technical Standards, which addresses selection, \ntailoring, application, and control of standards. In gen-\n\neral, the order of authority among standards for NASA \nprograms and projects is as follows:\n\nStandards mandated by law (e.g., environmental stan- ?\ndards),\nNational or international voluntary consensus stan- ?\ndards recognized by industry,\nOther Government standards, ?\nNASA policy directives, and  ?\nNASA technical standards. ?\n\nNASA may also designate mandatory or \u201ccore\u201d stan-\ndards that must be applied to all programs where tech-\nnically applicable. Waivers to designated core standards \nmust be justified and approved at the Agency level unless \notherwise delegated.\n\nTable 4.2?2 Requirements Metadata\n\nItem Function\n\nRequirement ID Provides a unique numbering system for sorting and tracking.\n\nRationale Provides additional information to help clarify the intent of the requirements at the time they were \nwritten. (See \u201cRationale\u201d box below on what should be captured.)\n\nTraced from Captures the bidirectional traceability between parent requirements and lower level (derived) \nrequirements and the relationships between requirements.\n\nOwner Person or group responsible for writing, managing, and/or approving changes to this requirement.\n\nVerification method Captures the method of verification (test, inspection, analysis, demonstration) and should be \ndetermined as the requirements are developed.\n\nVerification lead Person or group assigned responsibility for verifying the requirement. \n\nVerification level Specifies the level in the hierarchy at which the requirements will be verified (e.g., system, subsys-\ntem, element).\n\nRationale\n\nThe rationale should be kept up to date and include the following information:\n\nReason for the Requirement: ?  Often the reason for the requirement is not obvious, and it may be lost if not recorded \nas the requirement is being documented. The reason may point to a constraint or concept of operations. If there is a \nclear parent requirement or trade study that explains the reason, then reference it. \n\nDocument Assumptions: ?  If a requirement was written assuming the completion of a technology development pro-\ngram or a successful technology mission, document the assumption. \n\nDocument Relationships: ?  The relationships with the product\u2019s expected operations (e.g., expectations about how \nstakeholders will use a product). This may be done with a link to the ConOps.\n\nDocument Design Constraints: ?  Imposed by the results from decisions made as the design evolves. If the require-\nment states a method of implementation, the rationale should state why the decision was made to limit the solution \nto this one method of implementation.\n\n\n\nNASA Systems Engineering Handbook ? 49\n\nLogical Decomposition is the process for creating the \ndetailed functional requirements that enable NASA pro-\ngrams and projects to meet the stakeholder expectations. \nThis process identifies the \u201cwhat\u201d that must be achieved \nby the system at each level to enable a successful project. \nLogical decomposition utilizes functional analysis to \ncreate a system architecture and to decompose top-level \n(or parent) requirements and allocate them down to the \nlowest desired levels of the project.\n\nThe Logical Decomposition Process is used to:\nImprove understanding of the defined technical re- ?\nquirements and the relationships among the require-\nments (e.g., functional, behavioral, and temporal), \nand\nDecompose the parent requirements into a set of log- ?\nical decomposition models and their associated sets \nof derived technical requirements for input to the De-\nsign Solution Definition Process.\n\n4.3.1 Process Description \nFigure 4.3-1 provides a typical flow diagram for the Log-\nical Decomposition Process and identifies typical inputs, \noutputs, and activities to consider in addressing logical \ndecomposition.\n\n4.3.1.1 Inputs\nTypical inputs needed for the Logical Decomposition \nProcess would include the following:\n\nTechnical Requirements: ?  A validated set of require-\nments that represent a description of the problem to \nbe solved, have been established by functional and \nperformance analysis, and have been approved by the \ncustomer and other stakeholders. Examples of docu-\nmentation that capture the requirements are an SRD, \nPRD, and IRD.\nTechnical Measures: ?  An established set of measures \nbased on the expectations and requirements that will \nbe tracked and assessed to determine overall system \nor product effectiveness and customer satisfaction. \nThese measures are MOEs, MOPs, and a special \nsubset of these called TPMs. See Subsection 6.7.2.2 \nfor further details.\n\n4.3.1.2 Process Activities\nThe key first step in the Logical Decomposition Pro-\ncess is establishing the system architecture model. The \nsystem architecture activity defines the underlying struc-\nture and relationships of hardware, software, communi-\ncations, operations, etc., that provide for the implemen-\ntation of Agency, mission directorate, program, project, \n\nand subsequent levels of the \nrequirements. System archi-\ntecture activities drive the \npartitioning of system ele-\nments and requirements to \nlower level functions and \nrequirements to the point \nthat design work can be ac-\ncomplished. Interfaces and \nrelationships between parti-\ntioned subsystems and ele-\nments are defined as well.\n\nOnce the top-level (or \nparent) functional require-\nments and constraints have \nbeen established, the system \ndesigner uses functional \nanalysis to begin to formu-\nlate a conceptual system ar-\nchitecture. The system ar-\n\n4.3 Logical Decomposition\n\nFrom Technical\nRequirements De?nition \n\nand Con?guration\nManagement Processes \n\nTo Technical Data\nManagement Process\n\nDerived Technical \nRequirements\n\nLogical Decomposition\nWork Products\n\nBaselined Technical \nRequirements\n\nMeasures of \nPerformance\n\nTo Design Solution\nDe?nition and Requirements\nManagement  and  Interface\n\nManagement  Processes  \n\nLogical Decomposition\nModels\n\nFrom Technical\nRequirements De?nition\n\nand Technical Data\nManagement Processes \n\nTo Design Solution\nDe?nition and Con?guration\n\nManagement Processes  \n\nDe?ne one or more logical\ndecomposition models \n\nAllocate technical requirements to\nlogical decomposition models to form\na set of derived technical requirements \n\nResolve derived technical \nrequirement con?icts \n\nValidate the resulting set of derived\ntechnical requirements  \n\nEstablish the derived technical\nrequirements baseline \n\nFigure 4.3?1 Logical Decomposition Process\n\n\n\n50 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nchitecture can be seen as the strategic organization of \nthe functional elements of the system, laid out to enable \nthe roles, relationships, dependencies, and interfaces be-\ntween elements to be clearly defined and understood. It \nis strategic in its focus on the overarching structure of the \nsystem and how its elements fit together to contribute to \nthe whole, instead of on the particular workings of the \nelements themselves. It enables the elements to be de-\nveloped separately from each other while ensuring that \nthey work together effectively to achieve the top-level (or \nparent) requirements.\n\nMuch like the other elements of functional decomposi-\ntion, the development of a good system-level architec-\nture is a creative, recursive, and iterative process that \ncombines an excellent understanding of the project\u2019s end \nobjectives and constraints with an equally good knowl-\nedge of various potential technical means of delivering \nthe end products. \n\nFocusing on the project\u2019s ends, top-level (or parent) re-\nquirements, and constraints, the system architect must \ndevelop at least one, but preferably multiple, concept ar-\nchitectures capable of achieving program objectives. Each \narchitecture concept involves specification of the func-\ntional elements (what the pieces do), their relationships \nto each other (interface definition), and the ConOps, i.e., \nhow the various segments, subsystems, elements, units, \netc., will operate as a system when distributed by loca-\ntion and environment from the start of operations to the \nend of the mission. \n\nThe development process for the architectural concepts \nmust be recursive and iterative, with feedback from \nstakeholders and external reviewers, as well as from sub-\nsystem designers and operators, provided as often as \npossible to increase the likelihood of achieving the pro-\ngram\u2019s ends, while reducing the likelihood of cost and \nschedule overruns.\n\nIn the early stages of the mission, multiple concepts are \ndeveloped. Cost and schedule constraints will ultimately \nlimit how long a program or project can maintain mul-\ntiple architectural concepts. For all NASA programs, ar-\nchitecture design is completed during the Formulation \nphase. For most NASA projects (and tightly coupled pro-\ngrams), the selection of a single architecture will happen \nduring Phase A, and the architecture and ConOps will \nbe baselined during Phase B. Architectural changes at \nhigher levels occasionally occur as decomposition to \n\nlower levels produces complications in design, cost, or \nschedule that necessitate such changes.\n\nAside from the creative minds of the architects, there are \nmultiple tools that can be utilized to develop a system\u2019s \narchitecture. These are primarily modeling and simula-\ntion tools, functional analysis tools, architecture frame-\nworks, and trade studies. (For example, one way of doing \narchitecture is the Department of Defense (DOD) Ar-\nchitecture Framework (DODAF). See box.) As each \nconcept is developed, analytical models of the architec-\nture, its elements, and their operations will be developed \nwith increased fidelity as the project evolves. Functional \ndecomposition, requirements development, and trade \nstudies are subsequently undertaken. Multiple iterations \nof these activities feed back to the evolving architectural \nconcept as the requirements flow down and the design \nmatures.\n\nFunctional analysis is the primary method used in \nsystem architecture development and functional re-\nquirement decomposition. It is the systematic process \nof identifying, describing, and relating the functions a \nsystem must perform to fulfill its goals and objectives. \nFunctional analysis identifies and links system functions, \ntrade studies, interface characteristics, and rationales to \nrequirements. It is usually based on the ConOps for the \nsystem of interest.\n\nThree key steps in performing functional analysis are:\nTranslate top-level requirements into functions that  ?\nmust be performed to accomplish the requirements. \nDecompose and allocate the functions to lower levels  ?\nof the product breakdown structure.\nIdentify and describe functional and subsystem inter- ?\nfaces.\n\nThe process involves analyzing each system requirement \nto identify all of the functions that must be performed \nto meet the requirement. Each function identified is de-\nscribed in terms of inputs, outputs, and interface require-\nments. The process is repeated from the top down so that \nsubfunctions are recognized as part of larger functional \nareas. Functions are arranged in a logical sequence so \nthat any specified operational usage of the system can be \ntraced in an end-to-end path.\n\nThe process is recursive and iterative and continues until \nall desired levels of the architecture/system have been \nanalyzed, defined, and baselined. There will almost cer-\n\n\n\n4.3 Logical Decomposition\n\nNASA Systems Engineering Handbook ? 51\n\ntainly be alternative ways to decompose functions; there-\nfore, the outcome is highly dependent on the creativity, \nskills, and experience of the engineers doing the analysis. \nAs the analysis proceeds to lower levels of the architec-\nture and system and the system is better understood, the \nsystems engineer must keep an open mind and a will-\ningness to go back and change previously established ar-\nchitecture and system requirements. These changes will \nthen have to be decomposed down through the architec-\nture and systems again, with the recursive process con-\n\ntinuing until the system is fully defined, with all of the \nrequirements understood and known to be viable, verifi-\nable, and internally consistent. Only at that point should \nthe system architecture and requirements be baselined.\n\n4.3.1.3 Outputs\nTypical outputs of the Logical Decomposition Process \nwould include the following:\n\nSystem Architecture Model: ?  Defines the under-\nlying structure and relationship of the elements of the \n\nDOD Architecture Framework\n\nNew ways, called architecture frameworks, have been developed in the last decade to describe and characterize evolv-\ning, complex system-of-systems. In such circumstances, architecture descriptions are very useful in ensuring that stake-\nholder needs are clearly understood and prioritized, that critical details such as interoperability are addressed upfront, \nand that major investment decisions are made strategically. In recognition of this, the U.S. Department of Defense has \nestablished policies that mandate the use of the DODAF in capital planning, acquisition, and joint capabilities integra-\ntion.\n\nAn architecture can be understood as \u201cthe structure of components, their relationships, and the principles and guide-\nlines governing their design and evolution over time.\u201d* To describe an architecture, the DODAF defines several views: \noperational, systems, and technical standards. In addition, a dictionary and summary information are also required. (See \nfigure below.)\n\nWithin each of these views, DODAF contains specific products. For example, within the Operational View is a description \nof the operational nodes, their connectivity, and information exchange requirements. Within the Systems View is a de-\nscription of all the systems contained in the operational nodes and their interconnectivity. Not all DODAF products are \nrelevant to NASA systems engineering, but its underlying concepts and formalisms may be useful in structuring com-\nplex problems for the Technical Requirements Definition and Decision Analysis Processes.\n\n*Definition based on Institute of Electrical and Electronics Engineers (IEEE) STD 610.12.\n\nSource: DOD, DOD Architecture Framework.\n\nOperational View\nIdentifies what needs to be\n\naccomplished and by whom\n\nTechnical Standards View\nPrescribes standards\n\nand conventions\n\nSystems View\nRelates systems and characteristics\n\nto operational needs\n\nSpeci?c System Capabilities\nRequired to Satisfy\nInformation Exchanges\n\nTechnical Standards Criteria\nGoverning  Interoperable\nImplementation/Procurement\nof the Selected System Capabilities\n\nSy\nste\n\nms\n Th\n\nat\n\nSu\npp\n\nor\nt t\n\nhe\n\nAc\ntiv\n\niti\nes\n\n an\nd\n\nInf\nor\n\nma\ntio\n\nn\n\nEx\nch\n\nan\nge\n\ns\n\n? W\nha\n\nt N\nee\n\nds\n to\n\n Be\n D\n\non\ne\n\n? W\nho\n\n D\noe\n\ns It\n\n? I\nnfo\n\nrm\nati\n\non\n Ex\n\nch\nan\n\nge\ns\n\nRe\nqu\n\nire\nd t\n\no G\net \n\nIt D\non\n\ne\n\n? Basic Technology\n\nSupportability\n\n? New Technical\n\nCapabilities\n\nOperational Requirements\n\nand Capabilities\n\n\n\n52 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nsystem (e.g., hardware, software, communications, \noperations, etc.) and the basis for the partitioning of \nrequirements into lower levels to the point that design \nwork can be accomplished.\nEnd Product Requirements: ?  A defined set of make-\nto, buy-to, code-to, and other requirements from \nwhich design solutions can be accomplished.\n\n4.3.2 Logical Decomposition Guidance\n\n4.3.2.1 Product Breakdown Structure\nThe decompositions represented by the PBS and the \nWork Breakdown Structure (WBS) form important per-\nspectives on the desired product system. The WBS is a \nhierarchical breakdown of the work necessary to com-\nplete the project. See Subsection 6.1.2.1 for further in-\nformation on WBS development. The WBS contains the \nPBS, which is the hierarchical breakdown of the prod-\nucts such as hardware items, software items, and infor-\nmation items (documents, databases, etc.). The PBS is \nused during the Logical Decomposition and functional \nanalysis processes. The PBS should be carried down to \nthe lowest level for which there is a cognizant engineer \nor manager. Figure 4.3-2 is an example of a PBS.\n\n4.3.2.2 Functional Analysis Techniques\nAlthough there are many techniques available to per-\nform functional analysis, some of the more popular are \n(1) Functional Flow Block Diagrams (FFBDs) to depict \ntask sequences and relationships, (2) N2 diagrams (or \nN x N interaction matrix) to identify interactions or in-\nterfaces between major factors from a systems perspec-\ntive, and (3) Timeline Analyses (TLAs) to depict the time \nsequence of time-critical functions.\n\nFunctional Flow Block Diagrams\nThe primary functional analysis technique is the func-\ntional flow block diagram. The purpose of the FFBD is to \nindicate the sequential relationship of all functions that \nmust be accomplished by a system. When completed, \nthese diagrams show the entire network of actions that \nlead to the fulfillment of a function. \n\nFFBDs specifically depict each functional event (rep-\nresented by a block) occurring following the preceding \nfunction. Some functions may be performed in parallel, \nor alternative paths may be taken. The FFBD network \nshows the logical sequence of \u201cwhat\u201d must happen; it \ndoes not ascribe a time duration to functions or between \n\nfunctions. The duration of \nthe function and the time \nbetween functions may vary \nfrom a fraction of a second \nto many weeks. To under-\nstand time-critical require-\nments, a TLA is used. (See \nthe TLA discussion later in \nthis subsection.)\n\nThe FFBDs are function \noriented, not equipment \noriented. In other words, \nthey identify \u201cwhat\u201d must \nhappen and must not as-\nsume a particular answer \nto \u201chow\u201d a function will be \nperformed. The \u201chow\u201d is \nthen defined for each block \nat a given level by defining \nthe \u201cwhat\u201d functions at the \nnext lower level necessary \nto accomplish that block. \nIn this way, FFBDs are de-\nveloped from the top down, \n\nFlight Segment\n\nLaunch\nAccommodations \n\nSpacecraft\nBus \n\nPayload\nElement \n\nElectrical\n\nPayload\nAttached\n\nFitting\n\nSupply\n\nCommand\n& Data\n\nGuidance,\nNavigation &\n\nControl\n\nMechanisms\n\nPropulsionElectronics\n\nDetectors\n\nTelescope\n\nThermal\n\nElectrical\n\nPower\n\nStructure\n\nPayload\nInterface\n\nCommuni-\ncations\n\nSpacecraft\nInterface\n\nFigure 4.3?2 Example of a PBS\n\n\n\n4.3 Logical Decomposition\n\nNASA Systems Engineering Handbook ? 53\n\nin a series of levels, with tasks at each level identified \nthrough functional decomposition of a single task at a \nhigher level. The FFBD displays all of the tasks at each \nlevel in their logical, sequential relationship, with their \nrequired inputs and anticipated outputs (including met-\nrics, if applicable), plus a clear link back to the single, \nhigher level task. \n\nAn example of an FFBD is shown in Figure 4.3-3. The \nFFBD depicts the entire flight mission of a spacecraft. \n\nEach block in the first level of the diagram is expanded \nto a series of functions, as shown in the second-level dia-\ngram for \u201cPerform Mission Operations.\u201d Note that the \ndiagram shows both input (\u201cTransfer to OPS Orbit\u201d) and \noutput (\u201cTransfer to STS Orbit\u201d), thus initiating the in-\nterface identification and control process. Each block in \nthe second-level diagram can be progressively developed \ninto a series of functions, as shown in the third-level dia-\ngram.\n\nTOP LEVEL\n\n1.0\n\nAscent Into\nOrbit Injection\n\n(6.0) Ref.\nTransfer to\nSTS Orbit\n\n(3.0) Ref.\nTransfer to\nOPS Orbit \n\nAND\n\n (4.10) Ref.\nTransmit Pay-\n\nload & Sub-\nsystem Data \n\n(4.7) Ref.\nStore/Process\n\nCommand\n\nOR\n\nTHIRD LEVEL\n\nSECOND LEVEL\n\n2.0\n\nCheck Out\nand Deploy\n\n3.0\n\nTransfer to\nOPS Orbit\n\n5.0\n\nContingency\nOperations\n\n6.0\n\nTransfer to\nSTS Orbit\n\n7.0\n\nRetrieve\nSpacecraft\n\n8.0\n\nReenter and\nLand\n\n4.1\n\nProvide\nElectric Power\n\n4.2\nProvide\nAttitude\n\nStabilization\n\n4.3\nProvide\nThermal\nControl\n\n4.4\n\nProvide Orbit\nMain\n\n4.5\n\nReceive\nCommand\n\n4.6\n\nReceive Com-\nmand (Omni)\n\n4.7\n\nStore/Process\nCommand\n\n4.8\n\nAcquire\nPayload Data\n\n4.0\nPerform\nMission\n\nOperations\n\n4.10\nTransmit Pay-\n\nload & Sub-\nsystem Data\n\nOR\n\n4.9\nAcquire\n\nSubsystem\nStatus Data\n\n4.11\nTransmit\n\nSubsystem\nData\n\n4.8.1\nCompute TDRS\n\nPointing\nVector\n\n4.8.2\nSlew to\n\nand Track\nTDRS\n\n4.8.3\n\nRadar to\nStandby\n\n4.8.4\nCompute LOS\n\nPointing\nVector\n\n4.8.5\nSlew S/C\nto LOS\nVector\n\n4.8.6\nCommand \n\nERP PW\nRadar On\n\n4.8.7\nProcess Re-\n\nceiving Signal\nand Format\n\n4.8.8\n\nRadar to\nStandby\n\n4.8.9\n\nRadar O?\n\nOR OR\n\nOR\n\nFigure 4.3?3 Example of a functional flow block diagram\n\n\n\n54 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nFFBDs are used to develop, analyze, and flow down re-\nquirements, as well as to identify profitable trade studies, \nby identifying alternative approaches to performing each \nfunction. In certain cases, alternative FFBDs may be \nused to represent various means of satisfying a particular \nfunction until trade study data are acquired to permit se-\nlection among the alternatives. \n\nThe flow diagram also provides an understanding of \nthe total operation of the system, serves as a basis for \ndevelopment of operational and contingency proce-\ndures, and pinpoints areas where changes in opera-\ntional procedures could simplify the overall system \noperation. \n\nN2 Diagrams\nThe N-squared (N2 or N2) diagram is used to develop \nsystem interfaces. An example of an N2 diagram is \nshown in Figure 4.3-4. The system components or \nfunctions are placed on the diagonal; the remainder of \n\nthe squares in the N x N matrix represent the interface \ninputs and outputs. Where a blank appears, there is \nno interface between the respective components or \nfunctions. The N2 diagram can be taken down into \nsuccessively lower levels to the component functional \nlevels. In addition to defining the interfaces, the N2 \ndiagram also pinpoints areas where conflicts could \narise in interfaces, and highlights input and output \ndependency assumptions and requirements. \n\nTimeline Analysis\nTLA adds consideration of functional durations and is \nperformed on those areas where time is critical to mission \nsuccess, safety, utilization of resources, minimization of \ndowntime, and/or increasing availability. TLA can be ap-\nplied to such diverse operational functions as spacecraft \ncommand sequencing and launch; but for those functional \nsequences where time is not a critical factor, FFBDs or N2 \ndiagrams are sufficient. The following areas are often cat-\negorized as time-critical: (1) functions affecting system \n\nreaction time, (2) mission \nturnaround time, (3) time \ncountdown activities, and \n(4) functions for which op-\ntimum equipment and/or \npersonnel utilization are de-\npendent on the timing of \nparticular activities. \n\nTimeline Sheets (TLSs) are \nused to perform and record \nthe analysis of time-critical \nfunctions and functional \nsequences. For time-critical \nfunctional sequences, the \ntime requirements are spec-\nified with associated toler-\nances. Additional tools such \nas mathematical models and \ncomputer simulations may \nbe necessary to establish the \nduration of each timeline. \n\nFor additional information \non FFBD, N2 diagrams, \ntimeline analysis, and other \nfunctional analysis methods, \nsee Appendix F.\n\nA\n\nB\n\nC\n\nD\n\nE\n\nF\n\nG\n\nH\n\nSS\n\nM\n\nM\n\nM\n\nM\nE E\n\nE E\n\nOutput\nBeta\n\nInput\nAlpha\n\nE M SS\n\nE M SS\n\nE M SS E M SS\n\nLegend:\nE Electrical\n\nM Mechanical\n\nSS Supplied Services\n\n Interface\n\n A\u2013H: System or Subsystem \n\nFigure 4.3?4 Example of an N2 diagram\n\n\n\nNASA Systems Engineering Handbook ? 55\n\n4.4 Design Solution Definition\n\nThe Design Solution Definition Process is used to trans-\nlate the high-level requirements derived from the stake-\nholder expectations and the outputs of the Logical De-\ncomposition Process into a design solution. This involves \ntransforming the defined logical decomposition models \nand their associated sets of derived technical require-\nments into alternative solutions. These alternative solu-\ntions are then analyzed through detailed trade studies \nthat result in the selection of a preferred alternative. This \npreferred alternative is then fully defined into a final de-\nsign solution that will satisfy the technical requirements. \nThis design solution definition will be used to generate \nthe end product specifications that will be used produce \nthe product and to conduct product verification. This \nprocess may be further refined depending on whether \n\nthere are additional subsystems of the end product that \nneed to be defined.\n\n4.4.1 Process Description\nFigure 4.4-1 provides a typical flow diagram for the De-\nsign Solution Definition Process and identifies typical \ninputs, outputs, and activities to consider in addressing \ndesign solution definition.\n\n4.4.1.1 Inputs\n\nThere are several fundamental inputs needed to initiate \nthe Design Solution Definition Process:\n\nTechnical Requirements: ?  The customer and stake-\nholder needs that have been translated into a reason  \n\nFigure 4.4?1 Design Solution Definition Process \n* To Product Implementation Process\n\nBaselined Logical\nDecomposition\n\nModels \n\nSystem-Speci?ed\nRequirements \n\nTo Requirements Management\nand Interface Management Processes\n\nInitial Subsystem\nSpeci?cations\n\nTo Stakeholder Expectations De?nition\nand Requirements Management and Interface\n\nManagement Processes\n\nProduct Validation\nPlan \n\nTo Product Validation Process\n\nLogistics and Operate-\nTo Procedures\n\nTo Technical Data Management Process\n\nEnabling Product\nRequirements\n\nTo Stakeholder Expectations De?nition\nor Product Implementation and\nRequirements Management and\n\nInterface Management Processes   \n\nFrom Logical \nDecomposition and\n\nCon?guration Management\nProcesses\n\nInitiate development\nof enabling products \n\nNo Yes\n\nInitiate development \nof next lower level\n\nproducts \n\nDe?ne alternative design solutions\n\nAnalyze each alternative design solution\n\nSelect best design solution alternative\n\nGenerate full design description of the\nselected solution\n\nVerify the fully de?ned design solution\n\nBaseline design solution speci?ed requirements\nand design descriptions  \n\nNo\n\n*\nNeed\n\nlower level\nproduct? \n\nYes\n\n*\nEnabling\nproduct\nexists? \n\nEnd Product\u2013Speci?ed\nRequirements \n\nProduct Veri?cation\nPlan \n\nTo Product Veri?cation Process\n\nBaselined Derived\nTechnical\n\nRequirements\n\n\n\n56 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nably complete set of validated requirements for the \nsystem, including all interface requirements.\nLogical Decomposition Models: ?  Requirements de-\ncomposed by one or more different methods (e.g., \nfunction, time, behavior, data flow, states, modes, \nsystem architecture, etc.).\n\n4.4.1.2 Process Activities\n\nDefine Alternative Design Solutions\nThe realization of a system over its life cycle involves \na succession of decisions among alternative courses of \nac tion. If the alternatives are precisely defined and thor-\noughly understood to be well differentiated in the cost-\neffectiveness space, then the systems engineer can make \nchoices among them with confidence.\n\nTo obtain assessments that are crisp enough to facili-\ntate good decisions, it is often necessary to delve more \ndeeply into the space of possible designs than has yet \nbeen done, as is illustrated in Figure 4.4-2. It should be \nrealized, however, that this illustration represents neither \nthe project life cycle, which encompasses the system de-\nvelopment process from inception through disposal, nor \nthe product development process by which the system \ndesign is developed and implemented.\n\nEach create concepts step in Figure 4.4-2 involves a recur-\nsive and iterative design loop driven by the set of stake-\n\nholder expectations where a strawman architecture/\ndesign, the associated ConOps, and the derived require-\nments are developed. These three products must be con-\nsistent with each other and will require iterations and de-\nsign decisions to achieve this consistency. This recursive \nand iterative design loop is illustrated in Figure 4.0-1.\n\nEach create concepts step also involves an assessment of \npotential capabilities offered by the continually changing \nstate of technology and potential pitfalls captured through \nexperience-based review of prior program/project les-\nsons learned data. It is imperative that there be a con-\ntinual interaction between the technology development \nprocess and the design process to ensure that the design \nreflects the realities of the available technology and that \noverreliance on immature technology is avoided. Addi-\ntionally, the state of any technology that is considered \nenabling must be properly monitored, and care must be \ntaken when assessing the impact of this technology on \nthe concept performance. This interaction is facilitated \nthrough a periodic assessment of the design with respect \nto the maturity of the technology required to imple ment \nthe design. (See Subsection 4.4.2.1 for a more de tailed \ndiscussion of technology assessment.) These tech nology \nelements usually exist at a lower level in the PBS. Al-\nthough the process of design concept development by \nthe integration of lower level elements is a part of the sys-\ntems engineering process, there is always a danger that \nthe top-down process cannot keep up with the bottom-\nup process. Therefore, system architecture issues need to \nbe resolved early so that the system can be modeled with \nsufficient realism to do reliable trade studies.\n\nAs the system is realized, its particulars become clearer\u2014\nbut also harder to change. The purpose of systems engi-\nneering is to make sure that the Design Solution Defi-\nnition Process happens in a way that leads to the most \ncost-effective final system. The basic idea is that before \nthose decisions that are hard to undo are made, the al-\nternatives should be carefully assessed, particularly with \nrespect to the maturity of the required technology.\n\nCreate Alternative Design Concepts\n\nOnce it is understood what the system is to accomplish, \nit is possible to devise a variety of ways that those goals \ncan be met. Sometimes, that comes about as a conse-\nquence of considering alternative functional allocations \nand integrating available subsystem design options, all of \nwhich can have technologies at varying degrees of matu-Figure 4.4?2 The doctrine of successive refinement\n\nRecognize \nneed/\n\nopportunity\n\nIdentify and \nquantify goals\n\nCreate \n\nconcepts\n\nIdentify and \nquantify goals\n\nIdentify and \nquantify goals\n\nCreate \n\nconcepts\n\nCreate \n\nconcepts\n\nCreate \n\nconcepts\n\nIdentify and \nquantify goals\n\nDo\n tra\n\nde\n \n\nstu\ndie\n\ns\n\nDo\n tra\n\nde\n \n\nstu\ndie\n\ns\n\nDo\n tra\n\nde\n \n\nstu\ndie\n\ns\n\nSelectdesign\nSelectdesign\n\nSelectdesign\n\nSelectdesign\nDo\n\n tra\nde\n\n \n\nstu\ndie\n\ns\n\nIn\ncr\n\nea\nse\n\nre\nso\n\nlu\ntio\n\nn\n\nPerform\nmission\n\n     \n     \n\n     \n Im\n\nple\nme\n\nnt \nde\n\ncis\nion\n\ns\n\nIn\ncr\n\nea\nse\n\nre\nso\n\nlu\ntio\n\nn\nIn\n\ncr\nea\n\nse\nre\n\nso\nlu\n\ntio\nn\n\n\n\n4.4 Design Solution Definition\n\nNASA Systems Engineering Handbook ? 57\n\nrity. Ideally, as wide a range of plausible alternatives as is \nconsistent with the design organization\u2019s charter should \nbe defined, keeping in mind the current stage in the pro-\ncess of successive refinement. When the bottom-up pro-\ncess is operating, a problem for the systems engineer is \nthat the designers tend to become fond of the designs \nthey create, so they lose their objectivity; the systems en-\ngineer often must stay an \u201coutsider\u201d so that there is more \nobjectivity. This is particularly true in the assessment of \nthe technological maturity of the subsystems and com-\nponents required for implementation. There is a ten-\ndency on the part of technology developers and project \nmanagement to overestimate the maturity and applica-\nbility of a technology that is required to implement a de-\nsign. This is especially true of \u201cheritage\u201d equipment. The \nresult is that critical aspects of systems engineering are \noften overlooked.\n\nOn the first turn of the successive refinement in \nFigure 4.4-2, the subject is often general approaches or \nstrategies, sometimes architectural concepts. On the next, \nit is likely to be functional design, then detailed design, \nand so on. The reason for avoiding a premature focus on \na single design is to permit discovery of the truly best de-\nsign. Part of the systems engineer\u2019s job is to ensure that \nthe design concepts to be compared take into account all \ninterface requirements. \u201cDid you include the cabling?\u201d \nis a characteristic question. When possible, each design \nconcept should be described in terms of controllable de-\nsign parameters so that each represents as wide a class \nof designs as is reasonable. In doing so, the systems engi-\nneer should keep in mind that the potentials for change \nmay include organizational structure, schedules, proce-\ndures, and any of the other things that make up a system. \nWhen possible, constraints should also be described by \nparameters.\n\nAnalyze Each Alternative Design Solution\n\nThe technical team analyzes how well each of the design \nalternatives meets the system goals (technology gaps, ef-\nfectiveness, cost, schedule, and risk, both quantified and \notherwise). This assessment is accomplished through \nthe use of trade studies. The purpose of the trade study \nprocess is to ensure that the system architecture and de-\nsign decisions move toward the best solution that can be \nachieved with the available resources. The basic steps in \nthat process are:\n\nDevise some alternative means to meet the functional  ?\nrequirements. In the early phases of the project life-\n\ncycle, this means focusing on system architectures; in \nlater phases, emphasis is given to system designs.\nEvaluate these alternatives in terms of the MOEs  ?\nand system cost. Mathematical models are useful in \nthis step not only for forcing recognition of the rela-\ntionships among the outcome variables, but also for \nhelping to determine what the measures of perfor-\nmance must be quantitatively.\nRank the alternatives according to appropriate selec- ?\ntion criteria.\nDrop less promising alternatives and proceed to the  ?\nnext level of resolution, if needed. \n\nThe trade study process must be done openly and in-\nclusively. While quantitative techniques and rules are \nused, subjectivity also plays a significant role. To make \nthe process work effectively, participants must have open \nminds, and individuals with different skills\u2014systems en-\ngineers, design engineers, specialty engineers, program \nanalysts, decision scientists, and project managers\u2014\nmust cooperate. The right quantitative methods and se-\nlection criteria must be used. Trade study assumptions, \nmodels, and results must be documented as part of the \nproject archives. The participants must remain focused \non the functional requirements, including those for en-\nabling products. For an in-depth discussion of the trade \nstudy process, see Section 6.8. The ability to perform \nthese studies is enhanced by the development of system \nmodels that relate the design parameters to those assess-\nments\u2014but it does not depend upon them. \n\nThe technical team must consider a broad range of con-\ncepts when developing the system model. The model \nmust define the roles of crew, hardware, and software in \nthe system. It must identify the critical technologies re-\nquired to implement the mission and must consider the \nentire life cycle, from fabrication to disposal. Evalu ation \ncriteria for selecting concepts must be established. Cost \nis always a limiting factor. However, other criteria, such \nas time to develop and certify a unit, risk, and re liability, \nalso are critical. This stage cannot be accom plished \nwithout addressing the roles of operators and main-\ntainers. These contribute significantly to life-cycle costs \nand to the system reliability. Reliability analysis should \nbe performed based upon estimates of compo nent \nfailure rates for hardware. If probabilistic risk as sessment \nmodels are applied, it may be necessary to in clude occur-\nrence rates or probabilities for software faults or human \nerror events. Assessments of the maturity of the required \n\n\n\n58 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\ntechnology must be done and a technology development \nplan developed.\n\nControlled modification and development of design con-\ncepts, together with such system models, often per mits \nthe use of formal optimization techniques to find regions \nof the design space that warrant further inves tigation.\n\nWhether system models are used or not, the design \ncon cepts are developed, modified, reassessed, and com-\npared against competing alternatives in a closed-loop \nprocess that seeks the best choices for further develop-\nment. System and subsystem sizes are often determined \nduring the trade studies. The end result is the determina-\ntion of bounds on the relative cost-effectiveness of the \ndesign alternatives, measured in terms of the quantified \nsystem goals. (Only bounds, rather than final values, are \npossible because determination of the final details of the \ndesign is intentionally deferred.) Increasing detail asso-\nciated with the continually improving resolution reduces \nthe spread between upper and lower bounds as the pro-\ncess proceeds.\n\nSelect the Best Design Solution Alternative\n\nThe technical team selects the best design solution from \namong the alternative design concepts, taking into ac-\ncount subjective factors that the team was unable to \nquantify as well as estimates of how well the alterna-\ntives meet the quantitative requirements; the maturity \nof the available technology; and any effectiveness, cost, \nschedule, risk, or other constraints.\n\nThe Decision Analysis Process, as described in Sec-\ntion 6.8, should be used to make an evaluation of the al-\nternative design concepts and to recommend the \u201cbest\u201d \ndesign solution.\n\nWhen it is possible, it is usually well worth the trouble \nto develop a mathematical expression, called an \u201cobjec-\ntive function,\u201d that expresses the values of combinations \nof possible outcomes as a single measure of cost-effec-\ntiveness, as illustrated in Figure 4.4-3, even if both cost \nand effectiveness must be described by more than one \nmeasure. \n\nThe objective function (or \u201ccost function\u201d) assigns a real \nnumber to candidate solutions or \u201cfeasible solutions\u201d in \nthe alternative space or \u201csearch space.\u201d A feasible solu-\ntion that minimizes (or maximizes, if that is the goal) the \nobjective function is called an \u201coptimal solution.\u201d When \n\nachievement of the goals can be quantitatively expressed \nby such an objective function, designs can be compared \nin terms of their value. Risks associated with design con-\ncepts can cause these evaluations to be somewhat nebu-\nlous (because they are uncertain and are best described \nby probability distributions). \n\nIn Figure 4.4-3, the risks are relatively high for design \nconcept A. There is little risk in either effectiveness or \ncost for concept B, while the risk of an expensive failure \nis high for concept C, as is shown by the cloud of prob-\nability near the x axis with a high cost and essentially no \neffectiveness. Schedule factors may affect the effective-\nness and cost values and the risk distributions.\n\nThe mission success criteria for systems differ signifi-\ncantly. In some cases, effectiveness goals may be much \nmore important than all others. Other projects may de-\nmand low costs, have an immutable schedule, or require \nminimization of some kinds of risks. Rarely (if ever) is \nit possible to produce a combined quantitative measure \nthat relates all of the important factors, even if it is ex-\npressed as a vector with several components. Even when \nthat can be done, it is essential that the underlying fac-\ntors and relationships be thoroughly revealed to and un-\nderstood by the systems engineer. The systems engineer \n\nFigure 4.4?3 A quantitative objective function, \ndependent on life?cycle cost and all aspects of \n\neffectiveness\nNote: The different shaded areas indicate different levels of \nuncertainty. Dashed lines represent constant values of objective \nfunction (cost-effectiveness). Higher values of cost-effectiveness \nare achieved by moving toward upper left. A, B, and C are design \nconcepts with different risk patterns.\n\nB\n\nC\n\nA\n\nLife-Cycle Cost, Expressed in Constant Dollars\n\nSo\nm\n\ne \nA\n\nsp\nec\n\nt o\nf E\n\n?\nec\n\nti\nve\n\nne\nss\n\n,\nEx\n\npr\nes\n\nse\nd \n\nin\n Q\n\nua\nnt\n\nit\nat\n\niv\ne \n\nU\nni\n\nts\n\n\n\n4.4 Design Solution Definition\n\nNASA Systems Engineering Handbook ? 59\n\nmust weigh the importance of the unquantifiable factors \nalong with the quantitative data.\n\nTechnical reviews of the data and analyses, including \ntechnology maturity assessments, are an important \npart of the decision support packages prepared for the \ntechnical team. The decisions that are made are gener-\nally entered into the configuration management system \nas changes to (or elaborations of) the system baseline. \nThe supporting trade studies are archived for future use. \nAn essential feature of the systems engineering process \nis that trade studies are performed before decisions are \nmade. They can then be baselined with much more con-\nfidence.\n\nIncrease the Resolution of the Design\n\nThe successive refinement process of Figure 4.4-2 illus-\ntrates a continuing refinement of the system design. At \neach level of decomposition, the baselined derived (and \nallocated) requirements become the set of high-level re-\nquirements for the decomposed elements, and the pro-\ncess begins again. One might ask, \u201cWhen do we stop re-\nfining the design?\u201d The answer is that the design eff ort \nprecedes to a depth that is sufficient to meet several \nneeds: the design must penetrate sufficiently to allow an-\nalytical validation of the design to the requirements; it \nmust also have sufficient depth to support cost modeling \nand to convince a review team of a feasible design with \nperformance, cost, and risk margins.\n\nThe systems engineering engine is applied again and \nagain as the system is developed. As the system is real-\nized, the issues addressed evolve and the particulars of \nthe activity change. Most of the major system decisions \n(goals, architecture, acceptable life-cycle cost, etc.) are \nmade during the early phases of the project, so the suc-\ncessive refinements do not correspond precisely to the \nphases of the system life cycle. Much of the system archi-\ntecture can be seen even at the outset, so the successive \nrefinements do not correspond exactly to development \nof the architectural hierarchy, either. Rather, they corre-\nspond to the successively greater resolution by which the \nsystem is defined.\n\nIt is reasonable to expect the system to be defined with \nbetter resolution as time passes. This tendency is formal-\nized at some point (in Phase B) by defining a baseline \nsystem definition. Usually, the goals, objectives, and con-\nstraints are baselined as the requirements portion of the \nbaseline. The entire baseline is then subjected to config-\n\nuration control in an attempt to ensure that any subse-\nquent changes are indeed justified and affordable.\n\nAt this point in the systems engineering process, there is \na logical branch point. For those issues for which the pro-\ncess of successive refinement has proceeded far enough, \nthe next step is to implement the decisions at that level \nof resolution. For those issues that are still insufficiently \nresolved, the next step is to refine the development fur-\nther.\n\nFully Describe the Design Solution\n\nOnce the preferred design alternative has been selected \nand the proper level of refinement has been completed, \nthen the design is fully defined into a final design solu-\ntion that will satisfy the technical requirements. The de-\nsign solution definition will be used to generate the end \nproduct specifications that will be used to produce the \nproduct and to conduct product verification. This pro-\ncess may be further refined depending on whether there \nare additional subsystems of the end product that need \nto be defined. \n\nThe scope and content of the full design description \nmust be appropriate for the product life-cycle phase, the \nphase success criteria, and the product position in the \nPBS (system structure). Depending on these factors, the \nform of the design solution definition could be simply a \nsimulation model or a paper study report. The technical \ndata package evolves from phase to phase, starting with \nconceptual sketches or models and ending with com plete \ndrawings, parts list, and other details needed for product \nimplementation or product integration. Typical output \ndefinitions from the Design Solution Definition Process \nare shown in Figure 4.4-1 and are described in Subsec-\ntion 4.4.1.3. \n\nVerify the Design Solution\n\nOnce an acceptable design solution has been selected \nfrom among the various alternative designs and docu-\nmented in a technical data package, the design solution \nmust next be verified against the system requirements \nand constraints. A method to achieve this verification \nis by means of a peer review to evaluate the resulting \nde sign solution definition. Guidelines for conducting a \npeer review are discussed in Section 6.7.\n\nIn addition, peer reviews play a significant role as a de-\ntailed technical component of higher level technical and \n\n\n\n60 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nprogrammatic reviews. For example, the peer review of \na component battery design can go into much more tech-\nnical detail on the battery than the integrated power sub-\nsystem review. Peer reviews can cover the components of \na subsystem down to the level appropriate for verifying \nthe design against the requirements. Concerns raised at \nthe peer review might have implications on the power \nsubsystem design and verification and therefore must \nbe reported at the next higher level review of the power \nsubsystem.\n\nThe verification must show that the design solution defi-\nnition:\n\nIs realizable within the constraints imposed on the  ?\ntechnical effort;\nHas specified requirements that are stated in accept- ?\nable statements and have bidirectional traceability \nwith the derived technical requirements, technical re-\nquirements, and stakeholder expectations; and\nHas decisions and assumptions made in forming the  ?\nsolution consistent with its set of derived technical \nrequirements, separately allocated technical require-\nments, and identified system product and service \nconstraints.\n\nThis design solution verification is in contrast to the \nverification of the end product described in the end \nproduct verification plan which is part of the technical \ndata package. That verification occurs in a later life-cycle \nphase and is a result of the Product Verification Process \n(see Section 5.3) applied to the realization of the design \nsolution as an end product.\n\nValidate the Design Solution\n\nThe validation of the design solution is a recursive and \niterative process as shown in Figure 4.0-1. Each alterna-\ntive design concept is validated against the set of stake-\nholder expectations. The stakeholder expectations drive \nthe iterative design loop in which a strawman architec-\nture/design, the ConOps, and the derived requirements \nare developed. These three products must be consistent \nwith each other and will require iterations and design \ndecisions to achieve this consistency. Once consistency \nis achieved, functional analyses allow the study team \nto validate the design against the stakeholder expecta-\ntions. A simplified validation asks the questions: Does \nthe system work? Is the system safe and reliable? Is the \nsystem affordable? If the answer to any of these questions \nis no, then changes to the design or stakeholder expec-\n\ntations will be required, and the process is started over \nagain. This process continues until the system\u2014architec-\nture, ConOps, and requirements\u2014meets the stakeholder \nexpectations.\n\nThis design solution validation is in contrast to the vali-\ndation of the end product described in the end product \nvalidation plan, which is part of the technical data \npackage. That validation occurs in a later life-cycle phase \nand is a result of the Product Validation Process (see Sec-\ntion 5.4) applied to the realization of the design solution \nas an end product.\n\nIdentify Enabling Products\n\nEnabling products are the life-cycle support products \nand services (e.g., production, test, deployment, training, \nmaintenance, and disposal) that facilitate the progression \nand use of the operational end product through its life \ncycle. Since the end product and its enabling products \nare interdependent, they are viewed as a system. Project \nresponsibility thus extends to responsibility for acquiring \nservices from the relevant enabling products in each life-\ncycle phase. When a suitable enabling product does not \nalready exist, the project that is responsible for the end \nproduct also can be responsible for creating and using \nthe enabling product. \n\nTherefore, an important activity in the Design Solution \nDefinition Process is the identification of the enabling \nproducts that will be required during the life cycle of the \nselected design solution and then initiating the acquisi-\ntion or development of those enabling products. Need \ndates for the enabling products must be realistically \nidentified on the project schedules, incorporating ap-\npropriate schedule slack. Then firm commitments in the \nform of contracts, agreements, and/or operational plans \nmust be put in place to ensure that the enabling products \nwill be available when needed to support the product-\nline life-cycle phase activities. The enabling product re-\nquirements are documented as part of the technical data \npackage for the Design Solution Definition Process.\n\nAn environmental test chamber would be an example of \nan enabling product whose use would be acquired at an \nappropriate time during the test phase of a space flight \nsystem.\n\nSpecial test fixtures or special mechanical handling de-\nvices would be examples of enabling products that \nwould have to be created by the project. Because of long \n\n\n\n4.4 Design Solution Definition\n\nNASA Systems Engineering Handbook ? 61\n\nde velopment times as well as oversubscribed facilities, it \nis important to identify enabling products and secure the \ncommitments for them as early in the design phase as \npossible. \n\nBaseline the Design Solution\n\nAs shown earlier in Figure 4.0-1, once the selected system \ndesign solution meets the stakeholder expectations, the \nstudy team baselines the products and prepares for the \nnext life-cycle phase. Because of the recursive nature of \nsuccessive refinement, intermediate levels of decomposi-\ntion are often validated and baselined as part of the pro-\ncess. In the next level of decomposition, the baselined \nrequirements become the set of high-level requirements \nfor the decomposed elements, and the process begins \nagain. \n\nBaselining a particular design solution enables the tech-\nnical team to focus on one design out of all the alterna-\ntive design concepts. This is a critical point in the design \nprocess. It puts a stake in the ground and gets everyone \non the design team focused on the same concept. When \ndealing with complex systems, it is difficult for team \nmembers to design their portion of the system if the \nsystem design is a moving target. The baselined design \nis documented and placed under configuration control. \nThis includes the system requirements, specifications, \nand configuration descriptions. \n\nWhile baselining a design is beneficial to the design pro-\ncess, there is a danger if it is exercised too early in the De-\nsign Solution Definition Process. The early exploration \nof alternative designs should be free and open to a wide \nrange of ideas, concepts, and implementations. Base-\nlining too early takes the inventive nature out of the con-\ncept exploration. Therefore baselining should be one of \nthe last steps in the Design Solution Definition Process. \n\n4.4.1.3 Outputs\n\nOutputs of the Design Solution Definition Process are \nthe specifications and plans that are passed on to the \nproduct realization processes. They contain the design-\nto, build-to, and code-to documentation that complies \nwith the approved baseline for the system.\n\nAs mentioned earlier, the scope and content of the full \ndesign description must be appropriate for the product-\nline life-cycle phase, the phase success criteria, and the \nproduct position in the PBS.\n\nOutputs of the Design Solution Definition Process in-\nclude the following:\n\nThe System Specification: ?  The system specification \ncontains the functional baseline for the system that is \nthe result of the Design Solution Definition Process. \nThe system design specification provides sufficient \nguidance, constraints, and system requirements for \nthe design engineers to execute the design.\nThe System External Interface Specifications: ?  The \nsystem external interface specifications describe the \nfunctional baseline for the behavior and character-\nistics of all physical interfaces that the system has \nwith the external world. These include all structural, \nthermal, electrical, and signal interfaces, as well as the \nhuman-system interfaces.\nThe End-Product Specifications: ?  The end-product \nspecifications contain the detailed build-to and code-\nto requirements for the end product. They are de-\ntailed, exact statements of design particulars, such \nas statements prescribing materials, dimensions, and \nquality of work to build, install, or manufacture the \nend product.\nThe End-Product Interface Specifications: ?  The \nend-product interface specifications contain the \ndetailed build-to and code-to requirements for \nthe behavior and characteristics of all logical and \nphysical inter faces that the end product has with \nexternal elements, including the human-system in-\nterfaces.\nInitial Subsystem Specifications: ?  The end-product \nsubsystem initial specifications provide detailed in-\nformation on subsystems if they are required.\nEnabling Product Requirements: ?  The requirements \nfor associated supporting enabling products provide \ndetails of all enabling products. Enabling products are \nthe life-cycle support products and services that fa-\ncilitate the progression and use of the operational end \nproduct through its life cycle. They are viewed as part \nof the system since the end product and its enabling \nproducts are interdependent.\nProduct Verification Plan:  ? The end-product verifica-\ntion plan provides the content and depth of detail nec-\nessary to provide full visibility of all verification activ-\nities for the end product. Depending on the scope of \nthe end product, the plan encompasses qualification, \nacceptance, prelaunch, operational, and disposal veri-\nfication activities for flight hardware and software.\n\n\n\n62 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nProduct Validation Plan:  ? The end-product valida-\ntion plan provides the content and depth of detail \nnecessary to provide full visibility of all activities to \nvalidate the realized product against the baselined \nstakeholder expectations. The plan identifies the type \nof validation, the validation procedures, and the vali-\ndation environment that are appropriate to confirm \nthat the realized end product conforms to stakeholder \nexpectations. \nLogistics and Operate-to Procedures:  ? The applicable \nlogistics and operate-to procedures for the system de-\nscribe such things as handling, transportation, main-\ntenance, long-term storage, and operational consider-\nations for the particular design solution.\n\n4.4.2 Design Solution Definition Guidance\n\n4.4.2.1 Technology Assessment\n\nAs mentioned in the process description (Subsec-\ntion 4.4.1), the creation of alternative design solutions in-\nvolves assessment of potential capabilities offered by the \ncontinually changing state of technology. A continual in-\nteraction between the technology development process \nand the design process ensures that the design reflects \nthe realities of the available technology. This interaction \nis facilitated through periodic assessment of the design \nwith respect to the maturity of the technology required \nto implement the design.\n\nAfter identifying the technology gaps existing in a given \ndesign concept, it will frequently be necessary to under-\ntake technology development in order to ascertain via-\nbility. Given that resources will always be limited, it will \nbe necessary to pursue only the most promising technol-\nogies that are required to enable a given concept. \n\nIf requirements are defined without fully understanding \nthe resources required to accomplish needed technology \ndevelopments then the program/project is at risk. Tech-\nnology assessment must be done iteratively until require-\nments and available resources are aligned within an ac-\nceptable risk posture. Technology development plays a \nfar greater role in the life cycle of a program/project than \nhas been traditionally considered, and it is the role of the \nsystems engineer to develop an understanding of the ex-\ntent of program/project impacts\u2014maximizing benefits \nand minimizing adverse effects. Traditionally, from a \nprogram/project perspective, technology development \nhas been associated with the development and incor-\n\nporation of any \u201cnew\u201d technology necessary to meet re-\nquirements. However, a frequently overlooked area is \nthat associated with the modification of \u201cheritage\u201d sys-\ntems incorporated into different architectures and oper-\nating in different environments from the ones for which \nthey were designed. If the required modifications and/\nor operating environments fall outside the realm of expe-\nrience, then these too should be considered technology \ndevelopment. \n\nTo understand whether or not technology development \nis required\u2014and to subsequently quantify the associated \ncost, schedule, and risk\u2014it is necessary to systematically \nassess the maturity of each system, subsystem, or com-\nponent in terms of the architecture and operational en-\nvironment. It is then necessary to assess what is required in \nthe way of development to advance the maturity to a point \nwhere it can successfully be incorporated within cost, \nschedule, and performance constraints. A process for ac-\ncomplishing this assessment is described in Ap pendix G. \nBecause technology development has the po tential for \nsuch significant impacts on a program/project, technology \nassessment needs to play a role throughout the design and \ndevelopment process from concept de velopment through \nPreliminary Design Review (PDR). Lessons learned from \na technology development point of view should then be \ncaptured in the final phase of the program.\n\n4.4.2.2 Integrating Engineering Specialties \ninto the Systems Engineering Process\n\nAs part of the technical effort, specialty engineers in \nco operation with systems engineering and subsystem \nde signers often perform tasks that are common across \ndisciplines. Foremost, they apply specialized analytical \ntechniques to create information needed by the project \nmanager and systems engineer. They also help define \nand write system requirements in their areas of expertise, \nand they review data packages, Engineering Change Re-\nquests (ECRs), test results, and documentation for major \nproject reviews. The project manager and/or systems en-\ngineer needs to ensure that the information and prod-\nucts so generated add value to the project commensurate \nwith their cost. The specialty engineering technical effort \nshould be well integrated into the project. The roles and \nresponsibilities of the specialty engineering disciplines \nshould be summarized in the SEMP.\n\nThe specialty engineering disciplines included in this \nhandbook are safety and reliability, Quality Assurance \n\n\n\n4.4 Design Solution Definition\n\nNASA Systems Engineering Handbook ? 63\n\n(QA), ILS, maintainability, producibility, and human \nfactors. An overview of these specialty engineering dis-\nciplines is provided to give systems engineers a brief in-\ntroduction. It is not intended to be a handbook for any of \nthese discipline specialties.\n\nSafety and Reliability\n\nOverview and Purpose\nA reliable system ensures mission success by functioning \nproperly over its intended life. It has a low and accept able \nprobability of failure, achieved through simplicity, proper \ndesign, and proper application of reliable parts and mate-\nrials. In addition to long life, a reliable system is robust and \nfault tolerant, meaning it can tolerate fail ures and varia-\ntions in its operating parameters and en vironments.\n\nSafety and Reliability in the System Design \nProcess\n\nA focus on safety and reliability throughout the mission \nlife cycle is essential for ensuring mission success. The \nfidelity to which safety and reliability are designed and \nbuilt into the system depends on the information needed \nand the type of mission. For human-rated systems, safety \nand reliability is the primary objective throughout the \ndesign process. For science missions, safety and reli-\nability should be commensurate with the funding and \nlevel of risk a program or project is willing to accept. Re-\ngardless of the type of mission, safety and reliability con-\nsiderations must be an intricate part of the system design \nprocesses.\n\nTo realize the maximum benefit from reliability analysis, \nit is essential to integrate the risk and reliability analysts \nwithin the design teams. The importance of this cannot \nbe overstated. In many cases, the reliability and risk ana-\nlysts perform the analysis on the design after it has been \nformulated. In this case, safety and reliability features are \nadded on or outsourced rather than designed in. This \nresults in unrealistic analysis that is not focused on risk \ndrivers and does not provide value to the design. \n\nRisk and reliability analyses evolve to answer key ques-\ntions about design trades as the design matures. Reli-\nability analyses utilize information about the system, \nidentify sources of risk and risk drivers, and provide \nan important input for decisionmaking. NASA-STD-\n8729.1, Planning, Developing, and Maintaining an Ef-\nfective Reli ability and Maintainability (R&M) Program \noutlines en gineering activities that should be tailored \n\nfor each spe cific project. The concept is to choose an ef-\nfective set of reliability and maintainability engineering \nactivities to ensure that the systems designed, built, and \ndeployed will operate successfully for the required mis-\nsion life cycle.\n\nIn the early phases of a project, risk and reliability anal-\nyses help designers understand the interrelationships of \nrequirements, constraints, and resources, and uncover \nkey relationships and drivers so they can be properly con-\nsidered. The analyst must help designers go beyond the \nrequirements to understand implicit dependencies that \nemerge as the design concept matures. It is unrealistic to \nassume that design requirements will correctly capture \nall risk and reliability issues and \u201cforce\u201d a reliable design. \nThe systems engineer should develop a system strategy \nmapped to the PBS on how to allocate and coordinate \nreliability, fault tolerance, and recovery between systems \nboth horizontally and vertically within the architecture \nto meet the total mission requirements. System impacts \nof designs must play a key role in the design. Making \ndesigners aware of impacts of their decisions on overall \nmission reliability is key. \n\nAs the design matures, preliminary reliability analysis \noccurs using established techniques. The design and \nconcept of operations should be thoroughly examined \nfor accident initiators and hazards that could lead to \nmishaps. Conservative estimates of likelihood and con-\nsequences of the hazards can be used as a basis for ap-\nplying design resources to reduce the risk of failures. The \nteam should also ensure that the goals can be met and \nfailure modes are considered and take into account the \nentire system.\n\nDuring the latter phases of a project, the team uses risk \nassessments and reliability techniques to verify that the \ndesign is meeting its risk and reliability goals and to help \ndevelop mitigation strategies when the goals are not met \nor discrepancies/failures occur. \n\nAnalysis Techniques and Methods\nThis subsection provides a brief summary of the types of \nanalysis techniques and methods.\n\nEvent sequence diagrams/event trees are models that  ?\ndescribe the sequence of events and responses to off-\nnominal conditions that can occur during a mission.\nFailure Modes and Effects Analyses (FMEAs) are  ?\nbottom-up analyses that identify the types of failures \n\n\n\n64 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nthat can occur within a system and identify the causes, \neffects, and mitigating strategies that can be employed \nto control the effects of the failures. \nQualitative top-down logic models identify how fail- ?\nures within a system can combine to cause an unde-\nsired event.\nQuantitative logic models (probabilistic risk assess- ?\nment) extend the qualitative models to include the \nlikelihood of failure. These models involve developing \nfailure criteria based on system physics and system \nsuccess criteria, and employing statistical techniques \nto estimate the likelihood of failure along with uncer-\ntainty. \nReliability block diagrams are diagrams of the ele- ?\nments to evaluate the reliability of a system to provide \na function. \nPreliminary Hazard Analysis (PHA) is performed  ?\nearly based on the functions performed during the \nmission. Preliminary hazard analysis is a \u201cwhat if \u201d \nprocess that considers the potential hazard, initiating \nevent scenarios, effects, and potential corrective mea-\nsures and controls. The objective is to determine if the \nhazard can be eliminated, and if not, how it can be \ncontrolled.\nHazard analysis evaluates the completed design.  ?\nHazard analysis is a \u201cwhat if \u201d process that considers \nthe potential hazard, initiating event, effects, and po-\ntential corrective measures and controls. The objec-\ntive is to determine if the hazard can be eliminated, \nand if not, how it can be controlled.\nHuman reliability analysis is a method to understand  ?\nhow human failures can lead to system failure and es-\ntimate the likelihood of those failures.\nProbabilistic structural analysis provides a way to  ?\ncombine uncertainties in materials and loads to eval-\nuate the failure of a structural element.\nSparing/logistics models provide a means to estimate  ?\nthe interactions of systems in time. These models in-\nclude ground-processing simulations and mission \ncampaign simulations.\n\nLimitations on Reliability Analysis\nThe engineering design team must understand that reli-\nability is expressed as the probability of mission success. \nProbability is a mathematical measure expressing the \nlikelihood of occurrence of a specific event. Therefore, \nprobability estimates should be based on engineering \n\nand historical data, and any stated probabilities should \ninclude some measure of the uncertainty surrounding \nthat estimate. \n\nUncertainty expresses the degree of belief analysts have \nin their estimates. Uncertainty decreases as the quality of \ndata and understanding of the system improve. The ini-\ntial estimates of failure rates or failure probability might \nbe based on comparison to similar equipment, historical \ndata (heritage), failure rate data from handbooks, or ex-\npert elicitation.\n\nIn summary, \nReliability estimates express probability of success.  ?\nUncertainty should be included with reliability esti- ?\nmates.\nReliability estimates combined with FMEAs provide  ?\nadditional and valuable information to aid in the de-\ncisionmaking process.\n\nQuality Assurance\n\nEven with the best designs, hardware fabrication and \ntesting are subject to human error. The systems engineer \nneeds to have some confidence that the system actually \nproduced and delivered is in accordance with its func-\ntional, performance, and design requirements. QA pro-\nvides an independent assessment to the project manager/\nsystems engineer of the items produced and processes \nused during the project life cycle. The project manager/\nsystems engineer must work with the quality assurance \nengineer to develop a quality assurance program (the ex-\ntent, responsibility, and timing of QA activities) tailored \nto the project it supports.\n\nQA is the mainstay of quality as practiced at NASA. \nNPD 8730.5, NASA Quality Assurance Program Policy \nstates that NASA\u2019s policy is \u201cto comply with prescribed \nre quirements for performance of work and to provide \nfor independent assurance of compliance through imple-\nmentation of a quality assurance program.\u201d The quality \nfunction of Safety and Mission Assurance (SMA) en-\nsures that both contractors and other NASA functions \ndo what they say they will do and say what they intend to \ndo. This ensures that end product and program quality, \nreliability, and overall risk are at the level planned.\n\nThe Systems Engineer\u2019s Relationship to QA \nAs with reliability, producibility, and other characteris-\ntics, quality must be designed as an integral part of any \n\n\n\n4.4 Design Solution Definition\n\nNASA Systems Engineering Handbook ? 65\n\nsystem. It is important that the systems engineer under-\nstands SMA\u2019s safeguarding role in the broad context of \ntotal risk and supports the quality role explicitly and vig-\norously. All of this is easier if the SMA quality function is \nactively included and if quality is designed in with buy-\nin by all roles, starting at concept development. This will \nhelp mitigate conflicts between design and quality re-\nquirements, which can take on the effect of \u201ctolerance \nstacking.\u201d\n\nQuality is a vital part of risk management. Errors, vari-\nability, omissions, and other problems cost time, pro-\ngram resources, taxpayer dollars, and even lives. It is in-\ncumbent on the systems engineer to know how quality \naffects their projects and to encourage best practices to \nachieve the quality level.\n\nRigid adherence to procedural requirements is necessary \nin high-risk, low-volume manufacturing. In the absence \nof large samples and long production runs, compliance \nto these written procedures is a strong step toward en-\nsuring process, and, thereby, product consistency. To ad-\ndress this, NASA requires QA programs to be designed \nto mitigate risks associated with noncompliance to those \nrequirements. \n\nThere will be a large number of requirements and pro-\ncedures thus created. These must be flowed down to the \nsupply chain, even to lowest tier suppliers. For circum-\nstances where noncompliance can result in loss of life \nor loss of mission, there is a requirement to insert into \nprocedures Government Mandatory Inspection Points \n(GMIPs) to ensure 100 percent compliance with safety/\nmission-critical attributes. Safety/mission-critical attri-\nbutes include hardware characteristics, manufacturing \nprocess requirements, operating conditions, and func-\ntional performance criteria that, if not met, can result \nin loss of life or loss of mission. There will be in place \na Program/Project Quality Assurance Surveillance Plan \n(PQASP) as mandated by Federal Acquisition Regula-\ntion (FAR) Subpart 46.4. Preparation and content for \nPQASPs are outlined in NPR 8735.2, Management of \nGovernment Quality Assurance Functions for NASA Con-\ntracts. This document covers quality assurance require-\nments for both low-risk and high-risk acquisitions and \nincludes functions such as document review, product \nexamination, process witnessing, quality system evalu-\nation, nonconformance reporting and corrective action, \nplanning for quality assurance and surveillance, and \nGMIPs. In addition, most NASA projects are required to \n\nadhere to either ISO 9001 (noncritical work) or AS9100 \n(critical work) requirements for management of quality \nsystems. Training in these systems is mandatory for most \nNASA functions, so knowledge of their applicability by \nthe systems engineer is assumed. Their texts and intent \nare strongly reflected in NASA\u2019s quality procedural doc-\numents.\n\nIntegrated Logistics Support\n\nThe objective of ILS activities within the systems engi-\nneering process is to ensure that the product system is \nsupported during development (Phase D) and opera-\ntions (Phase E) in a cost-effective manner. ILS is particu-\nlarly important to projects that are reusable or service-\nable. Projects whose primary product does not evolve \nover its operations phase typically only apply ILS to \nparts of the project (for example, the ground system) or \nto some of the elements (for example, transportation). \nILS is primarily accomplished by early, concurrent con-\nsideration of supportability characteristics; performing \ntrade studies on alternative system and ILS concepts; \nquantifying resource requirements for each ILS element \nusing best practices; and acquiring the sup port items as-\nsociated with each ILS element. During op erations, ILS \nactivities support the system while seeking improve-\nments in cost-effectiveness by conducting anal yses in re-\nsponse to actual operational conditions. These analyses \ncontinually reshape the ILS system and its re source re-\nquirements. Neglecting ILS or poor ILS deci sions in-\nvariably have adverse effects on the life-cycle cost of the \nresultant system. Table 4.4-1 summarizes the ILS disci-\nplines. \n\nILS planning should begin early in the project life cycle \nand should be documented. This plan should address the \nelements above including how they will be considered, \nconducted, and integrated into the systems engineering \nprocess needs.\n\nMaintainability\n\nMaintainability is defined as the measure of the ability \nof an item to be retained in or restored to specified con-\nditions when maintenance is performed by personnel \nhaving specified skill levels, using prescribed procedures \nand resources, at each prescribed level of maintenance. It \nis the inherent characteristics of a design or installation \nthat contribute to the ease, economy, safety, and accu racy \nwith which maintenance actions can be performed.\n\n\n\n66 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nRole of the Maintainability Engineer\nMaintainability engineering is another major specialty \ndiscipline that contributes to the goal of a supportable \nsystem. This is primarily accomplished in the systems \nengineering process through an active role in imple-\nmenting specific design features to facilitate safe and \neffective maintenance actions in the predicted physical \nenvironments, and through a central role in developing \nthe ILS system. Example tasks of the maintainability en-\ngineer include: developing and maintaining a system \nmaintenance concept, establishing and allocating main-\ntainability requirements, performing analysis to quantify \nthe system\u2019s maintenance resource requirements, and \nverifying the system\u2019s maintainability requirements.\n\nProducibility\n\nProducibility is a system characteristic associated with \nthe ease and economy with which a completed design \ncan be transformed (i.e., fabricated, manufactured, or \ncoded) into a hardware and/or software realization. \nWhile major NASA systems tend to be produced in small \nquantities, a particular producibility feature can be crit-\n\nical to a system\u2019s cost-effectiveness, as experience with \nthe shuttle\u2019s thermal tiles has shown. Factors that influ-\nence the producibility of a design include the choice of \nmaterials, simplicity of design, flexibility in production \nalternatives, tight tolerance requirements, and clarity \nand simplicity of the technical data package.\n\nRole of the Production Engineer\nThe production engineer supports the systems engineer-\ning process (as a part of the multidisciplinary product \ndevelopment team) by taking an active role in imple-\nmenting specific design features to enhance producibility \nand by performing the production engineering analyses \nneeded by the project. These tasks and analyses include: \n\nPerforming the manufacturing/fabrication portion  ?\nof the system risk management program. This is ac-\ncomplished by conducting a rigorous production risk \nassessment and by planning effective risk mitigation \nactions.\nIdentifying system design features that enhance pro- ?\nducibility. Efforts usually focus on design simplifica-\n\nTable 4.4?1 ILS Technical Disciplines\n\nTechnical Discipline  Definition\n\nMaintenance sup port \nplanning\n\nOngoing and iterative planning, organization, and management activities necessary to ensure \nthat the logistics requirements for any given program are properly coordinated and implemented\n\nDesign interface The interaction and relationship of logistics with the systems engineering process to ensure that \nsupportability influences the definition and design of the system so as to reduce life-cycle cost\n\nTechnical data and \ntechnical publica tions\n\nThe recorded scientific, engineering, technical, and cost information used to define, produce, test, \nevaluate, modify, deliver, support, and operate the system\n\nTraining and training \nsupport\n\nEncompasses all personnel, equipment, facilities, data/documentation, and associated resources \nnecessary for the training of operational and maintenance personnel\n\nSupply support Actions required to provide all the necessary material to ensure the system\u2019s supportability and \nusability objectives are met\n\nTest and support \nequipment\n\nAll tools, condition-monitoring equipment, diagnostic and checkout equipment, special test \nequipment, metrology and calibration equipment, maintenance fixtures and stands, and special \nhandling equipment required to support operational maintenance functions\n\nPackaging, handling, \nstorage, and trans-\nportation\n\nAll materials, equipment, special provisions, containers (reusable and disposable), and supplies \nnecessary to support the packaging, safety and preservation, storage, handling, and transporta-\ntion of the prime mission-related elements of the system, including personnel, spare and repair \nparts, test and support equipment, technical data computer resources, and mobile facilities\n\nPersonnel Involves identification and acquisition of personnel with skills and grades required to operate and \nmaintain a system over its lifetime\n\nLogistics facilities All special facilities that are unique and are required to support logistics activities, including stor-\nage buildings and warehouses and maintenance facilities at all levels\n\nComputer resources \nsupport\n\nAll computers, associated software, connecting components, networks, and interfaces necessary \nto support the day-to-day flow of information for all logistics functions\n\nSource: Blanchard, System Engineering Management.\n\n\n\n4.4 Design Solution Definition\n\nNASA Systems Engineering Handbook ? 67\n\ntion, fabrication tolerances, and avoidance of haz-\nardous materials.\nConducting producibility trade studies to determine  ?\nthe most cost-effective fabrication/manufacturing \nprocess.\nAssessing production feasibility within project con- ?\nstraints. This may include assessing contractor and \nprincipal subcontractor production experience and \ncapability, new fabrication technology, special tooling, \nand production personnel training requirements.\nIdentifying long-lead items and critical materials. ?\nEstimating production costs as a part of life-cycle cost  ?\nmanagement.\nSupporting technology readiness assessments. ?\nDeveloping production schedules. ?\nDeveloping approaches and plans to validate fabrica- ?\ntion/manufacturing processes. \n\nThe results of these tasks and production engineering \nanalyses are documented in the manufacturing plan \nwith a level of detail appropriate to the phase of the \nproject. The production engineer also participates in and \ncon tributes to major project reviews (primarily PDR and \nCritical Design Review (CDR)) on the above items, and \nto special interim reviews such as the PRR.\n\nHuman Factors Engineering\n\nOverview and Purpose\nConsideration of human operators and maintainers of \nsystems is a critical part of the design process. Human \nfactors engineering is the discipline that studies the \nhuman-system interfaces and provides requirements, \nstandards, and guidelines to ensure the human compo-\nnent of the integrated system is able to function as in-\ntended. Human roles include operators (flight crews \nand ground crews), designers, manufacturers, ground \nsup port, maintainers, and passengers. Flight crew \nfunctions include system operation, troubleshooting, \nand in-flight maintenance. Ground crew functions in-\nclude space craft and ground system manufacturing, as-\nsembly, test, checkout, logistics, ground maintenance, \nrepair, refur bishment, launch control, and mission con-\ntrol.\n\nHuman factors are generally considered in four catego-\nries. The first is anthropometry and biomechanics\u2014\nthe physical size, shape, and strength of the humans. \nThe second is sensation and perception\u2014primarily \nvision and hearing, but senses such as touch are also \nimportant. The environment is a third factor\u2014am-\nbient noise and lighting, vibration, temperature and \nhumidity, atmo spheric composition, and contami-\nnants. Psychological factors comprise memory; in-\nformation processing com ponents such as pattern \nrecognition, decisionmaking, and signal detection; \nand affective factors\u2014e.g., emo tions, cultural pat-\nterns, and habits.\n\nHuman Factors Engineering in the System \nDesign Process\n\nStakeholder Expectations:  ? The operators, main-\ntainers, and passengers are all stakeholders in the \nsystem. The human factors specialist identifies roles \nand responsibilities that can be performed by hu-\nmans and scenarios that exceed human capabilities. \nThe human factors specialist ensures that system op-\nerational concept development includes task anal-\nysis and human/system function allocation. As these \nare refined, function allocation distributes operator \nroles and responsibilities for subtasks to the crew, ex-\nternal support teams, and automation. (For example, \nin aviation, tasks may be allocated to crew, air traffic \ncontrollers, or autopilots. In spacecraft, tasks may be \nperformed by crew, mission control, or onboard sys-\ntems.) \n\nPrototypes\n\nExperience has shown that prototype systems can be \neffective in enabling efficient producibility even when \nbuilding only a single flight system. Prototypes are \nbuilt early in the life cycle and they are made as close \nto the flight item in form, fit, and function as is feasi-\nble at that stage of the development. The prototype \nis used to \u201cwring out\u201d the design solution so that ex-\nperience gained from the prototype can be fed back \ninto design changes that will improve the manufac-\nture, integration, and maintainability of a single flight \nitem or the production run of several flight items. Un-\nfortunately, prototypes are often deleted from proj-\nects to save cost. Along with that decision, the proj-\nect accepts an increased risk in the development \nphase of the life cycle. Fortunately, advancements in \ncom puter-aided design and manufacturing have miti-\ngated that risk somewhat by enabling the designer \nto visualize the design and \u201cwalk through\u201d the integra-\ntion sequence to uncover problems before they be-\ncome a costly reality.\n\n\n\n68 ? NASA Systems Engineering Handbook\n\n4.0 System Design\n\nRequirements Definition: ?  Human factors require-\nments for spacecraft and space habitats are program/\nproject dependent, derived from NASA-STD-3001, \nNASA Space Flight Human System Standard Volume 1: \nCrew Health. Other human factors requirements of \nother missions and Earth-based activities for human \nspace flight missions are derived from human fac-\ntors standards such as MIL-STD-1472, Human En-\ngineering; NUREG-0700, Human-System Interface \nDesign Review Guidelines; and the Federal Aviation \nAdministration\u2019s Human Factors Design Standard.\nTechnical Solution:  ? Consider the human as a central \ncomponent when doing logical decomposition and \ndeveloping design concepts. The users\u2014operators or \nmaintainers\u2014will not see the entire system as the de-\nsigner does, only as the system interfaces with them. \nIn engineering design reviews, human factors spe-\ncialists promote the usability of the design solution. \nWith early involvement, human factors assessments \nmay catch usability problems at very early stages. \nFor example, in one International Space Station pay-\nload design project, a human factors assessment of a \nvery early block diagram of the layout of stowage and \nhardware identified problems that would have made \noperations very difficult. Changes were made to the \nconceptual design at negligible cost\u2014i.e., rearranging \nconceptual block diagrams based on the sequence in \nwhich users would access items.\nUsability Evaluations of Design Concepts: ?  Evalua-\ntions can be performed easily using rapid prototyping \ntools for hardware and software interfaces, standard \nhuman factors engineering data-gathering and anal-\nysis tools, and metrics such as task completion time \nand number of errors. Systematically collected sub-\njective reports from operators also provide useful \ndata. New technologies provide detailed objective in-\nformation\u2014e.g., eye tracking for display and control \nlayout assessment. Human factors specialists provide \nassessment capabilities throughout the iterative de-\nsign process.\nVerification: ?  As mentioned, verification of require-\nments for usability, error rates, task completion times, \nand workload is challenging. Methods range from tests \nwith trained personnel in mockups and simula tors, to \nmodels of human performance, to inspection by ex-\nperts. As members of the systems engineering team, \nhuman factors specialists provide verification guidance \nfrom the time requirements are first devel oped.\n\nHuman Factors Engineering Analyses \nTechniques and Methods\n\nExample methods used to provide human performance \ndata, predict human-system performance, and evaluate \nhuman-system designs include:\n\nTask Analysis: ?  Produces a detailed description of the \nthings a person must do in a system to accomplish a \ntask, with emphasis on requirements for information \npresentation, decisions to be made, task times, oper-\nator actions, and environmental conditions. \nTimeline Analysis: ?  Follows from task analysis. Dura-\ntions of tasks are identified in task analyses, and the \ntimes at which these tasks occur are plotted in graphs, \nwhich also show the task sequences. The purpose is to \nidentify requirements for simultaneous incompatible \nactivities and activities that take longer than is avail-\nable. Timelines for a given task can describe the activ-\nities of multiple operators or crewmembers.\nModeling and Simulation: ?  Models or mockups to \nmake predictions about system performance, com-\npare configurations, evaluate procedures, and eval-\nuate alternatives. Simulations can be as simple as \npositioning a graphical human model with realistic \nanthropometric dimensions with a graphical model \nof an operator station, or they can be complex sto-\nchastic models capturing decision points, error op-\nportunities, etc.\nUsability Testing: ?  Based on a task analysis and pre-\nliminary design, realistic tasks are carried out in a con-\ntrolled environment with monitoring and re cording \nequipment. Objective measures such as per formance \ntime and number of errors are evaluated; subjective \nratings are collected. The outputs system atically re-\nport on strengths and weaknesses of candi date design \nsolutions.\nWorkload Assessment:  ? Measurement on a standard-\nized scale such as the NASA-TLX or the Cooper-\nHarper rating scales of the amount and type of work. \nIt assesses operator and crew task loading, which de-\ntermines the ability of a human to perform the re quired \ntasks in the desired time with the desired ac curacy.\nHuman Error and Human Reliability Assessment:  ?\nTop-down (fault tree analyses) and bottom-up (human \nfactors process failure modes and effects analysis) \nanalyses. The goal is to promote human reliability by \ncreating a system that can tolerate and recover from \nhuman errors. Such a system must also support the \nhuman role in adding reliability to the system.\n\n\n\n4.4 Design Solution Definition\n\nNASA Systems Engineering Handbook ? 69\n\nRoles of the Human Factors Specialist\nThe human factors specialist supports the systems engi-\nneering process by representing the users\u2019 and maintain-\ners\u2019 requirements and capabilities throughout the design, \nproduction, and operations stages. Human factors spe-\ncialists\u2019 roles include:\n\nIdentify applicable requirements based on Agency  ?\nstandards for human-system integration during the \nrequirements definition phase.\nSupport development of mission concepts by pro- ?\nviding information on human performance capabili-\nties and limitations.\nSupport task analysis and function allocation with in- ?\nformation on human capabilities and limitations.\nIdentify system design features that enhance usability.  ?\nThis integrates knowledge of human performance ca-\npabilities and design features.\n\nSupport trade studies by providing data on effects of  ?\nalternative designs on time to complete tasks, work-\nload, and error rates.\nSupport trade studies by providing data on effects of  ?\nalternative designs on skills and training required to \noperate the system.\nSupport design reviews to ensure compliance with  ?\nhuman-systems integration requirements.\nConduct evaluations using mockups and pro- ?\ntotypes to provide detailed data on user perfor-\nmance.\nSupport development of training and maintenance  ?\nprocedures in conjunction with hardware designers \nand mission planners.\nCollect data on human-system integration issues  ?\nduring operations to inform future designs.\n\n\n\n\n\nNASA Systems Engineering Handbook ? 71\n\nThis chapter describes the activities in the product re-\nalization processes listed in Figure 2.1-1. The chapter is \nseparated into sections corresponding to steps 5 through \n9 listed in Figure 2.1-1. The processes within each step \nare discussed in terms of the inputs, the activities, and \nthe outputs. Additional guidance is provided using ex-\namples that are relevant to NASA projects. \n\nThe product realization side of the SE engine is where \nthe rubber meets the road. In this portion of the en-\ngine, five interdependent processes result in systems that \nmeet the design specifications and stakeholder expecta-\ntions. These products are produced, acquired, reused, or \ncoded; integrated into higher level assemblies; verified \nagainst design specifications; validated against stake-\nholder expectations; and transitioned to the next level of \nthe system. As has been mentioned in previous sections, \nproducts can be models and simulations, paper studies \nor proposals, or hardware and software. The type and \nlevel of product depends on the phase of the life cycle \nand the product\u2019s specific objectives. But whatever the \nproduct, all must effectively use the processes to ensure \nthe system meets the intended operational concept.\n\nThis effort starts with the technical team taking the output \nfrom the system design processes and using the appro-\npriate crosscutting functions, such as data and configu-\n\nration management, and technical assessments to make, \nbuy, or reuse subsystems. Once these subsystems are re-\nalized, they must be integrated to the appropriate level \nas designated by the appropriate interface requirements. \nThese products are then verified through the Technical \nAssessment Process to ensure they are consistent with \nthe technical data package and that \u201cthe product was \nbuilt right.\u201d Once consistency is achieved, the technical \nteam will validate the products against the stakeholder \nexpectations that \u201cthe right product was built.\u201d Upon \nsuccessful completion of validation, the products are \ntransitioned to the next level of the system. Figure 5.0-1 \nillustrates these processes.\n\nThis is an iterative and recursive process. Early in the life \ncycle, paper products, models, and simulations are run \nthrough the five realization processes. As the system ma-\ntures and progresses through the life cycle, hardware and \nsoftware products are run through these processes. It is \nimportant to catch errors and failures at the lowest level \nof integration and early in the life cycle so that changes \ncan be made through the design processes with min-\nimum impact to the project.\n\nThe next sections describe each of the five product re-\nalization processes and their associated products for a \ngiven NASA mission. \n\n5.0 Product Realization\n\nDESIGN REALIZATION\nPRODUCT\n\nTRANSITION\nPROCESS\n\nEVALUATION PROCESSES\n\n? Acquire\n? Make/Code\n? Reuse\n\nProduct\nImplementation\n\nProduct\nIntegration\n\nProduct\nVerification\n\nProduct\nValidation\n\nProduct\nTransition\n\n? Assembly\n? Functional\n\nEvaluation\n\n? Functional\n? Environmental\n? Operational Test-\n\ning in Integration\n& Test Environment\n\n? Operational \nTesting in Mission\nEnvironment\n\n? Delivery to Next\nHigher Level in PBS\n? Delivery to \n\nOperational\nSystem\n\nFigure 5.0?1 Product realization\n\n\n\n72 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nProduct Realization Keys\n\nGenerate and manage requirements for off-the-shelf hardware/software products as for all other products.  ?\n\nUnderstand the differences between verification testing and validation testing. ?\n\nVerification Testing: ?  Verification testing relates back to the approved requirements set (such as a System Require-\nments Document (SRD)) and can be performed at different stages in the product life cycle. Verification testing in-\ncludes: (1) any testing used to assist in the development and maturation of products, product elements, or manu-\nfacturing or support processes; and/or (2) any engineering-type test used to verify status of technical progress, to \nverify that design risks are minimized, to substantiate achievement of contract technical performance, and to cer-\ntify readiness for initial validation testing. Verification tests use instrumentation and measurements, and are gener-\nally accomplished by engineers, technicians, or operator-maintainer test personnel in a controlled environment to \nfacilitate failure analysis.\n\nValidation Testing: ?  Validation relates back to the ConOps document. Validation testing is conducted under realis-\ntic conditions (or simulated conditions) on any end product for the purpose of determining the effectiveness and \nsuitability of the product for use in mission operations by typical users; and the evaluation of the results of such \ntests. Testing is the detailed quantifying method of both verification and validation. However, testing is required to \nvalidate final end products to be produced and deployed.\n\nConsider all customer, stakeholder, technical, programmatic, and safety requirements when evaluating the input nec- ?\nessary to achieve a successful product transition. \n\nAnalyze for any potential incompatibilities with interfaces as early as possible. ?\n\nCompletely understand and analyze all test data for trends and anomalies. ?\n\nUnderstand the limitations of the testing and any assumptions that are made. ?\n\nEnsure that a reused product meets the verification and validation required for the relevant system in which it is to be  ?\nused, as opposed to relying on the original verification and validation it met for the system of its original use. It would \nthen be required to meet the same verification and validation as a purchased product or a built product. The \u201cpedi-\ngree\u201d of a reused product in its original application should not be relied upon in a different system, subsystem, or ap-\nplication.\n\n\n\nNASA Systems Engineering Handbook ? 73\n\nProduct implementation is the first process encountered \nin the SE engine that begins the movement from the \nbottom of the product hierarchy up towards the Product \nTransition Process. This is where the plans, designs, anal-\nysis, requirements development, and drawings are real-\nized into actual products.\n\nProduct implementation is used to generate a speci-\nfied product of a project or activity through buying, \nmaking/coding, or reusing previously developed hard-\nware, software, models, or studies to generate a product \nappropriate for the phase of the life cycle. The product \nmust satisfy the design solution and its specified require-\nments.\n\nThe Product Implementation Process is the key activity \nthat moves the project from plans and designs into real-\nized products. Depending on the project and life-cycle \nphase within the project, the product may be hardware, \nsoftware, a model, simulations, mockups, study reports, \nor other tangible results. These products may be realized \nthrough their purchase from commercial or other ven-\ndors, generated from scratch, or through partial or com-\nplete reuse of products from other projects or activities. \nThe decision as to which of these realization strategies, \nor which combination of strategies, will be used for the \n\nproducts of this project will have been made early in the \nlife cycle using the Decision Analysis Process.\n\n5.1.1 Process Description\nFigure 5.1-1 provides a typical flow diagram for the \nProduct Implementation Process and identifies typical \ninputs, outputs, and activities to consider in addressing \nproduct implementation.\n\n5.1.1.1 Inputs\nInputs to the Product Implementation activity depend \nprimarily on the decision as to whether the end prod-\nuct will be purchased, developed from scratch, or if the \nproduct will be formed by reusing part or all of products \nfrom other projects. Typical inputs are shown in Fig-\nure 5.1-1.\n\nInputs if Purchasing the End Product:  ? If the deci-\nsion was made to purchase part or all of the products \nfor this project, the end product design specifications \nare obtained from the configuration management \nsystem as well as other applicable documents such as \nthe SEMP.\nInputs if Making/Coding the End Product:  ? For end \nproducts that will be made/coded by the technical \n\n5.1 Product Implementation\n\nEnd Product\nDocuments and\n\nManuals \n\nFrom Configuration\nManagement Process\n\nDesired End\nProduct \n\nTo Product\nVerification Process\n\nEnd Product Design\nSpeci?cations and\n\nCon?guration\nDocumentation \n\nProduct\nImplementation\u2013\n\nEnabling Products \n\nTo Technical Data\nManagement Process\n\nProduct\nImplementation\nWork Products \n\nFrom existing\nresources or Product\nTransition Process\n\nRequired Raw\nMaterials \n\nFrom existing\nresources or\n\nexternal sources \n\nMake the speci?ed end product\n\nCapture product implementation\nwork products \n\nPrepare to conduct implementation\n\nIf implemented by \nbuying:\n\nParticipate in purchase\nof speci?ed end product\n\nPrepare appropriate\nproduct support documentation \n\nIf implemented by\nreuse:\n\nParticipate in acquiring\nthe reuse end product \n\nIf implemented by making:\nEvaluate readiness of\n\nproduct implementation\u2013\nenabling products \n\nFigure 5.1?1 Product Implementation Process\n\n\n\n74 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nteam, the inputs will be the configuration controlled \ndesign specifications and raw materials as provided to \nor purchased by the project.\nInputs Needed if Reusing an End Product:  ? For end \nproducts that will reuse part or all of products gener-\nated by other projects, the inputs may be the docu-\nmentation associated with the product, as well as the \nproduct itself. Care must be taken to ensure that these \nproducts will indeed meet the specifications and en-\nvironments for this project. These would have been \nfactors involved in the Decision Analysis Process to \ndetermine the make/buy/reuse decision.\n\n5.1.1.2 Process Activities \nImplementing the product can take one of three forms:\n\nPurchase/buy, ?\nMake/code, or ?\nReuse. ?\n\nThese three forms will be discussed in the following sub-\nsections. Figure 5.1-1 shows what kind of inputs, outputs, \nand activities are performed during product implemen-\ntation regardless of where in the product hierarchy or \nlife cycle it is. These activities include preparing to con-\nduct the implementation, purchasing/making/reusing \nthe product, and capturing the product implementation \nwork product. In some cases, implementing a product \nmay have aspects of more than one of these forms (such \nas a build-to-print). In those cases, the appropriate as-\npects of the applicable forms are used.\n\nPrepare to Conduct Implementation\n\nPreparing to conduct the product implementation is a \nkey first step regardless of what form of implementation \nhas been selected. For complex projects, implementation \nstrategy and detailed planning or procedures need to be \ndeveloped and documented. For less complex projects, \nthe implementation strategy and planning will need to \nbe discussed, approved, and documented as appropriate \nfor the complexity of the project.\n\nThe documentation, specifications, and other inputs will \nalso need to be reviewed to ensure they are ready and at \nan appropriate level of detail to adequately complete the \ntype of implementation form being employed and for \nthe product life-cycle phase. For example, if the \u201cmake\u201d \nimplementation form is being employed, the design \nspecifications will need to be reviewed to ensure they are \nat a design-to level that will allow the product to be de-\n\nveloped. If the product is to be bought as a pure Com-\nmercial-Off-the-Shelf (COTS) item, the specifications \nwill need to be checked to make sure they adequately \ndescribe the vendor characteristics to narrow to a single \nmake/model of their product line.\n\nFinally, the availability and skills of personnel needed to \nconduct the implementation as well as the availability of \nany necessary raw materials, enabling products, or spe-\ncial services should also be reviewed. Any special training \nnecessary for the personnel to perform their tasks needs \nto be performed by this time.\n\nPurchase, Make, or Reuse the Product\n\nPurchase the Product\nIn the first case, the end product is to be purchased from \na commercial or other vendor. Design/purchase speci-\nfications will have been generated during requirements \ndevelopment and provided as inputs. The technical team \nwill need to review these specifications and ensure they \nare in a form adequate for the contract or purchase order. \nThis may include the generation of contracts, Statements \nof Work (SOWs), requests for proposals, purchase or-\nders, or other purchasing mechanisms. The responsi-\nbilities of the Government and contractor team should \nhave been documented in the SEMP. This will define, \nfor example, whether NASA expects the vendor to pro-\nvide a fully verified and validated product or whether the \nNASA technical team will be performing those duties. \nThe team will need to work with the acquisition team \nto ensure the accuracy of the contract SOW or purchase \norder and to ensure that adequate documentation, cer-\ntificates of compliance, or other specific needs are re-\nquested of the vendor. \n\nFor contracted purchases, as proposals come back from \nthe vendors, the technical team should work with the \ncontracting officer and participate in the review of the \ntechnical information and in the selection of the vendor \nthat best meets the design requirements for acceptable \ncost and schedule.\n\nAs the purchased products arrive, the technical team \nshould assist in the inspection of the delivered product \nand its accompanying documentation. The team should \nensure that the requested product was indeed the one \ndelivered, and that all necessary documentation, such \nas source code, operator manuals, certificates of com-\npliance, safety information, or drawings have been re-\nceived. \n\n\n\n5.1 Product Implementation\n\nNASA Systems Engineering Handbook ? 75\n\nThe technical team should also ensure that any enabling \nproducts necessary to provide test, operations, main-\ntenance, and disposal support for the product also are \nready or provided as defined in the contract. \n\nDepending on the strategy and roles/responsibilities of \nthe vendor, as documented in the SEMP, a determina-\ntion/analysis of the vendor\u2019s verification and validation \ncompliance may need to be reviewed. This may be done \ninformally or formally as appropriate for the complexity \nof the product. For products that were verified and vali-\ndated by the vendor, after ensuring that all work prod-\nucts from this phase have been captured, the product \nmay be ready to enter the Product Transition Process to \nbe delivered to the next higher level or to its final end \nuser. For products that will be verified and validated by \nthe technical team, the product will be ready to be veri-\nfied after ensuring that all work products for this phase \nhave been captured.\n\nMake/Code the Product\nIf the strategy is to make or code the product, the tech-\nnical team should first ensure that the enabling prod-\nucts are ready. This may include ensuring all piece parts \nare available, drawings are complete and adequate, soft-\nware design is complete and reviewed, machines to cut \nthe material are available, interface specifications are ap-\nproved, operators are trained and available, procedures/\nprocesses are ready, software personnel are trained and \navailable to generate code, test fixtures are developed and \nready to hold products while being generated, and soft-\nware test cases are available and ready to begin model \ngeneration.\n\nThe product is then made or coded in accordance with \nthe specified requirements, configuration documenta-\ntion, and applicable standards. Throughout this process, \nthe technical team should work with the quality organi-\nzation to review, inspect, and discuss progress and status \nwithin the team and with higher levels of management as \nappropriate. Progress should be documented within the \ntechnical schedules. Peer reviews, audits, unit testing, \ncode inspections, simulation checkout, and other tech-\nniques may be used to ensure the made or coded product \nis ready for the verification process.\n\nReuse \nIf the strategy is to reuse a product that already exists, \ncare must be taken to ensure that the product is truly ap-\nplicable to this project and for the intended uses and the \n\nenvironment in which it will be used. This should have \nbeen a factor used in the decision strategy to make/buy/\nreuse.\n\nThe documentation available from the reuse product \nshould be reviewed by the technical team to become \ncompletely familiar with the product and to ensure it \nwill meet the requirements in the intended environment. \nAny supporting manuals, drawings, or other documen-\ntation available should also be gathered.\n\nThe availability of any supporting or enabling products \nor infrastructure needed to complete the fabrication, \ncoding, testing, analysis, verification, validation, or ship-\nping of the product needs to be determined. If any of \nthese products or services are lacking, they will need to \nbe developed or arranged for before progressing to the \nnext phase.\n\nSpecial arrangements may need to be made or forms \nsuch as nondisclosure agreements may need to be ac-\nquired before the reuse product can be received.\n\nA reused product will frequently have to undergo the \nsame verification and validation as a purchased product \nor a built product. Relying on prior verification and vali-\ndation should only be considered if the product\u2019s verifi-\ncation and validation documentation meets the verifica-\ntion, validation, and documentation requirements of the \ncurrent project and the documentation demonstrates \nthat the product was verified and validated against equiv-\nalent requirements and expectations. The savings gained \nfrom reuse is not necessarily from reduced testing, but \nin a lower likelihood that the item will fail tests and gen-\nerate rework.\n\nCapture Work Products\n\nRegardless of what implementation form was selected, \nall work products from the make/buy/reuse process \nshould be captured, including design drawings, design \ndocumentation, code listings, model descriptions, pro-\ncedures used, operator manuals, maintenance manuals, \nor other documentation as appropriate.\n\n5.1.1.3 Outputs\nEnd Product for Verification: ?  Unless the vendor \nperforms verification, the made/coded, purchased, \nor reused end product, in a form appropriate for the \nlife-cycle phase, is provided for the verification pro-\ncess. The form of the end product is a function of the \n\n\n\n76 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nlife-cycle phase and the placement within the system \nstructure (the form of the end product could be hard-\nware, software, model, prototype, first article for test, \nor single operational article or multiple production \narticle).\nEnd Product Documents and Manuals: ?  Appropriate \ndocumentation is also delivered with the end product \nto the verification process and to the technical data \nmanagement process. Documentation may include \napplicable design drawings; operation, user, mainte-\nnance, or training manuals; applicable baseline docu-\nments (configuration baseline, specifications, stake-\nholder expectations); certificates of compliance; or \nother vendor documentation. \n\nThe process is complete when the following activities \nhave been accomplished:\n\nEnd product is fabricated, purchased, or reuse mod- ?\nules acquired.\nEnd products are reviewed, checked, and ready for  ?\nverification.\nProcedures, decisions, assumptions, anomalies, cor- ?\nrective actions, lessons learned, etc., resulting from \nthe make/buy/reuse are recorded.\n\n5.1.2 Product Implementation Guidance\n\n5.1.2.1 Buying Off-the-Shelf Products\nOff-the-Shelf (OTS) products are hardware/software \nthat has an existing heritage and usually originates from \none of several sources, which include commercial, mili-\ntary, and NASA programs. Special care needs to be taken \nwhen purchasing OTS products for use in the space en-\nvironment. Most OTS products were developed for use \nin the more benign environments of Earth and may not \nbe suitable to endure the harsh space environments, in-\ncluding vacuum, radiation, extreme temperature ranges, \nextreme lighting conditions, zero gravity, atomic oxygen, \nlack of convection cooling, launch vibration or accelera-\ntion, and shock loads.\n\nWhen purchasing OTS products, requirements should \nstill be generated and managed. A survey of available \nOTS is made and evaluated as to the extent they satisfy \nthe requirements. Products that meet all the require-\nments are a good candidate for selection. If no product \ncan be found to meet all the requirements, a trade study \nneeds to be performed to determine whether the require-\nments can be relaxed or waived, the OTS can be modi-\n\nfied to bring it into compliance, or whether another op-\ntion to build or reuse should be selected.\n\nSeveral additional factors should be considered when se-\nlecting the OTS option:\n\nHeritage of the product; ?\nCritical or noncritical application; ?\nAmount of modification required and who performs  ?\nit;\nWhether sufficient documentation is available; ?\nProprietary, usage, ownership, warranty, and licensing  ?\nrights;\nFuture support for the product from the vendor/pro- ?\nvider;\nAny additional validation of the product needed by  ?\nthe project; and\nAgreement on disclosure of defects discovered by the  ?\ncommunity of users of the product.\n\n5.1.2.2 Heritage\n\u201cHeritage\u201d refers to the original manufacturer\u2019s level of \nquality and reliability that is built into parts and which \nhas been proven by (1) time in service, (2) number of \nunits in service, (3) mean time between failure perfor-\nmance, and (4) number of use cycles. High-heritage \nproducts are from the original supplier, who has main-\ntained the great majority of the original service, design, \nperformance, and manufacturing characteristics. Low-\nheritage products are those that (1) were not built by \nthe original manufacturer; (2) do not have a significant \nhistory of test and usage; or (3) have had significant as-\npects of the original service, design, performance, or \nmanufacturing characteristics altered. An important \nfactor in assessing the heritage of a COTS product is \nto ensure that the use/application of the product is rel-\nevant to the application for which it is now intended. A \nproduct that has high heritage in a ground-based appli-\ncation could have a low heritage when placed in a space \nenvironment.\n\nThe focus of a \u201cheritage review\u201d is to confirm the appli-\ncability of the component for the current application. \nAssessments must be made regarding not only technical \ninterfaces (hardware and software) and performance, \nbut also the environments to which the unit has been \npreviously qualified, including electromagnetic compat-\nibility, radiation, and contamination. The compatibility \nof the design with parts quality requirements must also \n\n\n\n5.1 Product Implementation\n\nNASA Systems Engineering Handbook ? 77\n\nbe assessed. All noncompliances must be identified, doc-\numented, and addressed either by modification to bring \nthe component into compliance or formal waivers/de-\nviations for accepted deficiencies. This heritage review is \ncommonly held closely after contract award.\n\nWhen reviewing a product\u2019s applicability, it is impor-\ntant to consider the nature of the application. A \u201ccata-\nstrophic\u201d application is one where a failure could cause \nloss of life or vehicle. A \u201ccritical\u201d application is one where \nfailure could cause loss of mission. For use in these appli-\ncations, several additional precautions should be taken, \nincluding ensuring the product will not be used near the \nboundaries of its performance or environmental enve-\nlopes. Extra scrutiny by experts should be applied during \nPreliminary Design Reviews (PDRs) and Critical Design \nReviews (CDRs) to ensure the appropriateness of its \nuse.\n\nModification of an OTS product may be required for it \nto be suitable for a NASA application. This affects the \nproduct\u2019s heritage, and therefore, the modified product \nshould be treated as a new design. If the product is mod-\nified by NASA and not the manufacturer, it would be \nbeneficial for the supplier to have some involvement in \nreviewing the modification. NASA modification may \nalso require the purchase of additional documentation \nfrom the supplier such as drawings, code, or other de-\nsign and test descriptions.\n\nFor additional information and suggested test and anal-\nysis requirements for OTS products, see JSC EA-WI-016 \nor MSFC MWI 8060.1 both titled Off the Shelf Hardware \nUtilization in Flight Hardware Development and G-118-\n2006e AIAA Guide for Managing the Use of Commercial \nOff the Shelf (COTS) Software Components for Mission-\nCritical Systems.\n\n\n\n78 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nProduct Integration is one of the SE engine product re-\nalization processes that make up the system structure. \nIn this process, lower level products are assembled into \nhigher level products and checked to make sure that the \nintegrated product functions properly. It is an element \nof the processes that lead realized products from a level \nbelow to realized end products at a level above, between \nthe Product Implementation, Verification, and Valida-\ntion Processes.\n\nThe purpose of the Product Integration Process is to \nsystematically assemble the higher level product from \nthe lower level products or subsystems (e.g., product \nelements, units, components, subsystems, or operator \ntasks); ensure that the product, as integrated, functions \nproperly; and deliver the product. Product integration \nis required at each level of the system hierarchy. The \nactivities associated with product integrations occur \nthroughout the entire product life cycle. This includes \nall of the incremental steps, including level-appropriate \ntesting, necessary to complete assembly of a product \nand to enable the top-level \nproduct tests to be con-\nducted. The Product Inte-\ngration Process may include \nand often begins with anal-\nysis and simulations (e.g., \nvarious types of prototypes) \nand progresses through in-\ncreasingly more realistic \nincremental functionality \nuntil the final product is \nachieved. In each succes-\nsive build, prototypes are \nconstructed, evaluated, im-\nproved, and reconstructed \nbased upon knowledge \ngained in the evaluation \nprocess. The degree of vir-\ntual versus physical proto-\ntyping required depends \non the functionality of the \ndesign tools and the com-\nplexity of the product and \nits associated risk. There is \na high probability that the \nproduct, integrated in this \n\nmanner, will pass product verification and validation. \nFor some products, the last integration phase will occur \nwhen the product is deployed at its intended operational \nsite. If any problems of incompatibility are discovered \nduring the product verification and validation testing \nphase, they are resolved one at a time.\n\nThe Product Integration Process applies not only to hard-\nware and software systems but also to service-oriented so-\nlutions, requirements, specifications, plans, and concepts. \nThe ultimate purpose of product integration is to ensure \nthat the system elements will function as a whole. \n\n5.2.1 Process Description\nFigure 5.2-1 provides a typical flow diagram for the \nProduct Integration Process and identifies typical in-\nputs, outputs, and activities to consider in addressing \nproduct integration. The activities of the Product Inte-\ngration Process are truncated to indicate the action and \nobject of the action.\n\n5.2 Product Integration\n\nProduct Documents\nand Manuals \n\nTo Product\nVeri?cation Process\n\nLower Level\nProducts to Be\n\nIntegrated  \n\nEnd Product Design\nSpeci?cations and\n\nCon?guration\nDocumentation  \n\nProduct Integration\u2013\nEnabling Products\n\nTo Technical Data\nManagement Process\n\nFrom Product\nTransition Process\n\nFrom Con?guration\nManagement Process\n\nFrom existing\nresources or Product\nTransition Process\n\nDesired Product\n\nProduct Integration\nWork Products  \n\nCon?rm that received products\nhave been validated \n\nPrepare the integration environment\nfor assembly and integration\n\nAssemble and integrate the\nreceived products into the desired\n\nend product\n\nObtain lower level products for\nassembly and integration \n\nCapture product integration work\nproducts \n\nPrepare to conduct product\nintegration\n\nPrepare appropriate product\nsupport documentation \n\nFigure 5.2?1 Product Integration Process\n\n\n\n5.2 Product Integration\n\nNASA Systems Engineering Handbook ? 79\n\n5.2.1.1 Inputs \nProduct Integration encompasses more than a one-time \nassembly of the lower level products and operator tasks \nat the end of the design and fabrication phase of the life \ncycle. An integration plan must be developed and docu-\nmented. An example outline for an integration plan is \nprovided in Appendix H. Product Integration is con-\nducted incrementally, using a recursive process of assem-\nbling lower level products and operator tasks; evaluating \nthem through test, inspection, analysis, or demonstra-\ntion; and then assembling more lower level products and \noperator tasks. Planning for Product Integration should \nbe initiated during the concept formulation phase of the \nlife cycle. The basic tasks that need to be established in-\nvolve the management of internal and external interfaces \nof the various levels of products and operator tasks to \nsupport product integration and are as follows:\n\nDefine interfaces; ?\nIdentify the characteristics of the interfaces (physical,  ?\nelectrical, mechanical, etc.);\nEnsure interface compatibility at all defined interfaces  ?\nby using a process documented and approved by the \nproject;\nEnsure interface compatibility at all defined interfaces; ?\nStrictly control all of the interface processes during  ?\ndesign, construction, operation, etc.;\nIdentify lower level products to be assembled and in- ?\ntegrated (from the Product Transition Process);\nIdentify assembly drawings or other documentation  ?\nthat show the complete configuration of the product \nbeing integrated, a parts list, and any assembly in-\nstructions (e.g., torque requirements for fasteners);\nIdentify end-product, design-definition-specified re- ?\nquirements (specifications), and configuration docu-\nmentation for the applicable work breakdown struc-\nture model, including interface specifications, in the \nform appropriate to satisfy the product-line life-cycle \nphase success criteria (from the Configuration Man-\nagement Process); and\nIdentify Product Integration\u2013enabling products (from  ?\nexisting resources or the Product Transition Process \nfor enabling product realization).\n\n5.2.1.2 Process Activities\nThis subsection addresses the approach to the top-level \nimplementation of the Product Integration Process, in-\ncluding the activities required to support the process, \n\nThe project would follow this approach throughout its \nlife cycle.\n\nThe following are typical activities that support the Prod-\nuct Integration Process:\n\nPrepare to conduct Product Integration by (1) preparing  ?\na product integration strategy, detailed planning for the \nintegration, and integration sequences and procedures \nand (2) determining whether the product configura-\ntion documentation is adequate to conduct the type of \nproduct integration applicable for the product-line life-\ncycle phase, location of the product in the system struc-\nture, and management phase success criteria.\nObtain lower level products required to assemble and  ?\nintegrate into the desired product.\nConfirm that the received products that are to be as- ?\nsembled and integrated have been validated to dem-\nonstrate that the individual products satisfy the \nagreed-to set of stakeholder expectations, including \ninterface requirements.\nPrepare the integration environment in which as- ?\nsembly and integration will take place, including eval-\nuating the readiness of the product integration\u2013en-\nabling products and the assigned workforce. \nAssemble and integrate the received products into the  ?\ndesired end product in accordance with the specified \nrequirements, configuration documentation, inter-\nface requirements, applicable standards, and integra-\ntion sequencing and procedures. \nConduct functional testing to ensure that assembly is  ?\nready to enter verification testing and ready to be in-\ntegrated into the next level.\nPrepare appropriate product support documentation  ?\nsuch as special procedures for performing product \nverification and product validation.\nCapture work products and related information gen- ?\nerated while performing the product integration pro-\ncess activities.\n\n5.2.1.3 Outputs\nThe following are typical outputs from this process and \ndestinations for the products from this process:\n\nIntegrated product(s) in the form appropriate to the  ?\nproduct-line life-cycle phase and to satisfy phase suc-\ncess criteria (to the Product Verification Process).\nDocumentation and manuals in a form appropriate  ?\nfor satisfying the life-cycle phase success criteria, in-\n\n\n\n80 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\ncluding as-integrated product descriptions and op-\nerate-to and maintenance manuals (to the Technical \nData Management Process).\nWork products, including reports, records, and non- ?\ndeliverable outcomes of product integration activi-\nties (to support the Technical Data Management Pro-\ncess); integration strategy document; assembly/check \narea drawings; system/component documentation se-\nquences and rationale for selected assemblies; interface \nmanagement documentation; personnel requirements; \nspecial handling requirements; system documenta-\ntion; shipping schedules; test equipment and drivers\u2019 \nrequirements; emulator requirements; and identifica-\ntion of limitations for both hardware and software.\n\n5.2.2 Product Integration Guidance\n\n5.2.2.1 Integration Strategy\nAn integration strategy is developed, as well as supporting \ndocumentation, to identify optimal sequence of receipt, \nassembly, and activation of the various components that \nmake up the system. This strategy should use business as \nwell as technical factors to ensure an assembly, activation, \nand loading sequence that minimizes cost and assembly \ndifficulties. The larger or more complex the system or the \nmore delicate the element, the more critical the proper \nsequence becomes, as small changes can cause large im-\npacts on project results.\n\nThe optimal sequence of assembly is built from the \nbottom up as components become subelements, ele-\nments, and subsystems, each of which must be checked \nprior to fitting into the next higher assembly. The se-\nquence will encompass any effort needed to establish \nand equip the assembly facilities (e.g., raised floor, hoists, \njigs, test equipment, input/output, and power connec-\ntions). Once established, the sequence must be period-\nically reviewed to ensure that variations in production \nand delivery schedules have not had an adverse impact \non the sequence or compromised the factors on which \nearlier decisions were made.\n\n5.2.2.2 Relationship to Product \nImplementation \n\nAs previously described, Product Implementation is \nwhere the plans, designs, analysis, requirements devel-\nopment, and drawings are realized into actual products. \nProduct Integration concentrates on the control of the \ninterfaces and the verification and validation to achieve \n\nthe correct product to meet the requirements. Product \nIntegration can be thought of as released or phased de-\nliveries. Product Integration is the process that pulls to-\ngether new and existing products and ensures that they \nall combine properly into a complete product without \ninterference or complications. If there are issues, the \nProduct Integration Process documents the exceptions, \nwhich can then be evaluated to determine if the product \nis ready for implementation/operations.\n\nIntegration occurs at every stage of a project\u2019s life cycle. \nIn the Formulation phase, the decomposed requirements \nneed to be integrated into a complete system to verify that \nnothing is missing or duplicated. In the Implementation \nphase, the design and hardware need to be integrated into \nan overall system to verify that they meet the require-\nments and that there are no duplications or omissions. \n\nThe emphasis on the recursive, iterative, and integrated \nnature of systems engineering highlights how the product \nintegration activities are not only integrated across all of \nthe phases of the entire life cycle in the initial planning \nstages of the project, but also used recursively across all \nof the life-cycle phases as the project product proceeds \nthrough the flow down and flow up conveyed by the SE \nengine. This ensures that when changes occur to require-\nments, design concepts, etc.\u2014usually in response to up-\ndates from stakeholders and results from analysis, mod-\neling, or testing\u2014that adequate course corrections are \nmade to the project. This is accomplished through re-\nevaluation by driving through the SE engine, enabling all \naspects of the product integration activities to be appro-\npriately updated. The result is a product that meets all of \nthe new modifications approved by the project and elim-\ninates the opportunities for costly and time-consuming \nmodifications in the later stages of the project.\n\n5.2.2.3 Product/Interface Integration Support\nThere are several processes that support the integration of \nproducts and interfaces. Each process allows either the in-\ntegration of products and interfaces or the validation that \nthe integrated products meet the needs of the project.\n\nThe following is a list of typical example processes and \nproducts that support the integration of products and \ninterfaces and that should be addressed by the project \nin the overall approach to Product Integration: require-\nments documents; requirements reviews; design re-\nviews; design drawings and specifications; integration \nand test plans; hardware configuration control docu-\n\n\n\n5.2 Product Integration\n\nNASA Systems Engineering Handbook ? 81\n\nmentation; quality assurance records; interface control \nrequirements/documents; ConOps documents; verifica-\ntion requirement documents; verification reports/anal-\nysis; NASA, military, and industry standards; best prac-\ntices; and lessons learned.\n\n5.2.2.4 Product Integration of the Design \nSolution\n\nThis subsection addresses the more specific implementa-\ntion of Product Integration related to the selected design \nsolution.\n\nGenerally, system/product designs are an aggregation of \nsubsystems and components. This is relatively obvious \nfor complex hardware and/or software systems. The same \nholds true for many service-oriented solutions. For ex-\nample, a solution to provide a single person access to the \nInternet involves hardware, software, and a communica-\ntions interface. The purpose of Product Integration is to \nensure that combination of these elements achieves the \nrequired result (i.e., works as expected). Consequently, \ninternal and external interfaces must be considered in \nthe design and evaluated prior to production. \n\nThere are a variety of different testing requirements to \nverify product integration at all levels. Qualification \ntesting and acceptance testing are examples of two of \nthese test types that are performed as the product is in-\ntegrated. Another type of testing that is important to the \ndesign and ultimate product integration is a planned test \nprocess in which development items are tested under ac-\ntual or simulated mission profile environments to dis-\nclose design deficiencies and to provide engineering \ninformation on failure modes and mechanisms. If ac-\ncomplished with development items, this provides early \ninsight into any issues that may otherwise only be ob-\nserved at the late stages of product integration where \nit becomes costly to incorporate corrective actions. For \nlarge, complex system/products, integration/verification \nefforts are accomplished using a prototype.\n\n5.2.2.5 Interface Management\nThe objective of the interface management is to achieve \nfunctional and physical compatibility among all inter-\nrelated system elements. Interface management is de-\nfined in more detail in Section 6.3. An interface is any \nboundary between one area and another. It may be cog-\nnitive, external, internal, functional, or physical. Inter-\nfaces occur within the system (internal) as well as be-\n\ntween the system and another system (external) and may \nbe functional or physical (e.g., mechanical, electrical) in \nnature. Interface requirements are documented in an In-\nterface Requirements Document (IRD). Care should be \ntaken to define interface requirements and to avoid spec-\nifying design solutions when creating the IRD. In its final \nform, the Interface Control Document (ICD) describes \nthe detailed implementation of the requirements con-\ntained in the IRD. An interface control plan describes \nthe management process for IRDs and ICDs. This plan \nprovides the means to identify and resolve interface in-\ncompatibilities and to determine the impact of interface \ndesign changes.\n\n5.2.2.6 Compatibility Analysis\nDuring the program\u2019s life, compatibility and accessi-\nbility must be maintained for the many diverse elements. \nCompatibility analysis of the interface definition dem-\nonstrates completeness of the interface and traceability \nrecords. As changes are made, an authoritative means \nof controlling the design of interfaces must be managed \nwith appropriate documentation, thereby avoiding the \nsituation in which hardware or software, when integrated \ninto the system, fails to function as part of the system as \nintended. Ensuring that all system pieces work together \nis a complex task that involves teams, stakeholders, con-\ntractors, and program management from the end of the \ninitial concept definition stage through the operations \nand support stage. Physical integra tion is accomplished \nduring Phase D. At the finer levels of resolution, pieces \nmust be tested, assembled and/or integrated, and tested \nagain. The systems engineer role includes performance \nof the delegated management duties such as configura-\ntion control and overseeing the integration, verification, \nand validation processes.\n\n5.2.2.7 Interface Management Tasks\nThe interface management tasks begin early in the devel-\nopment effort, when interface requirements can be influ-\nenced by all engineering disciplines and applicable inter-\nface standards can be invoked. They continue through \ndesign and checkout. During design, emphasis is on en-\nsuring that interface specifications are documented and \ncommunicated. During system element checkout, both \nprior to assembly and in the assembled configuration, \nemphasis is on verifying the implemented interfaces. \nThroughout the product integration process activities, \ninterface baselines are controlled to ensure that changes \n\n\n\n82 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nin the design of system elements have minimal impact \non other elements with which they interface. During \ntesting or other validation and verification activities, \nmultiple system elements are checked out as integrated \nsubsystems or systems. The following provides more de-\ntails on these tasks.\n\nDefine Interfaces\nThe bulk of integration problems arise from unknown or \nuncontrolled aspects of interfaces. Therefore, system and \nsubsystem interfaces are specified as early as possible in \nthe development effort. Interface specifications address \nlogical, physical, electrical, mechanical, human, and en-\nvironmental parameters as appropriate. Intra-system in-\nterfaces are the first design consideration for developers \nof the system\u2019s subsystems. Interfaces are used from pre-\nvious development efforts or are developed in accor-\ndance with interface standards for the given discipline \nor technology. Novel interfaces are constructed only for \ncompelling reasons. Interface specifications are verified \nagainst interface requirements. Typical products include \ninterface descriptions, ICDs, interface requirements, and \nspecifications.\n\nVerify Interfaces\nIn verifying the interfaces, the systems engineer must en-\nsure that the interfaces of each element of the system or \nsubsystem are controlled and known to the developers. \nAdditionally, when changes to the interfaces are needed, \nthe changes must at least be evaluated for possible im-\npact on other interfacing elements and then communi-\ncated to the affected developers. Although all affected \ndevelopers are part of the group that makes changes, \nsuch changes need to be captured in a readily accessible \nplace so that the current state of the interfaces can be \nknown to all. Typical products include ICDs and excep-\ntion reports.\n\nThe use of emulators for verifying hardware and soft-\nware interfaces is acceptable where the limitations of the \nemulator are well characterized and meet the operating \nenvironment characteristics and behavior requirements \nfor interface verification. The integration plan should \nspecifically document the scope of use for emulators. \n\nInspect and Acknowledge System and Subsystem \nElement Receipt\nAcknowledging receipt and inspecting the condition of \neach system or subsystem element is required prior to \n\nassembling the system in accordance with the intended \ndesign. The elements are checked for quantity, obvious \ndamage, and consistency between the element descrip-\ntion and a list of element requirements. Typical products \ninclude acceptance documents, delivery receipts, and \nchecked packing list.\n\nVerify System and Subsystem Elements\n\nSystem and subsystem element verification confirms \nthat the implemented design features of developed or \npurchased system elements meet their requirements. \nThis is intended to ensure that each element of the \nsystem or subsystem functions in its intended environ-\nment, including those elements that are OTS for other \nenvironments. Such verifications may be by test (e.g., \nregression testing as a tool or subsystem/elements are \ncombined), inspection, analysis (deficiency or compli-\nance reports), or demonstration and may be executed \neither by the organization that will assemble the system \nor subsystem or by the producing organization. A \nmethod of discerning the elements that \u201cpassed\u201d verifi-\ncation from those elements that \u201cfailed\u201d needs to be in \nplace. Typical products include verified system features \nand exception reports.\n\nVerify Element Interfaces\n\nVerification of the system element interfaces ensures \nthat the elements comply with the interface specification \nprior to assembly in the system. The intent is to ensure \nthat the interface of each element of the system or sub-\nsystem is verified against its corresponding interface \nspecification. Such verification may be by test, inspec-\ntion, analysis, or demonstration and may be executed \nby the organization that will assemble the system or \nsubsystem or by another organization. Typical prod-\nucts include verified system element interfaces, test re-\nports, and exception reports.\n\nIntegrate and Verify\n\nAssembly of the elements of the system should be per-\nformed in accordance with the established integration \nstrategy. This ensures that the assembly of the system el-\nements into larger or more complex assemblies is con-\nducted in accordance with the planned strategy. To \nensure that the integration has been completed, a verifi-\ncation of the integrated system interfaces should be per-\nformed. Typical products include integration reports, \nexception reports, and an integrated system.\n\n\n\nNASA Systems Engineering Handbook ? 83\n\nThe Product Verification Process is the first of the verifi-\ncation and validation processes conducted on a realized \nend product. As used in the context of the systems engi-\nneering common technical processes, a realized product \nis one provided by either the Product Implementation \nProcess or the Product Integration Process in a form \nsuitable for meeting applicable life-cycle phase success \ncriteria. Realization is the act of verifying, validating, and \ntransitioning the realized product for use at the next level \nup of the system structure or to the customer. Simply \nput, the Product Verification Process answers the crit-\nical question, Was the end product realized right? The \nProduct Validation Process addresses the equally critical \nquestion, Was the right end product realized?\n\nVerification proves that a realized product for any system \nmodel within the system structure conforms to the build-\nto requirements (for software elements) or realize-to spec-\nifications and design descriptive documents (for hardware \nelements, manual procedures, or composite products of \nhardware, software, and manual procedures). \n\nDistinctions Between Product Verification and \nProduct Validation\n\nFrom a process perspective, product verification and val-\nidation may be similar in nature, but the objectives are \nfundamentally different.\n\nIt is essential to confirm that the realized product is in \nconformance with its specifications and design descrip-\ntion documentation (i.e., verification). Such specifica-\ntions and documents will establish the configuration \nbaseline of that product, which may have to be modified \nat a later time. Without a verified baseline and appro-\npriate configuration controls, such later modifications \ncould be costly or cause major performance problems. \nHowever, from a customer point of view, the interest is in \nwhether the end product provided will do what the cus-\ntomer intended within the environment of use (i.e., vali-\ndation). When cost effective and warranted by analysis, \nthe expense of validation testing alone can be mitigated \nby combining tests to perform verification and valida-\ntion simultaneously.\n\nThe outcome of the Product Verification Process is \nconfirmation that the \u201cas-realized product,\u201d whether \nachieved by implementation or integration, conforms \n\nto its specified requirements, i.e., verification of the end \nproduct. This subsection discusses the process activities, \ninputs, outcomes, and potential deficiencies.\n\n5.3.1 Process Description\nFigure 5.3-1 provides a typical flow diagram for the \nProduct Verification Process and identifies typical in-\nputs, outputs, and activities to consider in addressing \nproduct verification.\n\n5.3.1.1 Inputs\nKey inputs to the process are the product to be verified, \nverification plan, specified requirements baseline, and \nany enabling products needed to perform the Product \nVerification Process (including the ConOps, mission \nneeds and goals, requirements and specifications, in-\n\n5.3 Product Verification \n\nDifferences Between Verification and \nValidation Testing\n\nVerification Testing\nVerification testing relates back to the approved re-\nquirements set (such as an SRD) and can be per-\nformed at different stages in the product life cycle. \nVerification testing includes: (1) any testing used to \nassist in the development and maturation of prod-\nucts, product elements, or manufacturing or support \nprocesses; and/or (2) any engineering-type test used \nto verify the status of technical progress, verify that \ndesign risks are minimized, substantiate achievement \nof contract technical performance, and certify readi-\nness for initial validation testing. Verification tests use \ninstrumentation and measurements and are gener-\nally accomplished by engineers, technicians, or op-\nerator-maintainer test personnel in a controlled envi-\nronment to facilitate failure analysis. \n\nValidation Testing\nValidation relates back to the ConOps document. Vali-\ndation testing is conducted under realistic conditions \n(or simulated conditions) on any end product to de-\ntermine the effectiveness and suitability of the prod-\nuct for use in mission operations by typical users and \nto evaluate the results of such tests. Testing is the de-\ntailed quantifying method of both verification and \nvalidation. However, testing is required to validate fi-\nnal end products to be produced and deployed. \n\n\n\n84 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nterface control drawings, testing standards and policies, \nand Agency standards and policies).\n\n5.3.1.2 Process Activities\nThere are five major steps in the Product Verification \nProcess: (1) verification planning (prepare to implement \nthe verification plan); (2) verification preparation (pre-\npare for conducting verification); (3) conduct verifica-\ntion (perform verification); (4) analyze verification re-\nsults; and (5) capture the verification work products. \n\nThe objective of the Product Verification Process is to \ngenerate evidence necessary to confirm that end prod-\nucts, from the lowest level of the system structure to the \nhighest, conform to the specified requirements (specifi-\ncations and descriptive documents) to which they were \nrealized whether by the Product Implementation Pro-\ncess or by the Product Integration Process.\n\nProduct Verification is usually performed by the devel-\noper that produced (or \u201crealized\u201d) the end product, with \nparticipation of the end user and customer. Product \nVerification confirms that the as-realized product, \nwhether it was achieved by Product Implementation or \nProduct Integration, conforms to its specified require-\n\nments (specifications and descriptive documentation) \nused for making or assembling and integrating the end \nproduct. Developers of the system, as well as the users, \nare typically involved in verification testing. The cus-\ntomer and Quality Assurance (QA) personnel are also \ncritical in the verification planning and execution ac-\ntivities.\n\nProduct Verification Planning \n\nPlanning to conduct the product verification is a key first \nstep. From relevant specifications and product form, the \ntype of verification (e.g., analysis, demonstration, inspec-\ntion, or test) should be established based on the life-cycle \nphase, cost, schedule, resources, and the position of the \nend product within the system structure. The verifica-\ntion plan should be reviewed (an output of the Technical \nPlanning Process, based on design solution outputs) for \nany specific procedures, constraints, success criteria, or \nother verification requirements. (See Appendix I for a \nsample verification plan outline.)\n\nVerification Plan and Methods \n\nThe task of preparing the verification plan includes es-\ntablishing the type of verification to be performed, de-\n\nFigure 5.3?1 Product Verification Process\n\nProduct Verification\nResults \n\nProduct Verification\nReport  \n\nFrom Product\nImplementation or\n\nProduct Integration Process\n\nEnd Product\nto Be Verified \n\nProduct Verification\nPlan \n\nSpecified Requirements\nBaseline  \n\nProduct Verification\u2013\nEnabling Products\n\nTo Technical\nAssessment Process\n\nFrom Design Solution \nDe?nition and Technical\n\nPlanning Processes\n\nVerified End Product \n\nTo Product\nValidation Process\n\nTo Technical Data\nManagement Process\n\nFrom Con?guration\nManagement Process\n\nFrom existing\nresources or Product\nTransition Process Product Verification\n\nWork Products  \n\nPerform the product\nverification\n\nAnalyze the outcomes of\nthe product verification\n\nCapture the work products\nfrom product verification \n\nPrepare a product\nverification report \n\nPrepare to conduct product\nverification\n\n\n\n5.3 Product Verification\n\nNASA Systems Engineering Handbook ? 85\n\npendent on the life-cycle phase; position of the product \nin the system structure; the form of the product used; \nand related costs of verification of individual specified \nrequirements. The types of verification include analyses, \ninspection, demonstration, and test or some combina-\ntion of these four. The verification plan, typically written \nat a detailed technical level, plays a pivotal role in bottom-\nup product realization. \n\nFlight units (end product that is flown, including proto- ?\nflight units).\n\nAny of these types of product forms may be in any of \nthese states:\n\nProduced (built, fabricated, manufactured, or coded); ?\nReused (modified internal nondevelopmental prod- ?\nucts or OTS product); and \nAssembled and integrated (a composite of lower level  ?\nproducts).\n\nThe conditions and environment under which the \nproduct is to be verified should be established and the \nverification planned based on the associated entrance/\nsuccess criteria identified. The Decision Analysis Process \nshould be used to help finalize the planning details. \n\nProcedures should be prepared to conduct verification \nbased on the type (e.g., analysis, inspection, demonstra-\ntion, or test) planned. These procedures are typically de-\nveloped during the design phase of the project life cycle \nand matured as the design is matured. Operational use \n\nTypes of Testing\n\nThere are many different types of testing that can be used in verification of an end product. These examples are pro-\nvided for consideration:\n\nAerodynamic ? Acceptance ? Acoustic  ?\n\nBurn-in ? Characterization ? Component  ?\n\nDrop ? Electromagnetic Compatibility ? Electromagnetic Interference ?\n\nEnvironmental ? G-loading  ? Go or No-Go  ?\n\nHigh-/Low-Voltage Limits  ? Human Factors Engineering/  ?\nHuman-in-the-Loop Testing\n\nIntegration  ?\n\nLeak Rates ? Lifetime/Cycling ? Manufacturing/Random Defects ?\n\nNominal ? Off-Nominal  ? Operational ?\n\nParametric ? Performance ? Pressure Cycling  ?\n\nPressure Limits  ? Qualification Flow ? Structural Functional ?\n\nSecurity Checks ? System ? Thermal Cycling ?\n\nThermal Limits ? Thermal Vacuum ? Vibration ?\n\nNote: The final, official verification of the end prod-\nuct should be for a controlled unit. Typically, attempt-\ning to \u201cbuy off\u201d a \u201cshall\u201d on a prototype is not accept-\nable; it is usually completed on a qualification, flight, \nor other more final, controlled unit. \n\nNote: Close alignment of the verification plan with \nthe project\u2019s SEMP is absolutely essential.\n\nVerification can be performed recursively throughout \nthe project life cycle and on a wide variety of product \nforms. For example:\n\nSimulated (algorithmic models, virtual reality simu- ?\nlator);\nMockup (plywood, brass board, breadboard); ?\nConcept description (paper report);  ?\nPrototype (product with partial functionality); ?\nEngineering unit (fully functional but may not be  ?\nsame form/fit);\nDesign verification test units (form, fit, and function  ?\nis the same, but they may not have flight parts);\nQualification units (identical to flight units but may  ?\nbe subjected to extreme environments); and\n\n\n\n86 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nscenarios are thought through so as to explore all pos-\nsible verification activities to be performed. \n\nOutcomes of verification planning include the following:\nThe verification type that is appropriate for showing  ?\nor proving the realized product conforms to its speci-\nfied requirements is selected.\nThe product verification procedures are clearly de- ?\nfined based on: (1) the procedures for each type of \nverification selected, (2) the purpose and objective of \neach procedure, (3) any pre-verification and post-ver-\n\nification actions, and (4) the criteria for determining \nthe success or failure of the procedure.\nThe verification environment (e.g., facilities, equip- ?\nment, tools, simulations, measuring devices, per-\nsonnel, and climatic conditions) in which the verifi-\ncation procedures will be implemented is defined. \nAs appropriate, project risk items are updated based  ?\non approved verification strategies that cannot du-\nplicate fully integrated test systems, configurations, \nand/or target operating environments. Rationales, \ntrade space, optimization results, and implications of \nthe approaches are documented in the new or revised \nrisk statements as well as references to accommodate \nfuture design, test, and operational changes to the \nproject baseline.\n\nProduct Verification Preparation\nIn preparation for verification, the specified require-\nments (outputs of the Design Solution Process) are col-\nlected and confirmed. The product to be verified is ob-\ntained (output from implementation or integration), as \nare any enabling products and support resources that \nare necessary for verification (requirements identified \nand acquisition initiated by design solution definition \n\nTypes of Verification\n\nAnalysis:  ? The use of mathematical modeling and analytical techniques to predict the suitability of a design to stake-\nholder expectations based on calculated data or data derived from lower system structure end product verifications. \nAnalysis is generally used when a prototype; engineering model; or fabricated, assembled, and integrated product is \nnot available. Analysis includes the use of modeling and simulation as analytical tools. A model is a mathematical rep-\nresentation of reality. A simulation is the manipulation of a model.\n\nDemonstration: ?  Showing that the use of an end product achieves the individual specified requirement. It is gener-\nally a basic confirmation of performance capability, differentiated from testing by the lack of detailed data gathering. \nDemonstrations can involve the use of physical models or mockups; for example, a requirement that all controls shall \nbe reachable by the pilot could be verified by having a pilot perform flight-related tasks in a cockpit mockup or sim-\nulator. A demonstration could also be the actual operation of the end product by highly qualified personnel, such as \ntest pilots, who perform a one-time event that demonstrates a capability to operate at extreme limits of system per-\nformance, an operation not normally expected from a representative operational pilot. \n\nInspection: ?  The visual examination of a realized end product. Inspection is generally used to verify physical design \nfeatures or specific manufacturer identification. For example, if there is a requirement that the safety arming pin has a \nred flag with the words \u201cRemove Before Flight\u201d stenciled on the flag in black letters, a visual inspection of the arming \npin flag can be used to determine if this requirement was met. \n\nTest: ?  The use of an end product to obtain detailed data needed to verify performance, or provide sufficient informa-\ntion to verify performance through further analysis. Testing can be conducted on final end products, breadboards, \nbrass boards or prototypes. Testing produces data at discrete points for each specified requirement under controlled \nconditions and is the most resource-intensive verification technique. As the saying goes, \u201cTest as you fly, and fly as you \ntest.\u201d (See Subsection 5.3.2.5.)\n\nNote: Verification planning is begun early in the proj-\nect life cycle during the requirements development \nphase. (See Section 4.2.) Which verification approach \nto use should be included as part of the requirements \ndevelopment to plan for the future activities, estab-\nlish special requirements derived from verification-\nenabling products identified, and to ensure that the \ntechnical statement is a verifiable requirement. Up-\ndates to verification planning continue throughout \nlogical decomposition and design development, es-\npecially as design reviews and simulations shed light \non items under consideration. (See Section 6.1.)\n\n\n\n5.3 Product Verification\n\nNASA Systems Engineering Handbook ? 87\n\nactivities). The final element of verification preparation \nincludes the preparation of the verification environment \n(e.g., facilities, equipment, tools, simulations, measuring \ndevices, personnel, and climatic conditions). Identifica-\ntion of the environmental requirements is necessary and \nthe implications of those requirements must be carefully \nconsidered.\n\nOutcomes of verification preparation include the follow-\ning:\n\nThe preparations for performing the verification as  ?\nplanned are completed;\nAn appropriate set of specified requirements and sup- ?\nporting configuration documentation is available and \non hand;\nArticles/models to be used for verification are on  ?\nhand, assembled, and integrated with the verifica-\ntion environment according to verification plans and \nschedules;\nThe resources needed to conduct the verification  ?\nare available according to the verification plans and \nschedules; and\nThe verification environment is evaluated for ade- ?\nquacy, completeness, readiness, and integration.\n\nConduct Planned Product Verification \nThe actual act of verifying the end product is conducted \nas spelled out in the plans and procedures and confor-\n\nmance established to each specified verification require-\nment. The responsible engineer should ensure that the \nprocedures were followed and performed as planned, \nthe verification-enabling products were calibrated cor-\nrectly, and the data were collected and recorded for re-\nquired verification measures.\n\nThe Decision Analysis Process should be used to help \nmake decisions with respect to making needed changes \nin the verification plans, environment, and/or conduct.\n\nOutcomes of conducting verification include the follow-\ning:\n\nA verified product is established with supporting con- ?\nfirmation that the appropriate results were collected \nand evaluated to show completion of verification ob-\njectives,\nA determination as to whether the realized end  ?\nproduct (in the appropriate form for the life-cycle \nphase) complies with its specified requirements,\nA determination that the verification product was ap- ?\npropriately integrated with the verification environ-\nment and each specified requirement was properly \nverified, and\nA determination that product functions were veri- ?\nfied both together and with interfacing products \nthroughout their performance envelope.\n\nAnalyze Product Verification Results\nOnce the verification activities have been completed, \nthe results are collected and analyzed. The data are an-\nalyzed for quality, integrity, correctness, consistency, \nand validity, and any verification anomalies, variations, \nand out-of-compliance conditions are identified and re-\nviewed.\n\nVariations, anomalies, and out-of-compliance condi-\ntions must be recorded and reported for followup action \nand closure. Verification results should be recorded in \nthe requirements compliance matrix developed during \nthe Technical Requirements Definition Process or other \nmechanism to trace compliance for each verification re-\nquirement. \n\nSystem design and product realization process activities \nmay be required to resolve anomalies not resulting from \npoor verification conduct, design, or conditions. If there \nare anomalies not resulting from the verification con-\nduct, design, or conditions, and the mitigation of these \n\nNote: Depending on the nature of the verification ef-\nfort and the life-cycle phase the program is in, some \ntype of review to assess readiness for verification (as \nwell as validation later) is typically held. In earlier \nphases of the life cycle, these reviews may be held in-\nformally; in later phases of the life cycle, this review \nbecomes a formal event called a Test Readiness Re-\nview. TRRs and other technical reviews are an activity \nof the Technical Assessment Process. \n\nOn most projects, a number of TRRs with tailored en-\ntrance/success criteria are held to assess the readiness \nand availability of test ranges; test facilities; trained \ntesters; instrumentation; integration labs; support \nequipment; and other enabling products; etc. \n\nPeer reviews are additional reviews that may be con-\nducted formally or informally to ensure readiness for \nverification (as well as the results of the verification \nprocess).\n\n\n\n88 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nanomalies results in a change to the product, the verifica-\ntion may need to be planned and conducted again. \n\nOutcomes of analyzing the verification results include \nthe following:\n\nEnd-product variations, anomalies, and out-of-com- ?\npliance conditions have been identified;\nAppropriate replanning, redefinition of requirements,  ?\ndesign and reverification have been accomplished for \nresolution for anomalies, variations, or out-of-com-\npliance conditions (for problems not caused by poor \nverification conduct);\nVariances, discrepancies, or waiver conditions have  ?\nbeen accepted or dispositioned;\nDiscrepancy and corrective action reports have been  ?\ngenerated as needed; and\nThe verification report is completed. ?\n\nReengineering\nBased on analysis of verification results, it could be nec-\nessary to re-realize the end product used for verification \nor to reengineer the end products assembled and inte-\ngrated into the product being verified, based on where \nand what type of defect was found.\n\nReengineering could require the reapplication of the \nsystem design processes (Stakeholder Expectations Def-\ninition, Technical Requirements Definition, Logical De-\ncomposition, and Design Solution Definition). \n\nVerification Deficiencies\nVerification test outcomes can be unsatisfactory for sev-\neral reasons. One reason is poor conduct of the verifica-\ntion (e.g., procedures not followed, equipment not cali-\nbrated, improper verification environmental conditions, \nor failure to control other variables not involved in veri-\nfying a specified requirement). A second reason could \nbe that the realized end product used was not realized \ncorrectly. Reapplying the system design processes could \ncreate the need for the following:\n\nReengineering products lower in the system structure  ?\nthat make up the product that were found to be de-\nfective (i.e., they failed to satisfy verification require-\nments) and/or\nReperforming the Product Verification Process. ?\n\nPass Verification But Fail Validation? \n\nMany systems successfully complete verification but then \nare unsuccessful in some critical phase of the validation \nprocess, delaying development and causing extensive re-\nwork and possible compromises with the stakeholder. \nDeveloping a solid ConOps in early phases of the project \n(and refining it through the requirements development \nand design phases) is critical to preventing unsuccessful \nvalidation. Communications with stakeholders helps to \nidentify operational scenarios and key needs that must \nbe understood when designing and implementing the \nend product. Should the product fail validation, rede-\nsign may be a necessary reality. Review of the under-\nstood requirements set, the existing design, operational \nscenarios, and support material may be necessary, as \nwell as negotiations and compromises with the cus-\ntomer, other stakeholders, and/or end users to deter-\nmine what, if anything, can be done to correct or re-\nsolve the situation. This can add time and cost to the \noverall project or, in some cases, cause the project to \nfail or be cancelled.\n\nCapture Product Verification Work Products\n\nVerification work products (inputs to the Technical Data \nManagement Process) take many forms and involve \nmany sources of information. The capture and recording \nof verification results and related data is a very impor-\ntant, but often underemphasized, step in the Product \nVerification Process.\n\nVerification results, anomalies, and any corrective \naction(s) taken should be captured, as should all relevant \nresults from the application of the Product Verification \nProcess (related decisions, rationale for the decisions \nmade, assumptions, and lessons learned). \n\nOutcomes of capturing verification work products in-\nclude the following:\n\nVerification of work products are recorded, e.g., type  ?\nof verification, procedures, environments, outcomes, \ndecisions, assumptions, corrective actions, lessons \nlearned.\n\nNote: Nonconformances and discrepancy reports \nmay be directly linked with the Technical Risk Man-\nagement Process. Depending on the nature of the \nnonconformance, approval through such bodies as a \nmaterial review board or configuration control board \n(which typically includes risk management participa-\ntion) may be required.\n\n\n\n5.3 Product Verification\n\nNASA Systems Engineering Handbook ? 89\n\nVariations, anomalies, and out-of-compliance condi- ?\ntions have been identified and documented, including \nthe actions taken to resolve them.\nProof that the realized end product did or did not sat- ?\nisfy the specified requirements is documented. \nThe verification report is developed, including: ?\n\nRecorded test/verification results/data; ?\nVersion of the set of specified requirements used; ?\nVersion of the product verified; ?\nVersion or standard for tools, data, and equipment  ?\nused;\nResults of each verification including pass or fail  ?\ndeclarations; and\nExpected versus actual discrepancies. ?\n\n5.3.1.3 Outputs\nKey outputs from the process are: \n\nDiscrepancy reports and identified corrective actions; ?\nVerified product to validation or integration; and ?\nVerification report(s) and updates to requirements  ?\ncompliance documentation (including verification \nplans, verification procedures, verification matrices, \nverification results and analysis, and test/demonstra-\ntion/inspection/analysis records).\n\nSuccess criteria include: (1) documented objective evi-\ndence of compliance (or waiver, as appropriate) with \neach system-of-interest requirement and (2) closure of \nall discrepancy reports. The Product Verification Pro-\ncess is not considered or designated complete until all \ndiscrepancy reports are closed (i.e., all errors tracked to \nclosure).\n\n5.3.2 Product Verification Guidance\n\n5.3.2.1 Verification Program\nA verification program should be tailored to the project \nit supports. The project manager/systems engineer must \nwork with the verification engineer to develop a verifi-\ncation program concept. Many factors need to be con-\nsidered in developing this concept and the subsequent \nverification program. These factors include:\n\nProject type, especially for flight projects. Verification  ?\nmethods and timing depend on:\n\nThe type of flight article involved (e.g., an experi- ?\nment, payload, or launch vehicle). \n\nNASA payload classification ( ? NPR 8705.4, Risk \nClassification for NASA Payloads). Guidelines are \nintended to serve as a starting point for establish-\nment of the formality of test programs which can be \ntailored to the needs of a specific project based on \nthe \u201cA-D\u201d payload classification.\nProject cost and schedule implications. Verifi- ?\ncation activities can be significant drivers of a \nproject\u2019s cost and schedule; these implications \nshould be considered early in the development \nof the verification program. Trade studies should \nbe performed to support decisions about verifi-\ncation methods and requirements and the selec-\ntion of facility types and locations. For example, a \ntrade study might be made to decide between per-\nforming a test at a centralized facility or at several \ndecentralized locations.\nRisk implications. Risk management must be con- ?\nsidered in the development of the verification pro-\ngram. Qualitative risk assessments and quantitative \nrisk analyses (e.g., a Failure Mode and Effects Anal-\nysis (FMECA)) often identify new concerns that can \nbe mitigated by additional testing, thus increasing \nthe extent of verification activities. Other risk as-\nsessments contribute to trade studies that determine \nthe preferred methods of verification to be used and \nwhen those methods should be performed. For ex-\nample, a trade might be made between performing \na model test versus determining model characteris-\ntics by a less costly, but less revealing, analysis. The \nproject manager/systems engineer must determine \nwhat risks are acceptable in terms of the project\u2019s \ncost and schedule. \n\nAvailability of verification facilities/sites and trans- ?\nportation assets to move an article from one location \nto another (when needed). This requires coordination \nwith the Integrated Logistics Support (ILS) engineer.\nAcquisition strategy (i.e., in-house development or  ?\nsystem contract). Often, a NASA field center can \nshape a contractor\u2019s verification process through the \nproject\u2019s SOW.\nDegree of design inheritance and hardware/software  ?\nreuse.\n\n5.3.2.2 Verification in the Life Cycle\n\nThe type of verification completed will be a function of \nthe life-cycle phase and the position of the end product \n\n\n\n90 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nwithin the system structure. The end product must be \nverified and validated before it is transitioned to the next \nlevel up as part of the bottom-up realization process. \n(See Figure 5.3-2.)\n\nWhile illustrated here as separate processes, there can be \nconsiderable overlap between some verification and vali-\ndation events when implemented. \n\nQuality Assurance in Verification\nEven with the best of available designs, hardware fabri-\ncation, software coding, and testing, projects are subject \nto the vagaries of nature and human beings. The systems \nengineer needs to have some confidence that the system \nactually produced and delivered is in accordance with its \nfunctional, performance, and design requirements. QA \nprovides an independent assessment to the project man-\nager/systems engineer of the items produced and pro-\ncesses used during the project life cycle. The QA engi-\nneer typically acts as the systems engineer\u2019s eyes and ears \nin this context. \n\nThe QA engineer typically monitors the resolution and \ncloseout of nonconformances and problem/failure re-\nports; verifies that the physical configuration of the \nsystem conforms to the build-to (or code-to) documen-\ntation approved at CDR; and collects and maintains QA \ndata for subsequent failure analyses. The QA engineer also \nparticipates in major reviews (primarily SRR, PDR, CDR, \nand FRR) on issues of design, materials, workmanship, \nfabrication and verification processes, and other charac-\nteristics that could degrade product system quality.\n\nThe project manager/systems engineer must work with \nthe QA engineer to develop a QA program (the extent, \nresponsibility, and timing of QA activities) tailored to \nthe project it supports. In part, the QA program ensures \nverification requirements are properly specified, espe-\ncially with respect to test environments, test configura-\ntions, and pass/fail criteria, and monitors qualification \nand acceptance tests to ensure compliance with verifica-\ntion requirements and test procedures to ensure that test \ndata are correct and complete. \n\nTier 1\nEnd Product\n\nDeliver veri?ed\nend product\n\nValidate against stakeholder\nexpectations and ConOps \n\nTo end user/\nuse environment \n\nDeliver veri?ed\nend product\n\nDeliver veri?ed\nend product\n\nDeliver veri?ed\nend product\n\nDeliver veri?ed\nend product\n\nValidate against stakeholder\nexpectations and ConOps \n\nValidate against stakeholder\nexpectations and ConOps \n\nValidate against stakeholder\nexpectations and ConOps \n\nVerify against\nend product\n\nspeci?ed\nrequirements\n\nVerify against\nend product\n\nspeci?ed\nrequirements\n\nVerify against\nend product\n\nspeci?ed\nrequirements\n\nVerify against\nend product\n\nspeci?ed\nrequirements\n\nVerify against\nend product\n\nspeci?ed\nrequirements\n\nTier 2\nEnd Product\n\nTier 3\nEnd Product\n\nTier 4\nEnd Product\n\nTier 5\nEnd Product\n\nFigure 5.3?2 Bottom?up realization process\n\n\n\n5.3 Product Verification\n\nNASA Systems Engineering Handbook ? 91\n\nConfiguration Verification\n\nConfiguration verification is the process of verifying that \nresulting products (e.g., hardware and software items) \nconform to the baselined design and that the baseline \ndocumentation is current and accurate. Configuration \nverification is accomplished by two types of control gate \nactivity: audits and technical reviews. \n\nQualification Verification \n\nQualification-stage verification activities begin after \ncompletion of development of the flight/operations hard-\nware designs and include analyses and testing to ensure \nthat the flight/operations or flight-type hardware (and \nsoftware) will meet functional and performance require-\nments in anticipated environmental conditions. During \nthis stage, many performance requirements are verified, \nwhile analyses and models are updated as test data are ac-\nquired. Qualification tests generally are designed to sub-\nject the hardware to worst-case loads and environmental \nstresses plus a defined level of margin. Some of the veri-\nfications performed to ensure hardware compliance are \nvibration/acoustic, pressure limits, leak rates, thermal \nvacuum, thermal cycling, Electromagnetic Interference \nand Electromagnetic Compatibility (EMI/EMC), high- \nand low-voltage limits, and lifetime/cycling. Safety re-\nquirements, defined by hazard analysis reports, may also \nbe satisfied by qualification testing.\n\nQualification usually occurs at the component or sub-\nsystem level, but could occur at the system level as well. A \nproject deciding against building dedicated qualification \nhardware\u2014and using the flight/operations hardware it-\nself for qualification purposes\u2014is termed \u201cprotoflight.\u201d \nHere, the requirements being verified are typically less \nthan that of qualification levels but higher than that of \nacceptance levels.\n\nQualification verification verifies the soundness of the \ndesign. Test levels are typically set with some margin \nabove expected flight/operations levels, including the \nmaximum number of cycles that can be accumulated \nduring acceptance testing. These margins are set to ad-\ndress design safety margins in general, and care should \nbe exercised not to set test levels so that unrealistic failure \nmodes are created.\n\nAcceptance Verification \n\nAcceptance-stage verification activities provide the as-\nsurance that the flight/operations hardware and soft-\n\nware are in compliance with all functional, performance, \nand design requirements and are ready for shipment to \nthe launch site. The acceptance stage begins with the ac-\nceptance of each individual component or piece part for \nassembly into the flight/operations article, continuing \nthrough the System Acceptance Review (SAR). (See \nSubsection 6.7.2.1.) \n\nSome verifications cannot be performed after a flight/\noperations article, especially a large one, has been assem-\nbled and integrated (e.g., due to inaccessibility). When \nthis occurs, these verifications are to be performed during \nfabrication and integration, and are known as \u201cin-pro-\ncess\u201d tests. In this case, acceptance testing begins with in-\nprocess testing and continues through functional testing, \nenvironmental testing, and end-to-end compatibility \ntesting. Functional testing normally begins at the com-\nponent level and continues at the systems level, ending \nwith all systems operating simultaneously. \n\nWhen flight/operations hardware is unavailable, or its \nuse is inappropriate for a specific test, simulators may be \nused to verify interfaces. Anomalies occurring during \na test are documented on the appropriate reporting \nsystem, and a proposed resolution should be defined be-\nfore testing continues. Major anomalies, or those that are \nnot easily dispositioned, may require resolution by a col-\nlaborative effort of the systems engineer and the design, \ntest, and other organizations. Where appropriate, anal-\nyses and models are validated and updated as test data \nare acquired.\n\nAcceptance verification verifies workmanship, not de-\nsign. Test levels are set to stress items so that failures arise \nfrom defects in parts, materials, and workmanship. As \nsuch, test levels are those anticipated during flight/op-\nerations with no additional margin.\n\nDeployment Verification \n\nThe pre-launch verification stage begins with the arrival \nof the flight/operations article at the launch site and con-\ncludes at liftoff. During this stage, the flight/operations \narticle is processed and integrated with the launch ve-\nhicle. The launch vehicle could be the shuttle or some \nother launch vehicle, or the flight/operations article \ncould be part of the launch vehicle. Verifications per-\nformed during this stage ensure that no visible damage \nto the system has occurred during shipment and that the \nsystem continues to function properly. \n\n\n\n92 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nIf system elements are shipped separately and integrated \nat the launch site, testing of the system and system in-\nterfaces is generally required. If the system is integrated \ninto a carrier, the interface to the carrier must also be \nverified. Other verifications include those that occur fol-\nlowing integration into the launch vehicle and those that \noccur at the launch pad; these are intended to ensure that \nthe system is functioning and in its proper launch con-\nfiguration. Contingency verifications and procedures are \ndeveloped for any contingencies that can be foreseen to \noccur during pre-launch and countdown. These contin-\ngency verifications and procedures are critical in that \nsome contingencies may require a return of the launch \nvehicle or flight/operations article from the launch pad \nto a processing facility.\n\nOperational and Disposal Verification \nOperational verification begins in Phase E and provides \nthe assurance that the system functions properly in a rel-\nevant environment. These verifications are performed \nthrough system activation and operation, rather than \nthrough a verification activity. Systems that are assem-\nbled on-orbit must have each interface verified and must \nfunction properly during end-to-end testing. Mechan-\nical interfaces that provide fluid and gas flow must be \nverified to ensure no leakage occurs and that pressures \nand flow rates are within specification. Environmental \nsystems must be verified. \n\nDisposal verification provides the assurance that the \nsafe deactivation and disposal of all system products \nand processes has occurred. The disposal stage begins in \nPhase F at the appropriate time (i.e., either as scheduled, \nor earlier in the event of premature failure or accident) \nand concludes when all mission data have been acquired \nand verifications necessary to establish compliance with \ndisposal requirements are finished. \n\nBoth operational and disposal verification activities may \nalso include validation assessments, that is, assessments \nof the degree to which the system accomplished the de-\nsired mission goals/objectives.\n\n5.3.2.3 Verification Procedures \nVerification procedures provide step-by-step instructions \nfor performing a given verification activity. This proce-\ndure could be a test, demonstration, or any other verifica-\ntion-related activity. The procedure to be used is written \nand submitted for review and approval at the Test Readi-\n\nness Review (TRR) for the verification activity. (See Test \nReadiness Review discussion in Subsection 6.7.2.1.) \n\nProcedures are also used to verify the acceptance of fa-\ncilities, electrical and mechanical ground support equip-\nment, and special test equipment. The information gen-\nerally contained in a procedure is as follows, but it may \nvary according to the activity and test article:\n\nNomenclature and identification of the test article or  ?\nmaterial;\nIdentification of test configuration and any differences  ?\nfrom flight/operations configuration;\nIdentification of objectives and criteria established for  ?\nthe test by the applicable verification specification;\nCharacteristics and design criteria to be inspected or  ?\ntested, including values, with tolerances, for accep-\ntance or rejection;\nDescription, in sequence, of steps and operations to  ?\nbe taken;\nIdentification of computer software required; ?\nIdentification of measuring, test, and recording equip- ?\nment to be used, specifying range, accuracy, and type;\nCredentials showing that required computer test pro- ?\ngrams/support equipment and software have been \nverified prior to use with flight/operations hardware;\nAny special instructions for operating data recording  ?\nequipment or other automated test equipment as ap-\nplicable;\nLayouts, schematics, or diagrams showing identifica- ?\ntion, location, and interconnection of test equipment, \ntest articles, and measuring points;\nIdentification of hazardous situations or operations; ?\nPrecautions and safety instructions to ensure safety of  ?\npersonnel and prevent degradation of test articles and \nmeasuring equipment;\nEnvironmental and/or other conditions to be main- ?\ntained with tolerances;\nConstraints on inspection or testing; ?\nSpecial instructions for nonconformances and anom- ?\nalous occurrences or results; and\nSpecifications for facility, equipment maintenance,  ?\nhousekeeping, quality inspection, and safety and han-\ndling requirements before, during, and after the total \nverification activity.\n\nThe written procedure may provide blank spaces in the \nformat for the recording of results and narrative com-\n\n\n\n5.3 Product Verification\n\nNASA Systems Engineering Handbook ? 93\n\nments so that the completed procedure can serve as part \nof the verification report. The as-run and certified copy \nof the procedure is maintained as part of the project\u2019s ar-\nchives.\n\n5.3.2.4 Verification Reports\nA verification report should be provided for each anal-\nysis and, at a minimum, for each major test activity\u2014\nsuch as functional testing, environmental testing, and \nend-to-end compatibility testing\u2014occurring over long \nperiods of time or separated by other activities. Verifi-\ncation reports may be needed for each individual test \nactivity, such as functional testing, acoustic testing, vi-\nbration testing, and thermal vacuum/thermal balance \ntesting. Verification reports should be completed within \na few weeks following a test and should provide evidence \nof compliance with the verification requirements for \nwhich it was conducted. \n\nThe verification report should include as appropriate:\nVerification objectives and the degree to which they  ?\nwere met;\nDescription of verification activity; ?\nTest configuration and differences from flight/opera- ?\ntions configuration;\nSpecific result of each test and each procedure, in- ?\ncluding annotated tests;\nSpecific result of each analysis; ?\nTest performance data tables, graphs, illustrations,  ?\nand pictures;\nDescriptions of deviations from nominal results,  ?\nproblems/failures, approved anomaly corrective ac-\ntions, and retest activity;\nSummary of nonconformance/discrepancy reports,  ?\nincluding dispositions;\nConclusions and recommendations relative to success  ?\nof verification activity;\nStatus of support equipment as affected by test; ?\nCopy of as-run procedure; and ?\nAuthentication of test results and authorization of ac- ?\nceptability.\n\n5.3.2.5 End-to-End System Testing \nThe objective of end-to-end testing is to demonstrate \ninterface compatibility and desired total functionality \namong different elements of a system, between systems, \n\nand within a system as a whole. End-to-end tests per-\nformed on the integrated ground and flight system in-\nclude all elements of the payload, its control, stimulation, \ncommunications, and data processing to demonstrate \nthat the entire system is operating in a manner to fulfill \nall mission requirements and objectives. \n\nEnd-to-end testing includes executing complete threads \nor operational scenarios across multiple configuration \nitems, ensuring that all mission and performance re-\nquirements are verified. Operational scenarios are used \nextensively to ensure that the system (or collections of \nsystems) will successfully execute mission requirements. \nOperational scenarios are a step-by-step description of \nhow the system should operate and interact with its users \nand its external interfaces (e.g., other systems). Scenarios \nshould be described in a manner that will allow engi-\nneers to walk through them and gain an understanding \nof how all the various parts of the system should function \nand interact as well as verify that the system will satisfy \nthe user\u2019s needs and expectations. Operational scenarios \nshould be described for all operational modes, mis-\nsion phases (e.g., installation, startup, typical examples \nof normal and contingency operations, shutdown, and \nmaintenance), and critical sequences of activities for all \nclasses of users identified. Each scenario should include \nevents, actions, stimuli, information, and interactions as \nappropriate to provide a comprehensive understanding \nof the operational aspects of the system.\n\nNote: It is important to understand that, over the life-\ntime of a system, requirements may change or com-\nponent obsolescence may make a design solution \ntoo difficult to produce from either a cost or technical \nstandpoint. In these instances, it is critical to employ \nthe systems engineering design processes at a lower \nlevel to ensure the modified design provides a proper \ndesign solution. An evaluation should be made to de-\ntermine the magnitude of the change required, and \nthe process should be tailored to address the issues \nappropriately. A modified qualification, verification, \nand validation process may be required to baseline a \nnew design solution, consistent with the intent previ-\nously described for those processes. The acceptance \ntesting will also need to be updated as necessary to \nverify that the new product has been manufactured \nand coded in compliance with the revised baselined \ndesign.\n\n\n\n94 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nFigure 5.3-3 presents an example of an end-to-end data \nflow for a scientific satellite mission. Each arrow in the \ndiagram represents one or more data or control flows \nbetween two hardware, software, subsystem, or system \nconfiguration items. End-to-end testing verifies that the \ndata flows throughout the multisystem environment are \ncorrect, that the system provides the required function-\nality, and that the outputs at the eventual end points cor-\nrespond to expected results. Since the test environment is \nas close an approximation as possible to the operational \nenvironment, performance requirements testing is also \nincluded. This figure is not intended to show the full ex-\ntent of end-to-end testing. Each system shown would \nneed to be broken down into a further level of granu-\nlarity for completeness.\n\nEnd-to-end testing is an integral part of the verification \nand validation of the total system and is an activity that \nis employed during selected hardware, software, and \nsystem phases throughout the life cycle. In comparison \nwith configuration item testing, end-to-end testing ad-\ndresses each configuration item only down to the level \nwhere it interfaces externally to other configuration \nitems, which can be either hardware, software, or human \nbased. Internal interfaces (e.g., software subroutine calls, \nanalog-to-digital conversion) of a configuration item are \nnot within the scope of end-to-end testing.\n\nHow to Perform End?to?End Testing \nEnd-to-end testing is probably the most significant el-\nement of any project verification program and the test \nshould be designed to satisfy the edict to \u201ctest the way \nwe fly.\u201d This means assembling the system in its real-\nistic configuration, subjecting it to a realistic environ-\nment and then \u201cflying\u201d it through all of its expected op-\nerational modes. For a scientific robotic mission, targets \nand stimuli should be designed to provide realistic in-\nputs to the scientific instruments. The output signals \nfrom the instruments would flow through the satellite \ndata-handling system and then be transmitted to the \nactual ground station through the satellite communica-\ntions system. If data are transferred to the ground station \nthrough one or more satellite or ground relays (e.g., the \nTracking and Data Relay Satellite System (TDRSS)) then \nthose elements must be included as part of the test. \n\nThe end-to-end compatibility test encompasses the en-\ntire chain of operations that will occur during all mission \nmodes in such a manner as to ensure that the system will \nfulfill mission requirements. The mission environment \nshould be simulated as realistically as possible, and the \ninstruments should receive stimuli of the kind they will \nreceive during the mission. The Radio Frequency (RF) \nlinks, ground station operations, and software functions \nshould be fully exercised. When acceptable simulation \n\nFigure 5.3?3 Example of end?to?end data flow for a scientific satellite mission\n\nEXTERNAL\nSTIMULI\n\nInstrument\nSet B \n\nX-rays\n\nInfrared\n\nVisible\n\nUltraviolet\n\nMicrowave\n\nInstrument\nSet A\n\nSpacecraft\nExecution\n\nScienti?c\nCommunity\n\nMission\nPlanning\n\nData\nCapture\n\nSoftware\nLoads\n\nCommand\nGenerationPlanning\n\nArchival\n\nTransmission\n\nAnalysis\n\nUplink Process\n\nDownlink Process\n\nEXTERNAL\nSYSTEMS\n\nGROUND SYSTEM FLIGHT SYSTEM\n\n\n\n5.3 Product Verification\n\nNASA Systems Engineering Handbook ? 95\n\nfacilities are available for portions of the operational sys-\ntems, they may be used for the test instead of the actual \nsystem elements. The specific environments under which \nthe end-to-end test is conducted and the stimuli, pay-\nload configuration, RF links, and other system elements \nto be used must be determined in accordance with the \ncharacteristics of the mission.\n\nAlthough end-to-end testing is probably the most com-\nplex test in any system verification program, the same \ncareful preparation is necessary as for any other system-\nlevel test. For example, a test lead must be appointed and \nthe test team selected and trained. Adequate time must \nbe allocated for test planning and coordination with the \ndesign team. Test procedures and test software must be \ndocumented, approved, and placed under configuration \ncontrol.\n\nPlans, agreements, and facilities must be put in place well \nin advance of the test to enable end-to-end testing be-\ntween all components of the system. \n\nOnce the tests are run, the test results are documented \nand any discrepancies carefully recorded and reported. \nAll test data must be maintained under configuration \ncontrol.\n\nBefore completing end-to-end testing, the following ac-\ntivities are completed for each configuration item:\n\nAll requirements, interfaces, states, and state tran- ?\nsitions of each configuration item should be tested \nthrough the exercise of comprehensive test proce-\ndures and test cases to ensure the configuration items \nare complete and correct. \nA full set of operational range checking tests should  ?\nbe conducted on software variables to ensure that the \nsoftware performs as expected within its complete \nrange and fails, or warns, appropriately for out-of-\nrange values or conditions. \n\nEnd-to-end testing activities include the following:\nOperational scenarios are created that span all of the 1. \nfollowing items (during nominal, off-nominal, and \nstressful conditions) that could occur during the \nmission:\n\nMission phase, mode, and state transitions; ?\nFirst-time events; ?\nOperational performance limits; ?\nFault protection routines; ?\nFailure Detection, Isolation, and Recovery (FDIR)  ?\nlogic;\nSafety properties; ?\nOperational responses to transient or off-nom- ?\ninal sensor signals; and\nCommunication uplink and downlink.  ?\n\nThe operational scenarios are used to test the con-2. \nfiguration items, interfaces, and end-to-end perfor-\nmance as early as possible in the configuration items\u2019 \ndevelopment life cycle. This typically means simula-\ntors or software stubs have to be created to imple-\nment a full scenario. It is extremely important to \nproduce a skeleton of the actual system to run full \nscenarios as soon as possible with both simulated/\nstubbed-out and actual configuration items.\nA complete diagram and inventory of all interfaces 3. \nare documented.\nTest cases are executed to cover human-human, 4. \nhuman-hardware, human-software, hardware-soft-\nware, software-software, and subsystem-subsystem \ninterfaces and associated inputs, outputs, and modes \nof operation (including safing modes).\nIt is strongly recommended that during end-to-end 5. \ntesting, an operations staff member who has not pre-\nviously been involved in the testing activities be des-\nignated to exercise the system as it is intended to be \nused to determine if it will fail.\nThe test environment should approximate/simulate 6. \nthe actual operational conditions when possible. The \nfidelity of the test environment should be authenti-\ncated. Differences between the test and operational \nenvironment should be documented in the test or \nverification plan.\nWhen testing of a requirement is not possible, veri-7. \nfication is demonstrated by other means (e.g., model \nchecking, analysis, or simulation). If true end-to-end \ntesting cannot be achieved, then the testing must \nbe done piecemeal and patched together by anal-\nysis and simulation. An example of this would be a \nsystem that is assembled on orbit where the various \nelements come together for the first time on orbit.\nWhen an error in the developed system is identified 8. \nand fixed, regression testing of the system or compo-\n\nNote: This is particularly important when missions are \ndeveloped with international or external partners.\n\n\n\n96 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nnent is performed to ensure that modifications have \nnot caused unintended effects and that the system \nor component still complies with previously tested \nspecified requirements.\nWhen tests are aborted or a test is known to be flawed 9. \n(e.g., due to configuration, test environment), the test \nshould be rerun after the identified problem is fixed.\nThe operational scenarios should be used to formu-10. \nlate the final operations plan.\nPrior to system delivery, as part of the system qualifi-11. \ncation testing, test cases should be executed to cover \nall of the plans documented in the operations plan in \nthe order in which they are expected to occur during \nthe mission.\n\nEnd-to-end test documentation includes the following:\nInclusion of end-to-end testing plans as a part of the  ?\ntest or verification plan.\nA document, matrix, or database under configura- ?\ntion control that traces the end-to-end system test \nsuite to the results. Data that are typically recorded \ninclude the test-case identifier, subsystems/hardware/\nprogram sets exercised, list of the requirements being \nverified, interfaces exercised, date, and outcome of \ntest (i.e., whether the test actual output met the ex-\npected output).\nEnd-to-end test cases and procedures (including in- ?\nputs and expected outputs).\nA record of end-to-end problems/failures/anomalies. ?\n\nEnd-to-end testing can be integrated with other project \ntesting activities; however, the documentation men-\ntioned in this subsection should be readily extractable \nfor review, status, and assessment.\n\n5.3.2.6 Modeling and Simulation\nFor the Product Verification Process, a model is a phys-\nical, mathematical, or logical representation of an end \nproduct to be verified. Modeling and Simulation (M&S) \ncan be used to augment and support the Product Verifi-\ncation Process and is an effective tool for performing the \nverification whether in early life-cycle phases or later. Both \nthe facilities and the model itself are developed using the \nsystem design and product realization processes. \n\nThe model used, as well as the M&S facility, are enabling \nproducts and must use the 17 technical processes (see NPR \n7123.1, NASA Systems Engineering Processes and Require-\n\nments) for their development and realization (including \nacceptance by the operational community) to ensure that \nthe model and simulation adequately represent the opera-\ntional environment and performance of the modeled end \nproduct. Additionally, in some cases certification is re-\nquired before models and simulations can be used. \n\nM&S assets can come from a variety of sources; for ex-\nample, contractors, other Government agencies, or labo-\nratories can provide models that address specific system \nattributes.\n\n5.3.2.7 Hardware-in-the-Loop\nFully functional end products, such as an actual piece of \nhardware, may be combined with models and simula-\ntions that simulate the inputs and outputs of other end \nproducts of the system. This is referred to as \u201cHardware-\nin-the-Loop\u201d (HWIL) testing. HWIL testing links all el-\nements (subsystems and test facilities) together within \na synthetic environment to provide a high-fidelity, real-\ntime operational evaluation for the real system or sub-\nsystems. The operator can be intimately involved in the \ntesting, and HWIL resources can be connected to other \nfacilities for distributed test and analysis applications. \nOne of the uses of HWIL testing is to get as close to the \nactual concept of operation as possible to support verifi-\ncation and validation when the operational environment \nis difficult or expensive to recreate. \n\nDuring development, this HWIL verification normally \ntakes place in an integration laboratory or test facility. For \nexample, HWIL could be a complete spacecraft in a spe-\ncial test chamber, with the inputs/outputs being provided \nas output from models that simulate the system in an op-\nerational environment. Real-time computers are used to \ncontrol the spacecraft and subsystems in projected op-\nerational scenarios. Flight dynamics, responding to the \n\nNote: The development of the physical, mathemati-\ncal, or logical model includes evaluating whether the \nmodel to be used as representative of the system end \nproduct was realized according to its design\u2013solu-\ntion-specified requirements for a model and whether \nit will be valid for use as a model. In some cases, the \nmodel must also be accredited to certify the range of \nspecific uses for which the model can be used. Like \nany other enabling product, budget and time must \nbe planned for creating and evaluating the model to \nbe used to verify the applicable system end product.\n\n\n\n5.3 Product Verification\n\nNASA Systems Engineering Handbook ? 97\n\ncommands issued by the guidance and control system \nhardware/software, are simulated in real-time to deter-\nmine the trajectory and to calculate system flight condi-\ntions. HWIL testing verifies that the end product being \nevaluated meets the interface requirements, properly \ntransforming inputs to required outputs. HWIL mod-\neling can provide a valuable means of testing physical \nend products lower in the system structure by providing \nsimulated inputs to the end product or receiving outputs \nfrom the end product to evaluate the quality of those out-\nputs. This tool can be used throughout the life cycle of a \nprogram or project. The shuttle program uses an HWIL \nto verify software and hardware updates for the control \nof the shuttle main engines. \n\nModeling, simulation, and hardware/human-in-the-\nloop technology, when appropriately integrated and se-\nquenced with testing, provide a verification method at \na reasonable cost. This integrated testing process specif-\nically (1) reduces the cost of life-cycle testing, (2) pro-\nvides significantly more engineering/performance in-\nsights into each system evaluated, and (3) reduces test \ntime and lowers project risk. This process also signifi-\ncantly reduces the number of destructive tests required \nover the life of the product. The integration of M&S \ninto verification testing provides insights into trends \nand tendencies of system and subsystem performance \nthat might not otherwise be possible due to hardware \nlimitations. \n\n\n\n98 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nThe Product Validation Process is the second of the ver-\nification and validation processes conducted on a real-\nized end product. While verification proves whether \n\u201cthe system was done right,\u201d validation proves whether \n\u201cthe right system was done.\u201d In other words, verifica-\ntion provides objective evidence that every \u201cshall\u201d state-\nment was met, whereas validation is performed for the \nbenefit of the customers and users to ensure that the \nsystem functions in the expected manner when placed \nin the intended environment. This is achieved by ex-\namining the products of the system at every level of the \nstructure.\n\nValidation confirms that realized end products at any \nposition within the system structure conform to their \nset of stakeholder expectations captured in the ConOps, \nand ensures that any anomalies discovered during vali-\ndation are appropriately resolved prior to product de-\nlivery. This section discusses the process activities, \ntypes of validation, inputs and outputs, and potential \ndeficiencies.\n\nDistinctions Between Product Verification and \nProduct Validation\nFrom a process perspective, Product Verification and \nProduct Validation may be similar in nature, but the ob-\njectives are fundamentally different.\n\nFrom a customer point of view, the interest is in whether \nthe end product provided will do what they intend within \nthe environment of use. It is essential to confirm that the \n\nrealized product is in conformance with its specifications \nand design description documentation because these \nspecifications and documents will establish the configura-\ntion baseline of the product, which may have to be mod-\nified at a later time. Without a verified baseline and ap-\npropriate configuration controls, such later modifications \ncould be costly or cause major performance problems.\n\nWhen cost-effective and warranted by analysis, var-\nious combined tests are used. The expense of validation \ntesting alone can be mitigated by ensuring that each end \nproduct in the system structure was correctly realized in \naccordance with its specified requirements before con-\nducting validation.\n\n5.4.1 Process Description\nFigure 5.4-1 provides a typical flow diagram for the \nProduct Validation Process and identifies typical inputs, \noutputs, and activities to consider in addressing product \nvalidation.\n\n5.4.1.1 Inputs\n\nKey inputs to the process are: \nVerified product, ?\nValidation plan, ?\nBaselined stakeholder expectations (including ConOps  ?\nand mission needs and goals), and\nAny enabling products needed to perform the Product  ?\nValidation Process.\n\n5.4 Product Validation\n\nDifferences Between Verification and Validation Testing\n\nVerification Testing:  ? Verification testing relates back to the approved requirements set (such as an SRD) and can be \nperformed at different stages in the product life cycle. Verification testing includes: (1) any testing used to assist in the \ndevelopment and maturation of products, product elements, or manufacturing or support processes; and/or (2) any \nengineering-type test used to verify status of technical progress, to verify that design risks are minimized, to substan-\ntiate achievement of contract technical performance, and to certify readiness for initial validation testing. Verification \ntests use instrumentation and measurements, and are generally accomplished by engineers, technicians, or operator-\nmaintainer test personnel in a controlled environment to facilitate failure analysis. \n\nValidation Testing: ?  Validation relates back to the ConOps document. Validation testing is conducted under realistic \nconditions (or simulated conditions) on any end product for the purpose of determining the effectiveness and suit-\nability of the product for use in mission operations by typical users; and the evaluation of the results of such tests. Test-\ning is the detailed quantifying method of both verification and validation. However, testing is required to validate fi-\nnal end products to be produced and deployed. \n\n\n\n5.4 Product Validation\n\nNASA Systems Engineering Handbook ? 99\n\n5.4.1.2 Process Activities\nThe Product Validation Process demonstrates that the \nrealized end product satisfies its stakeholder (customer \nand other interested party) expectations within the in-\ntended operational environments, with validation per-\nformed by anticipated operators and/or users. The type \nof validation is a function of the life-cycle phase and the \nposition of the end product within the system structure. \n\nThere are five major steps in the validation process: \n(1) validation planning (prepare to implement the val-\nidation plan), (2) validation preparation (prepare for \nconducting validation), (3) conduct planned validation \n(perform validation), (4) analyze validation results, and \n(5) capture the validation work products.\n\nThe objectives of the Product Validation Process are: \nTo confirm that ?\n\nThe right product was realized\u2014the one wanted by  ?\nthe customer,\nThe realized product can be used by intended op- ?\nerators/users, and\nThe Measures of Effectiveness (MOEs) are satisfied. ?\n\nTo confirm that the realized product fulfills its intended  ?\nuse when operated in its intended environment:\n\nValidation is performed for each realized (imple- ?\nmented or integrated) product from the lowest end \nproduct in a system structure branch up to the top \nWBS model end product. \nEvidence is generated as necessary to confirm that  ?\nproducts at each layer of the system structure meet \nthe capability and other operational expectations \nof the customer/user/operator and other interested \nparties.\n\nTo ensure that any problems discovered are appropri- ?\nately resolved prior to delivery of the realized product \n(if validation is done by the supplier of the product) or \nprior to integration with other products into a higher \nlevel assembled product (if validation is done by the \nreceiver of the product).\n\nVerification and validation events are illustrated as sepa-\nrate processes, but when used, can considerably overlap. \nWhen cost effective and warranted by analysis, various \ncombined tests are used. However, while from a process \nperspective verification and validation are similar in na-\nture, their objectives are fundamentally different. \n\nFrom a customer\u2019s point of view, the interest is in whether \nthe end product provided will supply the needed capa-\nbilities within the intended environments of use. The \n\nFigure 5.4?1 Product Validation Process\n\nFrom Product\nVeri?cation Process To Product\n\nTransition Process\n\nProduct Validation\nResults \n\nProduct Validation\nReport  \n\nEnd Product\nto Be Validated \n\nProduct Validation\nPlan \n\nStakeholder Expectation\nBaseline  \n\nProduct Validation\u2013\nEnabling Products\n\nTo Technical\nAssessment Process\n\nFrom Design Solution \nDe?nition  and Technical\n\nPlanning Processes\n\nValidated End Product \n\nTo Technical Data\nManagement Process\n\nFrom Con?guration\nManagement Process\n\nFrom existing\nresources or Product\nTransition Process Product Validation\n\nWork Products  \n\nPerform the product\nvalidation\n\nAnalyze the outcomes of\nthe product validation\n\nCapture the work products\nfrom product validation \n\nPrepare a product\nvalidation report \n\nPrepare to conduct product\nvalidation\n\n\n\n100 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nexpense of validation testing alone can be mitigated by \nensuring that each end product in the system structure \nwas correctly realized in accordance with its specified re-\nquirements prior to validation, during verification. It is \npossible that the system design was not done properly \nand, even though the verification tests were successful \n(satisfying the specified requirements), the validation \ntests would still fail (stakeholder expectations not satis-\nfied). Thus, it is essential that validation of lower prod-\nucts in the system structure be conducted as well as veri-\nfication so as to catch design failures or deficiencies as \nearly as possible.\n\nProduct Validation Planning \nPlanning to conduct the product validation is a key first \nstep. The type of validation to be used (e.g., analysis, \ndemonstration, inspection, or test) should be established \nbased on the form of the realized end product, the appli-\ncable life-cycle phase, cost, schedule, resources available, \nand location of the system product within the system \nstructure. (See Appendix I for a sample verification and \nvalidation plan outline.)\n\nAn established set or subset of requirements to be val-\nidated should be identified and the validation plan re-\nviewed (an output of the Technical Planning Process, \nbased on design solution outputs) for any specific pro-\ncedures, constraints, success criteria, or other validation \nrequirements. The conditions and environment under \nwhich the product is to be validated should be estab-\nlished and the validation planned based on the relevant \nlife-cycle phase and associated success criteria identified. \nThe Decision Analysis Process should be used to help fi-\nnalize the planning details. \n\nIt is important to review the validation plans with rel-\nevant stakeholders and understand the relationship be-\ntween the context of the validation and the context of use \n(human involvement). As part of the planning process, \nvalidation-enabling products should be identified, and \nscheduling and/or acquisition initiated. \n\nProcedures should be prepared to conduct validation \nbased on the type (e.g., analysis, inspection, demon-\nstration, or test) planned. These procedures are typi-\ncally developed during the design phase of the project \nlife cycle and matured as the design is matured. Op-\nerational and use-case scenarios are thought through \nso as to explore all possible validation activities to be \nperformed. \n\nValidation Plan and Methods \nThe validation plan is one of the work products of the \nTechnical Planning Process and is generated during the \nDesign Solution Process to validate the realized product \nagainst the baselined stakeholder expectations. This plan \ncan take many forms. The plan describes the total Test \nand Evaluation (T&E) planning from development of \nlower end through higher end products in the system \nstructure and through operational T&E into production \nand acceptance. It may include the verification and val-\nidation plan. (See Appendix I for a sample verification \nand validation plan outline.)\n\nThe types of validation include test, demonstration, in-\nspection, and analysis. While the name of each method \n\nTypes of Validation\n\nAnalysis:  ? The use of mathematical modeling and \nanalytical techniques to predict the suitability of a \ndesign to stakeholder expectations based on cal-\nculated data or data derived from lower system \nstructure end product validations. It is generally \nused when a prototype; engineering model; or fab-\nricated, assembled, and integrated product is not \navailable. Analysis includes the use of both model-\ning and simulation.\n\nDemonstration: ?  The use of a realized end product \nto show that a set of stakeholder expectations can \nbe achieved. It is generally used for a basic confir-\nmation of performance capability and is differenti-\nated from testing by the lack of detailed data gath-\nering. Validation is done under realistic conditions \nfor any end product within the system structure for \nthe purpose of determining the effectiveness and \nsuitability of the product for use in NASA missions \nor mission support by typical users and evaluating \nthe results of such tests.\n\nInspection ? : The visual examination of a realized \nend product. It is generally used to validate phys-\nical design features or specific manufacturer iden-\ntification.\n\nTest:  ? The use of a realized end product to obtain \ndetailed data to validate performance or to pro-\nvide sufficient information to validate performance \nthrough further analysis. Testing is the detailed \nquantifying method of both verification and valida-\ntion but it is required in order to validate final end \nproducts to be produced and deployed.\n\n\n\n5.4 Product Validation\n\nNASA Systems Engineering Handbook ? 101\n\nis the same as the name of the methods for verification, \nthe purpose and intent are quite different.\n\nValidation is conducted by the user/operator or by the \ndeveloper, as determined by NASA Center directives or \nthe contract with the developers. Systems-level valida-\ntion (e.g., customer T&E and some other types of valida-\ntion) may be performed by an acquirer testing organiza-\ntion. For those portions of validation performed by the \ndeveloper, appropriate agreements must be negotiated to \nensure that validation proof-of-documentation is deliv-\nered with the realized product. \n\nAll realized end products, regardless of the source (buy, \nmake, reuse, assemble and integrate) and the position \nin the system structure, should be validated to demon-\nstrate/confirm satisfaction of stakeholder expectations. \nVariations, anomalies, and out-of-compliance condi-\ntions, where such have been detected, are documented \nalong with the actions taken to resolve the discrepancies. \nValidation is typically carried out in the intended oper-\national environment under simulated or actual opera-\ntional conditions, not under the controlled conditions \nusually employed for the Product Verification Process. \n\nValidation can be performed recursively throughout the \nproject life cycle and on a wide variety of product forms. \nFor example:\n\nSimulated (algorithmic models, virtual reality simu- ?\nlator);\nMockup (plywood, brassboard, breadboard); ?\nConcept description (paper report); ?\nPrototype (product with partial functionality); ?\nEngineering unit (fully functional but may not be  ?\nsame form/fit);\nDesign validation test units (form, fit and function  ?\nmay be the same, but they may not have flight parts);\nQualification unit (identical to flight unit but may be  ?\nsubjected to extreme environments); or\nFlight unit (end product that is flown). ?\n\nAny of these types of product forms may be in any of \nthese states:\n\nProduced (built, fabricated, manufactured, or coded); ?\nReused (modified internal nondevelopmental prod- ?\nucts or off-the-shelf product); or \nAssembled and integrated (a composite of lower level  ?\nproducts).\n\nOutcomes of validation planning include the following:\nThe validation type that is appropriate to confirm that  ?\nthe realized product or products conform to stake-\nholder expectations (based on the form of the real-\nized end product) has been identified.\nValidation procedures are defined based on: (1) the  ?\nneeded procedures for each type of validation se-\nlected, (2) the purpose and objective of each proce-\ndure step, (3) any pre-test and post-test actions, and \n(4) the criteria for determining the success or failure \nof the procedure.\nA validation environment (e.g., facilities, equipment,  ?\ntools, simulations, measuring devices, personnel, and \noperational conditions) in which the validation pro-\ncedures will be implemented has been defined.\n\nProduct Validation Preparation\nTo prepare for performing product validation, the ap-\npropriate set of expectations against which the valida-\ntion is to be made should be obtained. Also, the product \nto be validated (output from implementation, or integra-\ntion and verification), as well as the validation-enabling \nproducts and support resources (requirements identi-\nfied and acquisition initiated by design solution activi-\nties) with which validation will be conducted, should be \ncollected. \n\nNote: The final, official validation of the end product \nshould be for a controlled unit. Typically, attempt-\ning final validation against operational concepts on \na prototype is not acceptable: it is usually completed \non a qualification, flight, or other more final, con-\ntrolled unit. \n\nNote: In planning for validation, consideration should \nbe given to the extent to which validation testing will \nbe done. In many instances, off-nominal operational \nscenarios and nominal operational scenarios should \nbe utilized. Off-nominal testing offers insight into a \nsystem\u2019s total performance characteristics and often \nassists in identification of design issues and human-\nmachine interface, training, and procedural changes \nrequired to meet the mission goals and objectives. \nOff-nominal testing, as well as nominal testing, should \nbe included when planning for validation.\n\n\n\n102 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nThe validation environment is then prepared (set up the \nequipments, sensors, recording devices, etc., that will be \ninvolved in the validation conduct) and the validation \nprocedures reviewed to identify and resolve any issues \nimpacting validation.\n\nOutcomes of validation preparation include the following:\nPreparation for doing the planned validation is com- ?\npleted;\nAppropriate set of stakeholder expectations are avail- ?\nable and on hand;\nArticles or models to be used for validation with the  ?\nvalidation product and enabling products are inte-\ngrated within the validation environment according \nto plans and schedules;\nResources are available according to validation plans  ?\nand schedules; and\nThe validation environment is evaluated for adequacy,  ?\ncompleteness, readiness, and integration.\n\nConduct Planned Product Validation \nThe act of validating the end product is conducted as \nspelled out in the validation plans and procedures and \nconformance established to each specified validation re-\nquirement. The responsible engineer should ensure that \n\nthe procedures were followed and performed as planned, \nthe validation-enabling products were calibrated cor-\nrectly, and the data were collected and recorded for re-\nquired validation measures. \n\nWhen poor validation conduct, design, or conditions \ncause anomalies, the validation should be replanned as \nnecessary, the environment preparation anomalies cor-\nrected, and the validation conducted again with im-\nproved or correct procedures and resources. The Deci-\nsion Analysis Process should be used to make decisions \nfor issues identified that may require alternative choices \nto be evaluated and a selection made or when needed \nchanges to the validation plans, environment, and/or \nconduct are required.\n\nOutcomes of conducting validation include the following:\nA validated product is established with supporting  ?\nconfirmation that the appropriate results were col-\nlected and evaluated to show completion of validation \nobjectives. \nA determination is made as to whether the fabricated/ ?\nmanufactured or assembled and integrated products \n(including software or firmware builds, as applicable) \ncomply with their respective stakeholder expecta-\ntions.\nA determination is made that the validated product  ?\nwas appropriately integrated with the validation en-\nvironment and the selected stakeholder expectations \nset was properly validated. \nA determination is made that the product being vali- ?\ndated functions together with interfacing products \nthroughout their performance envelopes.\n\nAnalyze Product Validation Results\n\nOnce the validation activities have been completed, the \nresults are collected and the data are analyzed to confirm \nthat the end product provided will supply the customer\u2019s \nneeded capabilities within the intended environments of \nuse, validation procedures were followed, and enabling \nproducts and supporting resources functioned correctly. \nThe data are also analyzed for quality, integrity, correct-\nness, consistency, and validity and any unsuitable prod-\nucts or product attributes are identified and reported. \n\nIt is important to compare the actual validation results to \nthe expected results and to conduct any required system \ndesign and product realization process activities to re-\nsolve deficiencies. The deficiencies, along with recom-\n\nExamples of Enabling Products and Support \nResources for Preparing to Conduct \n\nValidation\n\nOne of the key tasks in the Product Validation Process \n\u201cprepare for conducting validation\u201d is to obtain neces-\nsary enabling products and support resources needed \nto conduct validation. Examples of these include:\n\nMeasurement tools (scopes, electronic devices,  ?\nprobes);\n\nEmbedded test software; ?\n\nTest wiring, measurement devices, and telemetry  ?\nequipment;\n\nRecording equipment (to capture test results); ?\n\nEnd products in the loop (software, electronics, or  ?\nmechanics) for hardware-in-the-loop simulations;\n\nExternal interfacing products of other systems; ?\n\nActual external interfacing products of other sys- ?\ntems (aircraft, vehicles, humans); and\n\nFacilities and skilled operators. ?\n\n\n\n5.4 Product Validation\n\nNASA Systems Engineering Handbook ? 103\n\nmended corrective actions and resolution results, should \nbe recorded and validation repeated, as required.\n\nOutcomes of analyzing validation results include the fol-\nlowing:\n\nProduct deficiencies and/or issues are identified.  ?\nAssurances that appropriate replanning, redefinition  ?\nof requirements, design, and revalidation have been \naccomplished for resolution of anomalies, variations, \nor out-of-compliance conditions (for problems not \ncaused by poor validation conduct). \nDiscrepancy and corrective action reports are gener- ?\nated as needed. \nThe validation report is completed.  ?\n\nValidation Notes \n\nThe types of validation used are dependent on the life-\ncycle phase; the product\u2019s location in the system struc-\nture; and cost, schedule, and resources available. Valida-\ntion of products within a single system model may be \nconducted together (e.g., an end product with its relevant \nenabling products, such as operational (control center or a \nradar with its display), maintenance (required tools work \nwith product), or logistical (launcher or transporter).\n\nEach realized product of system structure should be vali-\ndated against stakeholder expectations before being inte-\ngrated into a higher level product. \n\nReengineering\n\nBased on the results of the Product Validation Process, \nit could become necessary to reengineer a deficient end \nproduct. Care should be taken that correcting a deficiency, \nor set of deficiencies, does not generate a new issue with \na part or performance that had previously operated sat-\nisfactorily. Regression testing, a formal process of rerun-\nning previously used acceptance tests primarily used for \nsoftware, is one method to ensure a change did not affect \nfunction or performance that was previously accepted. \n\nValidation Deficiencies\n\nValidation outcomes can be unsatisfactory for several \nreasons. One reason is poor conduct of the validation \n(e.g., enabling products and supporting resources miss-\ning or not functioning correctly, untrained operators, \nprocedures not followed, equipment not calibrated, or \nimproper validation environmental conditions) and fail-\nure to control other variables not involved in validating \n\na set of stakeholder expectations. A second reason could \nbe a shortfall in the verification process of the end prod-\nuct. This could create the need for: \n\nReengineering end products lower in the system struc- ?\nture that make up the end product that was found to \nbe deficient (which failed to satisfy validation require-\nments) and/or\nReperforming any needed verification and validation  ?\nprocesses.\n\nOther reasons for validation deficiencies (particularly \nwhen M&S are involved) may be incorrect and/or inap-\npropriate initial or boundary conditions; poor formula-\ntion of the modeled equations or behaviors; the impact of \napproximations within the modeled equations or behav-\niors; failure to provide the required geometric and physics \nfidelities needed for credible simulations for the intended \npurpose; referent for comparison of poor or unknown un-\ncertainty quantification quality; and/or poor spatial, tem-\nporal, and perhaps, statistical resolution of physical phe-\nnomena used in M&S.\n\nNote: Care should be exercised to ensure that the cor-\nrective actions identified to remove validation de-\nficiencies do not conflict with the baselined stake-\nholder expectations without first coordinating such \nchanges with the appropriate stakeholders.\n\nCapture Product Validation Work Products\nValidation work products (inputs to the Technical Data \nManagement Process) take many forms and involve \nmany sources of information. The capture and recording \nof validation-related data is a very important, but often un-\nderemphasized, step in the Product Validation Process.\n\nValidation results, deficiencies identified, and corrective \nactions taken should be captured, as should all relevant \nresults from the application of the Product Validation \nProcess (related decisions, rationale for decisions made, \nassumptions, and lessons learned).\n\nOutcomes of capturing validation work products include \nthe following:\n\nWork products and related information generated  ?\nwhile doing Product Validation Process activities and \ntasks are recorded; i.e., type of validation conducted, \nthe form of the end product used for validation, val-\n\n\n\n104 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nidation procedures used, validation environments, \noutcomes, decisions, assumptions, corrective actions, \nlessons learned, etc. (often captured in a matrix or \nother tool\u2014see Appendix E).\nDeficiencies (e.g., variations and anomalies and  ?\nout-of-compliance conditions) are identified and \ndocumented, including the actions taken to re-\nsolve.\nProof is provided that the realized product is in con- ?\nformance with the stakeholder expectation set used in \nthe validation. \nValidation report including: ?\n\nRecorded validation results/data; ?\nVersion of the set of stakeholder expectations used; ?\nVersion and form of the end product validated; ?\nVersion or standard for tools and equipment used,  ?\ntogether with applicable calibration data;\nOutcome of each validation including pass or fail  ?\ndeclarations; and\nDiscrepancy between expected and actual results. ?\n\n5.4.1.3 Outputs\nKey outputs of validation are:\n\nValidated product, ?\nDiscrepancy reports and identified corrective actions,  ?\nand\nValidation reports. ?\n\nSuccess criteria for this process include: (1) objective ev-\nidence of performance and the results of each system-\nof-interest validation activity are documented, and (2) \nthe validation process should not be considered or des-\nignated as complete until all issues and actions are re-\nsolved.\n\n5.4.2 Product Validation Guidance\nThe following is some generic guidance for the Product \nValidation Process.\n\n5.4.2.1 Modeling and Simulation\nAs stressed in the verification process material, M&S is \nalso an important validation tool. M&S usage consider-\nations involve the verification, validation, and certifica-\ntion of the models and simulations.\n\n5.4.2.2 Software\n\nSoftware verification is a software engineering activity \nthat demonstrates the software products meet specified \nrequirements. Methods of software verification include \npeer reviews/inspections of software engineering prod-\nucts for discovery of defects, software verification of re-\nquirements by use of simulations, black box and white \nbox testing techniques, analyses of requirement imple-\nmentation, and software product demonstrations.\n\nSoftware validation is a software engineering activity \nthat demonstrates the as-built software product or soft-\nware product component satisfies its intended use in its \nintended environment. Methods of software validation \ninclude: peer reviews/inspections of software product \ncomponent behavior in a simulated environment, ac-\nceptance testing against mathematical models, analyses, \nand operational environment demonstrations. The proj-\nect\u2019s approach for software verification and validation is \ndocumented in the software development plan. Specific \nAgency-level requirements for software verification and \nvalidation, peer reviews (see Appendix N), testing and \nreporting are contained in NPR 7150.2, NASA Software \nRequirements.\n\nNote: For systems where only a single deliverable item \nis developed, the Product Validation Process normally \ncompletes acceptance testing of the system. How-\never, for systems with several production units, it is \nimportant to understand that continuing verification \nand validation is not an appropriate approach to use \nfor the items following the first deliverable. Instead, \nacceptance testing is the preferred means to ensure \nthat subsequent deliverables comply with the base-\nlined design.\n\nModel Verification and Validation\n\nModel Verification: ?  Degree to which a model ac-\ncurately meets its specifications. Answers \u201cIs it what \nI intended?\u201d \n\nModel Validation: ?  The process of determining the \ndegree to which a model is an accurate representa-\ntion of the real world from the perspective of the in-\ntended uses of the model.\n\nModel Certification: ?  Certification for use for a specific \npurpose. Answers, \u201cShould I endorse this model?\u201d\n\n\n\n5.4 Product Validation\n\nNASA Systems Engineering Handbook ? 105\n\nThe rigor and techniques used to verify and validate \nsoftware depend upon software classifications (which \nare different from project and payload classifications). A \ncomplex project will typically contain multiple systems \nand subsystems having different software classifications. \nIt is important for the project to classify its software and \nplan verification and validation approaches that appro-\npriately address the risks associated with each class.\n\nIn some instances, NASA management may select \na project for additional independent software veri-\nfication and validation by the NASA Software In-\ndependent Verification and Validation (IV&V) Fa-\ncility in Fairmount, West Virginia. In this case a \nMemorandum of Understanding (MOU) and sepa-\nrate software IV&V plan will be created and imple-\nmented.\n\n\n\n106 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nThe Product Transition Process is used to transition a \nverified and validated end product that has been gener-\nated by product implementation or product integration \nto the customer at the next level in the system structure \nfor integration into an end product or, for the top-level \nend product, transitioned to the intended end user. The \nform of the product transitioned will be a function of the \nproduct-line life-cycle phase success criteria and the lo-\ncation within the system structure of the WBS model in \nwhich the end product exits. \n\nProduct transition occurs during all phases of the life \ncycle. During the early phases, the technical team\u2019s prod-\nucts are documents, models, studies, and reports. As the \nproject moves through the life cycle, these paper or soft \nproducts are transformed through implementation and \nintegration processes into hardware and software solu-\ntions to meet the stakeholder expectations. They are re-\npeated with different degrees of rigor throughout the life \ncycle. The Product Transition Process includes product \ntransitions from one level of the system architecture up-\nward. The Product Transition Process is the last of the \nproduct realization pro-\ncesses, and it is a bridge \nfrom one level of the system \nto the next higher level.\n\nThe Product Transition Pro-\ncess is the key to bridge from \none activity, subsystem, or \nelement to the overall engi-\nneered system. As the system \ndevelopment nears comple-\ntion, the Product Transition \nProcess is again applied for \nthe end product, but with \nmuch more rigor since now \nthe transition objective is \ndelivery of the system-level \nend product to the actual \nend user. Depending on the \nkind or category of system \ndeveloped, this may involve \na Center or the Agency and \nimpact thousands of indi-\nviduals storing, handling, \nand transporting multiple \n\n5.5 Product Transition \n\nend products; preparing user sites; training operators \nand maintenance personnel; and installing and sus-\ntaining, as applicable. Examples are transitioning the \nexternal tank, solid rocket boosters, and orbiter to Ken-\nnedy Space Center (KSC) for integration and flight.\n\n5.5.1 Process Description\nFigure 5.5-1 provides a typical flow diagram for the \nProduct Transition Process and identifies typical inputs, \noutputs, and activities to consider in addressing product \ntransition.\n\n5.5.1.1 Inputs\nInputs to the Product Transition Process depend primari-\nly on the transition requirements, the product that is being \ntransitioned, the form of the product transition that is tak-\ning place, and where the product is transitioning to. Typi-\ncal inputs are shown in Figure 5.5-1 and described below.\n\nThe End Product or Products To Be Transitioned  ?\n(from Product Validation Process): The product to \nbe transitioned can take several forms. It can be a sub-\n\nFigure 5.5?1 Product Transition Process\n\nTo Product\nImplementation,\n\nIntegration, Veri?cation,\nValidation, and \n\nTransition Processes   \n\nProduct Transition\nWork Products\n\nEnd Product to Be\nTransitioned \n\nDocumentation to\nAccompany the\n\nDelivered End Product\n\nProduct Transition\u2013\nEnabling Products\n\nTo Technical Data\nManagement Process\n\nFrom Product\nValidation Process \n\nFrom Technical Data\nManagement Process  \n\nFrom existing\nresources or Product\nTransition Process  \n\nDelivered End Product\nWith Applicable\nDocumentation\n\nRealized Enabling\nProducts  \n\nPrepare the end product for transition \n\nPrepare sites, as required, where the\nend product will be stored, assembled,\n\nintegrated, installed, used, and/or\nmaintained\n\nEvaluate the end product, personnel,\nand enabling product readiness for\n\nproduct transition \n\nCapture product implementation\nwork products \n\nPrepare to conduct product\ntransition\n\nTransition the end product to the \ncustomer with required documentation\nbased on the type of transition required\n\nTo end user or Product\nIntegration Process \n\n(recursive loop)\n\n\n\n5.5 Product Transition\n\nNASA Systems Engineering Handbook ? 107\n\nsystem component, system assembly, or top-level end \nproduct. It can be hardware or software. It can be newly \nbuilt, purchased, or reused. A product can transition \nfrom a lower system product to a higher one by being \nintegrated with other transitioned products. This pro-\ncess may be repeated until the final end product is \nachieved. Each succeeding transition requires unique \ninput considerations when preparing for the validated \nproduct for transition to the next level. \n\n Early phase products can take the form of informa-\ntion or data generated from basic or applied research \nusing analytical or physical models and often are in \npaper or electronic form. In fact, the end product for \nmany NASA research projects or science activities is a \nreport, paper, or even an oral presentation. In a sense, \nthe dissemination of information gathered through \nNASA research and development is an important \nform of product transition.\nDocumentation Including Manuals, Procedures,  ?\nand Processes That Are To Accompany the End \nProduct (from Technical Data Management Pro-\ncess): The documentation required for the Product \nTransition Process depends on the specific end \nproduct; its current location within the system struc-\nture; and the requirements identified in various agree-\nments, plans, or requirements documents. Typically, \na product has a unique identification (i.e., serial \nnumber) and may have a pedigree (documentation) \nthat specifies its heritage and current state. Pertinent \ninformation may be documented through a configu-\nration management system or work order system as \nwell as design drawings and test reports. Documen-\ntation often includes proof of verification and valida-\ntion conformance. A COTS product would typically \ncontain a manufacturer\u2019s specification or fact sheet. \nDocumentation may include operations manuals, in-\nstallation instructions, and other information.\n\n The documentation level of detail is dependent upon \nwhere the product is within the product hierarchy \nand the life cycle. Early in the life cycle, this docu-\nmentation may be preliminary in nature. Later in the \nlife cycle, the documentation may be detailed design \ndocuments, user manuals, drawings, or other work \nproducts. Documentation that is gathered during \nthe input process for the transition phase may re-\nquire editing, assembling, or repackaging to ensure \nit is in the required condition for acceptance by the \ncustomer.\n\n Special consideration must be given to safety, in-\ncluding clearly identifiable tags and markings that \nidentify the use of hazardous materials, special han-\ndling instructions, and storage requirements. \nProduct-Transition-Enabling Products, Including  ?\nPackaging Materials; Containers; Handling Equip-\nment; and Storage, Receiving, and Shipping Facili-\nties (from Existing Resources or Product Transition \nProcess for Enabling Product Realization): Product-\ntransition-enabling products may be required to fa-\ncilitate the implementation, integration, evaluation, \ntransition, training, operations, support, and/or retire-\nment of the transition product at its next higher level \nor for the transition of the final end product. Some or \nall of the enabling products may be defined in transi-\ntion-related agreements, system requirements docu-\nments, or project plans. In some cases, product-tran-\nsition-enabling products are developed during the \nrealization of the product itself or may be required to \nbe developed during the transition stage. \n\n As a product is developed, special containers, holders, \nor other devices may also be developed to aid in the \nstoring and transporting of the product through de-\nvelopment and realization. These may be temporary \naccommodations that do not satisfy all the transition \nrequirements, but allow the product to be initiated \ninto the transition process. In such cases, the tempo-\nrary accommodations will have to be modified or new \naccommodations will need to be designed and built \nor procured to meet specific transportation, handling, \nstorage, and shipping requirements. \n\n Sensitive or hazardous products may require special \nenabling products such as monitoring equipment, \ninspection devices, safety devices, and personnel \ntraining to ensure adequate safety and environmental \nrequirements are achieved and maintained.\n\n5.5.1.2 Process Activities\nTransitioning the product can take one of two forms:\n\nThe delivery of lower system end products to higher  ?\nones for integration into another end product or\nThe delivery of the final end product to the customer  ?\nor user that will use it in its operational environment.\n\nIn the first case, the end product is one of perhaps several \nother pieces that will ultimately be integrated together to \nform the item in the second case for final delivery to the \ncustomer. For example, the end product might be one of \n\n\n\n108 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nseveral circuit cards that will be integrated together to \nform the final unit that is delivered. Or that unit might \nalso be one of several units that have to be integrated to-\ngether to form the final product. \n\nThe form of the product transitioned is not only a func-\ntion of the location of that product within the system \nproduct hierarchy (i.e., WBS model), but also a func-\ntion of the life-cycle phase. Early life-cycle phase prod-\nucts may be in the form of paper, electronic files, phys-\nical models, or technology demonstration prototypes. \nLater phase products may be preproduction prototypes \n(engineering models), the final study report, or the flight \nunits.\n\nFigure 5.5-1 shows what kind of inputs, outputs, and ac-\ntivities are performed during product transition regard-\nless of where in the product hierarchy or life cycle the \nproduct is. These activities include preparing to conduct \nthe transition; making sure the end product, all per-\nsonnel, and any enabling products are ready for tran-\nsitioning; preparing the site; and performing the tran-\nsition including capturing and documenting all work \nproducts.\n\nHow these activities are performed and what form the \ndocumentation takes will depend on where the end \nitems are in the product hierarchy (WBS model) and its \nlife-cycle phase.\n\nPrepare to Implement Transition\n\nThe first task is to identify which of the two forms of \ntransition is needed: (1) the delivery of lower system end \nproducts to higher ones for integration into another end \nproduct or (2) the delivery of the final end product to \nthe customer or user that will use the end product in its \noperational environment. The form of the product being \ntransitioned will affect transition planning and the kind \nof packaging, handling, storage, and transportation that \nwill be required. The customer and other stakeholder ex-\npectations, as well as the specific design solution, may in-\ndicate special transition procedures or enabling product \nneeds for packaging, storage, handling, shipping/trans-\nporting, site preparation, installation, and/or sustain-\nability. These requirements need to be reviewed during \nthe preparation stage.\n\nOther tasks in preparing to transition a product involve \nmaking sure the end product, personnel, and any en-\nabling products are ready for that transition. This in-\n\ncludes the availability of the documentation that will be \nsent with the end product, including proof of verifica-\ntion and validation conformance. The appropriateness \nof detail for that documentation depends upon where \nthe product is within the product hierarchy and the life \ncycle. Early in the life cycle, this documentation may be \npreliminary in nature. Later in the life cycle, the docu-\nmentation may be detailed design documents, user man-\nuals, drawings, or other work products. Procedures nec-\nessary for conducting the transition should be reviewed \nand approved by this time. This includes all necessary \napprovals by management, legal, safety, quality, property, \nor other organizations as identified in the SEMP.\n\nFinally, the availability and skills of personnel needed to \nconduct the transition as well as the availability of any \nnecessary packaging materials/containers, handling \nequipment, storage facilities, and shipping/transporter \nservices should also be reviewed. Any special training \nnecessary for the personnel to perform their tasks needs \nto be performed by this time.\n\nPrepare the Product for Transition\n\nWhether transitioning a product to the next room for \nintegration into the next higher assembly, or for final \ntransportation across the country to the customer, care \nmust be taken to ensure the safe transportation of the \nproduct. The requirements for packaging, handling, \nstorage, and transportation should have been identified \nduring system design. Preparing for the packaging for \nprotection, security, and prevention of deterioration is \ncritical for products placed in storage or when it is nec-\nessary to transport or ship between and within organi-\nzational facilities or between organizations by land, air, \nand/or water vehicles. Particular emphasis needs to be \non protecting surfaces from physical damage, preventing \ncorrosion, eliminating damage to electronic wiring or \ncabling, shock or stress damage, heat warping or cold \nfractures, moisture, and other particulate intrusion that \ncould damage moving parts.\n\nThe design requirements should have already addressed \nthe ease of handling or transporting the product such as \ncomponent staking, addition of transportation hooks, \ncrating, etc. The ease and safety of packing and un-\npacking the product should also have been addressed. \nAdditional measures may also need to be implemented \nto show accountability and to securely track the product \nduring transportation. In cases where hazardous mate-\n\n\n\n5.5 Product Transition\n\nNASA Systems Engineering Handbook ? 109\n\nrials are involved, special labeling or handling needs in-\ncluding transportation routes need to be in place.\n\nPrepare the Site to Receive the Product\n\nFor either of the forms of product transition, the re-\nceiving site needs to be prepared to receive the product. \nHere the end product will be stored, assembled, inte-\ngrated, installed, used, and/or maintained, as appropriate \nfor the life-cycle phase, position of the end product in \nthe system structure, and customer agreement. \n\nA vast number of key complex activities, many of them \noutside direct control of the technical team, have to be \nsynchronized to ensure smooth transition to the end \nuser. If transition activities are not carefully controlled, \nthere can be impacts on schedule, cost, and safety of the \nend product.\n\nA site survey may need to be performed to determine \nthe issues and needs. This should address the adequacy \nof existing facilities to accept, store, and operate the new \nend product and identify any logistical-support-en-\nabling products and services required but not planned \nfor. Additionally, any modifications to existing facilities \nmust be planned well in advance of fielding; therefore, \nthe site survey should be made during an early phase in \nthe product life cycle. These may include logistical en-\nabling products and services to provide support for end-\nproduct use, operations, maintenance, and disposal. \nTraining for users, operators, maintainers, and other \nsupport personnel may need to be conducted. National \nEnvironmental Policy Act documentation or approvals \nmay need to be obtained prior to the receipt of the end \nproduct.\n\nPrior to shipment or after receipt, the end product may \nneed to be stored in suitable storage conditions to pro-\ntect and secure the product and prevent damage or the \ndeterioration of it. These conditions should have been \nidentified early in the design life cycle. \n\nTransition the Product\n\nThe end product is then transitioned (i.e., moved, trans-\nported, or shipped) with required documentation to the \ncustomer based on the type of transition required, e.g., \nto the next higher level item in the Product Breakdown \nStructure (PBS) for product integration or to the end \nuser. Documentation may include operations manuals, \ninstallation instructions, and other information.\n\nThe end product is finally installed into the next higher \nassembly or into the customer/user site using the preap-\nproved installation procedures.\n\nConfirm Ready to Support\n\nAfter installation, whether into the next higher assembly \nor into the final customer site, functional and acceptance \ntesting of the end product should be conducted. This en-\nsures no damage from the shipping/handling process \nhas occurred and that the product is ready for support. \nAny final transitional work products should be captured \nas well as documentation of product acceptance.\n\n5.5.1.3 Outputs\nDelivered End Product for Integration to Next Level  ?\nup in System Structure: This includes the appropriate \ndocumentation. The form of the end product and ap-\nplicable documentation are a function of the life-cycle \nphase and the placement within the system structure. \n(The form of the end product could be hardware, soft-\nware, model, prototype, first article for test, or single \noperational article or multiple production article.) \nDocumentation includes applicable draft installation, \noperation, user, maintenance, or training manuals; \napplicable baseline documents (configuration base-\nline, specifications, and stakeholder expectations); \nand test results that reflect completion of verification \nand validation of the end product.\nDelivered Operational End Product for End Users: ?  \nThe appropriate documentation is to be delivered with \nthe delivered end product as well as the operational \nend product appropriately packaged. Documentation \nincludes applicable final installation, operation, user, \nmaintenance, or training manuals; applicable base-\nline documents (configuration baseline, specifications, \nstakeholder expectations); and test results that reflect \ncompletion of verification and validation of the end \nproduct. If the end user will perform end product vali-\ndation, sufficient documentation to support end user \nvalidation activities is delivered with the end product.\nWork Products from Transition Activities to Tech- ?\nnical Data Management: Work products could in-\nclude the transition plan, site surveys, measures, \ntraining modules, procedures, decisions, lessons \nlearned, corrective actions, etc.\nRealized Enabling End Products to Appropriate  ?\nLife-Cycle Support Organization: Some of the en-\nabling products that were developed during the var-\n\n\n\n110 ? NASA Systems Engineering Handbook\n\n5.0 Product Realization\n\nious phases could include fabrication or integration \nspecialized machines; tools; jigs; fabrication processes \nand manuals; integration processes and manuals; \nspecialized inspection, analysis, demonstration, or \ntest equipment; tools; test stands; specialized pack-\naging materials and containers; handling equipment; \nstorage-site environments; shipping or transporta-\ntion vehicles or equipment; specialized courseware; \ninstructional site environments; and delivery of the \ntraining instruction. For the later life-cycle phases, \nenabling products that are to be delivered may include \nspecialized mission control equipment; data collec-\ntion equipment; data analysis equipment; operations \nmanuals; specialized maintenance equipment, tools, \nmanuals, and spare parts; specialized recovery equip-\nment; disposal equipment; and readying recovery or \ndisposal site environments.\n\nThe process is complete when the following activities \nhave been accomplished:\n\nThe end product is validated against stakeholder ex- ?\npectations unless the validation is to be done by the \nintegrator before integration is accomplished.\nFor deliveries to the integration path, the end product is  ?\ndelivered to intended usage sites in a condition suitable \nfor integration with other end products or composites \nof end products. Procedures, decisions, assumptions, \nanomalies, corrective actions, lessons learned, etc., re-\nsulting from transition for integration are recorded.\nFor delivery to the end user path, the end products  ?\nare installed at the appropriate sites; appropriate ac-\nceptance and certification activities are completed; \ntraining of users, operators, maintainers, and other nec-\nessary personnel is completed; and delivery is closed \nout with appropriate acceptance documentation.\nAny realized enabling end products are also delivered as  ?\nappropriate including procedures, decisions, assump-\ntions, anomalies, corrective actions, lessons learned, \netc., resulting from transition-enabling products.\n\n5.5.2 Product Transition Guidance\n\n5.5.2.1 Additional Product Transition Input \nConsiderations\n\nIt is important to consider all customer, stakehold-\ner, technical, programmatic, and safety requirements \n\nwhen evaluating the input necessary to achieve a suc-\ncessful Product Transition Process. This includes the \nfollowing:\n\nTransportability Requirements: ?  If applicable, re-\nquirements in this section define the required con-\nfiguration of the system of interest for transport. Fur-\nther, this section details the external systems (and the \ninterfaces to those systems) required for transport of \nthe system of interest.\nEnvironmental Requirements:  ? Requirements in this \nsection define the environmental conditions in which \nthe system of interest is required to be during transi-\ntion (including storage and transportation).\nMaintainability Requirements:  ? Requirements in this \nsection detail how frequently, by whom, and by what \nmeans the system of interest will require maintenance \n(also any \u201ccare and feeding,\u201d if required).\nSafety Requirements:  ? Requirements in this sec-\ntion define the life-cycle safety requirements for the \nsystem of interest and associated equipment, facilities, \nand personnel.\nSecurity Requirements:  ? This section defines the In-\nformation Technology (IT) requirements, Federal and \ninternational export and security requirements, and \nphysical security requirements for the system of in-\nterest.\nProgrammatic Requirements: ?  Requirements in this \nsection define cost and schedule requirements. \n\n5.5.2.2 After Product Transition to the End \nUser\u2014What Next?\n\nAs mentioned in Chapter 2.0, there is a relationship be-\ntween the SE engine and the activities performed after \nthe product is transitioned to the end user. As shown in \nFigure 2.3-8, after the final deployment to the end user, \nthe end product is operated, managed, and maintained \nthrough sustaining engineering functions. The tech-\nnical management processes described in Section 6.0 are \nused during these activities. If at any time a new capa-\nbility, upgrade, or enabling product is needed, the devel-\nopmental processes of the engine are reengaged. When \nthe end product\u2019s use is completed, the plans developed \nearly in the life cycle to dispose, retire, or phase out the \nproduct are enacted.\n\n\n\nNASA Systems Engineering Handbook ? 111\n\n6.0 Crosscutting Technical Management\n\nThis chapter describes the activities in the technical man-\nagement processes listed in Figure 2.1-1. The chapter \nis separated into sections corresponding to steps 10 \nthrough 17 listed in Figure 2.1-1. The processes within \neach step are discussed in terms of the inputs, the activ-\nities, and the outputs. Additional guidance is provided \nusing examples that are relevant to NASA projects.\n\nThe technical management processes are the bridges be-\ntween project management and the technical team. In \nthis portion of the engine, eight crosscutting processes \nprovide the integration of the crosscutting functions that \nallow the design solution to be realized. Even though \nevery technical team member might not be directly in-\nvolved with these eight processes, they are indirectly af-\nfected by these key functions. Every member of the tech-\nnical team relies on technical planning; management \nof requirements, interfaces, technical risk, configura-\ntion, and technical data; technical assessment; and de-\ncision analysis to meet the project\u2019s objectives. Without \nthese crosscutting processes, individual members and \ntasks cannot be integrated into a functioning system that \nmeets the ConOps within cost and schedule. The project \nmanagement team also uses these crosscutting functions \nto execute project control on the apportioned tasks.\n\nThis effort starts with the technical team conducting ex-\ntensive planning early in Pre-Phase A. With this early, \ndetailed baseline plan, technical team members will \nunderstand the roles and responsibilities of each team \nmember, and the project can establish its program cost \nand schedule goals and objectives. From this effort, the \nSystems Engineering Management Plan (SEMP) is devel-\noped and baselined. Once a SEMP has been established, \nit must be synchronized with the project master plans \nand schedule. In addition, the plans for establishing and \nexecuting all technical contracting efforts are identified.\n\nThis is a recursive and iterative process. Early in the life \ncycle, the plans are established and synchronized to run \nthe design and realization processes. As the system ma-\ntures and progresses through the life cycle, these plans \nmust be updated as necessary to reflect the current en-\n\nvironment and resources and to control the project\u2019s \nperformance, cost, and schedule. At a minimum, these \nupdates will occur at every Key Decision Point (KDP). \nHowever, if there is a significant change in the project, \nsuch as new stakeholder expectations, resource adjust-\nments, or other constraints, all plans must be analyzed \nfor the impact of these changes to the baselined project.\n\nThe next sections describe each of the eight technical \nmanagement processes and their associated products for \na given NASA mission. \n\nCrosscutting Technical Management Keys\n\nThoroughly understand and plan the scope of the  ?\ntechnical effort by investing time upfront to de-\nvelop the technical product breakdown structure, \nthe technical schedule and workflow diagrams, \nand the technical resource requirements and con-\nstraints (funding, budget, facilities, and long-lead \nitems) that will be the technical planning infra-\nstructure.\n\nDefine all interfaces and assign interface author- ?\nities and responsibilities to each, both intra- and  \ninterorganizational. This includes understanding \npotential incompatibilities and defining the transi-\ntion processes.\n\nControl of the configuration is critical to under- ?\nstanding how changes will impact the system. \nFor example, changes in design and environment \ncould invalidate previous analysis results.\n\nConduct milestone reviews to enable a critical and  ?\nvaluable assessment to be performed. These re-\nviews are not to be used to meet contractual or \nscheduling incentives. These reviews have specific \nentrance criteria and should be conducted when \nthese are met. \n\nUnderstand any biases, assumptions, and con- ?\nstraints that impact the analysis results.\n\nPlace all analysis under configuration control to be  ?\nable to track the impact of changes and understand \nwhen the analysis needs to be reevaluated. \n\n\n\n112 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\n6.1 Technical Planning\n\nThe Technical Planning Process, the first of the eight \ntechnical management processes contained in the sys-\ntems engineering engine, establishes a plan for applying \nand managing each of the common technical processes \nthat will be used to drive the development of system \nproducts and associated work products. This process also \nestablishes a plan for identifying and defining the tech-\nnical effort required to satisfy the project objectives and \nlife-cycle phase success criteria within the cost, schedule, \nand risk constraints of the project. \n\n6.1.1 Process Description\nFigure 6.1-1 provides a typical flow diagram for the Tech-\nnical Planning Process and identifies typical inputs, out-\nputs, and activities to consider in addressing technical \nplanning.\n\n6.1.1.1 Inputs\nInput to the Technical Planning Process comes from \nboth the project management and technical teams as \n\noutputs from the other common technical processes. Ini-\ntial planning utilizing external inputs from the project to \ndetermine the general scope and framework of the tech-\nnical effort will be based on known technical and pro-\ngrammatic requirements, constraints, policies, and pro-\ncesses. Throughout the project\u2019s life cycle, the technical \nteam continually incorporates results into the technical \nplanning strategy and documentation and any internal \nchanges based on decisions and assessments generated \nby the other processes of the SE engine or from require-\nments and constraints mandated by the project. \n\nAs the project progresses through the life-cycle phases, \ntechnical planning for each subsequent phase must be \nassessed and continually updated. When a project tran-\nsitions from one life-cycle phase to the next, the techni-\ncal planning for the upcoming phase must be assessed \nand updated to reflect the most recent project data.\n\nExternal Inputs from the Project:  ? The project plan \nprovides the project\u2019s top-level technical require-\nments, the available budget allocated to the project \n\nProject Technical Effort\nRequirements and Project\n\nResource Constraints\n\nTo Technical Data\nManagement Process\n\nFrom project \n\nFrom Technical Data\nManagement Process\n\nTo Technical\nAssessment Process\n\nTo project \n\nTo applicable technical\nprocesses\n\nTo applicable technical teams\n\nFrom Technical Assessment\nand Technical Risk\n\nManagement Processes\n\nPrepare to conduct technical\nplanning\n\nDefine the technical work \n\nSchedule, organize, and cost\nthe technical work \n\nPrepare SEMP and other\ntechnical plans\n\nObtain stakeholder commitments\nto technical plans\n\nIssue authorized technical\nwork directives\n\nCapture technical planning\nwork products\n\nAgreements, Capability\nNeeds, Applicable Product-\n\nLine Life-Cycle Phase\n\nApplicable Policies, \nProcedures, Standards, \n\nand Organizational \nProcesses\n\nPrior Phase or\nBaseline Plans\n\nReplanning Needs\n\nCost Estimates,\nSchedules, and Resource\n\nRequests\n\nProduct and Process\nMeasures\n\nSEMP and Other\nTechnical Plans\n\nTechnical Planning\nWork Products\n\nTechnical Work Directives\n\nFigure 6.1?1 Technical Planning Process\n\n\n\n6.1 Technical Planning\n\nNASA Systems Engineering Handbook ? 113\n\nfrom the program, and the desired schedule for the \nproject to support overall program needs. Although \nthe budget and schedule allocated to the project will \nserve as constraints on the project, the technical team \nwill generate a technical cost estimate and schedule \nbased on the actual work required to satisfy the proj-\nect\u2019s technical requirements. Discrepancies between \nthe project\u2019s allocated budget and schedule and the \ntechnical team\u2019s actual cost estimate and schedule \nmust be reconciled continuously throughout the proj-\nect\u2019s life cycle.\n\n The project plan also defines the applicable project \nlife-cycle phases and milestones, as well as any in-\nternal and external agreements or capability needs \nrequired for successful project execution. The proj-\nect\u2019s life-cycle phases and programmatic milestones \nwill provide the general framework for establishing \nthe technical planning effort and for generating the \ndetailed technical activities and products required to \nmeet the overall project milestones in each of the life-\ncycle phases.\n\n Finally, the project plan will include all programmatic \npolicies, procedures, standards, and organizational \nprocesses that must be adhered to during execution \nof the technical effort. The technical team must de-\nvelop a technical approach that ensures the project \nrequirements will be satisfied and that any technical \nprocedures, processes, and standards to be used in de-\nveloping the intermediate and final products comply \nwith the policies and processes mandated in the \nproject plan.\nInternal Inputs from Other Common Technical  ?\nProcesses: The latest technical plans (either baselined \nor from the previous life-cycle phase) from the Data \nManagement or Configuration Management Pro-\ncesses should be used in updating the technical plan-\nning for the upcoming life-cycle phase.\n\n Technical planning updates may be required based on \nresults from technical reviews conducted in the Tech-\nnical Assessment Process, issues identified during the \nTechnical Risk Management Process, or from deci-\nsions made during the Decision Analysis Process.\n\n6.1.1.2 Process Activities\nTechnical planning as it relates to systems engineering at \nNASA is intended to identify, define, and plan how the \n17 common technical processes in NPR 7123.1, NASA \nSystems Engineering Processes and Requirements will be \n\napplied in each life-cycle phase for all levels of the WBS \nmodel (see Subsection 6.1.2.1) within the system struc-\nture to meet product-line life-cycle phase success criteria. \nA key document generated by this process is the SEMP.\n\nThe SEMP is a subordinate document to the project \nplan. While the SEMP defines to all project participants \nhow the project will be technically managed within the \nconstraints established by the project, the project plan \ndefines how the project will be managed to achieve its \ngoals and objectives within defined programmatic con-\nstraints. The SEMP also communicates how the systems \nengineering management techniques will be applied \nthroughout all phases of the project life cycle.\n\nTechnical planning should be tightly integrated with the \nTechnical Risk Management Process (see Section 6.4) \nand the Technical Assessment Process (see Section 6.7) \nto ensure corrective action for future activities will be in-\ncorporated based on current issues identified within the \nproject.\n\nTechnical planning, as opposed to program or project \nplanning, addresses the scope of the technical effort re-\nquired to develop the system products. While the project \nmanager concentrates on managing the overall project \nlife cycle, the technical team, led by the systems engineer, \nconcentrates on managing the technical aspects of the \nproject. The technical team identifies, defines, and de-\nvelops plans for performing decomposition, definition, \nintegration, verification, and validation of the system \nwhile orchestrating and incorporating the appropriate \nconcurrent engineering. Additional planning will in-\nclude defining and planning for the appropriate tech-\nnical reviews, audits, assessments, and status reports and \ndetermining any specialty engineering and/or design \nverification requirements.\n\nThis section describes how to perform the activities \ncontained in the Technical Planning Process shown in \nFigure 6.1-1. The initial technical planning at the be-\nginning of the project will establish the technical team \nmembers; their roles and responsibilities; and the tools, \nprocesses, and resources that will be utilized in executing \nthe technical effort. In addition, the expected activities \nthe technical team will perform and the products it will \nproduce will be identified, defined, and scheduled. Tech-\nnical planning will continue to evolve as actual data from \ncompleted tasks are received and details of near-term \nand future activities are known. \n\n\n\n114 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nTechnical Planning Preparation\n\nFor technical planning to be conducted properly, the \nprocesses and procedures to conduct technical plan-\nning should be identified, defined, and communicated. \nAs participants are identified, their roles and responsi-\nbilities and any training and/or certification activities \nshould be clearly defined and communicated.\n\nOnce the processes, people, and roles and responsibili-\nties are in place, a planning strategy may be formulated \nfor the technical effort. A basic technical planning strat-\negy should address the following:\n\nThe level of planning documentation required for the  ?\nSEMP and all other technical planning documents;\nIdentifying and collecting input documentation; ?\nThe sequence of technical work to be conducted, in- ?\ncluding inputs and outputs;\nThe deliverable products from the technical work; ?\nHow to capture the work products of technical activi- ?\nties;\nHow technical risks will be identified and managed; ?\nThe tools, methods, and training needed to conduct  ?\nthe technical effort;\nThe involvement of stakeholders in each facet of the  ?\ntechnical effort;\nHow the NASA technical team will be involved with  ?\nthe technical efforts of external contractors;\nThe entry and success criteria for milestones, such as  ?\ntechnical reviews and life-cycle phases;\nThe identification, definition, and control of internal  ?\nand external interfaces; \nThe identification and incorporation of relevant les- ?\nsons learned into the technical planning;\nThe approach for technology development and how  ?\nthe resulting technology will be incorporated into the \nproject;\nThe identification and definition of the technical met- ?\nrics for measuring and tracking progress to the real-\nized product;\nThe criteria for make, buy, or reuse decisions and in- ?\ncorporation criteria for Commercial Off-the-Shelf \n(COTS) software and hardware;\nThe plan to identify and mitigate off-nominal perfor- ?\nmance;\nThe \u201chow-tos\u201d for contingency planning and replan- ?\nning;\n\nThe plan for status assessment and reporting; and ?\nThe approach to decision analysis, including materials  ?\nneeded, required skills, and expectations in terms of \naccuracy. \n\nBy addressing these items and others unique to the \nproject, the technical team will have a basis for under-\nstanding and defining the scope of the technical effort, \nincluding the deliverable products that the overall tech-\nnical effort will produce, the schedule and key milestones \nfor the project that the technical team must support, and \nthe resources required by the technical team to perform \nthe work.\n\nA key element in defining the technical planning effort is \nunderstanding the amount of work associated with per-\nforming the identified activities. Once the scope of the \ntechnical effort begins to coalesce, the technical team \nmay begin to define specific planning activities and to \nestimate the amount of effort and resources required to \nperform each task. Historically, many projects have un-\nderestimated the resources required to perform proper \nplanning activities and have been forced into a position \nof continuous crisis management in order to keep up \nwith changes in the project.\n\nDefine the Technical Work\n\nThe technical effort must be thoroughly defined. When \nperforming the technical planning, realistic values for \ncost, schedule, and labor resources should be used. \nWhether extrapolated from historical databases or from \ninteractive planning sessions with the project and stake-\nholders, realistic values must be calculated and provided \nto the project team. Contingency should be included in \nany estimate and based on complexity and criticality of \nthe effort. Contingency planning must be conducted. \nThe following are examples of contingency planning:\n\nAdditional, unplanned-for software engineering re- ?\nsources are typically needed during hardware and \nsystems development and testing to aid in trouble-\nshooting errors/anomalies. Frequently, software engi-\nneers are called upon to help troubleshoot problems \nand pinpoint the source of errors in hardware and sys-\ntems development and testing (e.g., for writing addi-\ntion test drivers to debug hardware problems). Addi-\ntional software staff should be planned into the project \ncontingencies to accommodate inevitable component \nand system debugging and avoid cost and schedule \noverruns.\n\n\n\n6.1 Technical Planning\n\nNASA Systems Engineering Handbook ? 115\n\nHardware-in-the-Loop (HWIL) must be accounted  ?\nfor in the technical planning contingencies. HWIL \ntesting is typically accomplished as a debugging ex-\nercise where the hardware and software are brought \ntogether for the first time in the costly environment of \nan HWIL. If upfront work is not done to understand \nthe messages and errors arising during this test, ad-\nditional time in the HWIL facility may result in sig-\nnificant cost and schedule impacts. Impacts may be \nmitigated through upfront planning, such as making \nappropriate debugging software available to the tech-\nnical team prior to the test, etc.\n\nSchedule, Organize, and Cost the Technical Effort\n\nOnce the technical team has defined the technical work \nto be done, efforts can focus on producing a schedule \nand cost estimate for the technical portion of the project. \nThe technical team must organize the technical tasks \naccording to the project WBS in a logical sequence of \nevents, taking into consideration the major project mile-\nstones, phasing of available funding, and timing of avail-\nability of supporting resources.\n\nScheduling\nProducts described in the WBS are the result of activi-\nties that take time to complete. These activities have time \nprecedence relationships among them that may used \nto create a network schedule explicitly defining the de-\npendencies of each activity on other activities, the avail-\nability of resources, and the receipt of receivables from \noutside sources. \n\nScheduling is an essential component of planning and \nmanaging the activities of a project. The process of cre-\nating a network schedule provides a standard method \nfor defining and communicating what needs to be done, \nhow long it will take, and how each element of the project \nWBS might affect other elements. A complete network \nschedule may be used to calculate how long it will take to \ncomplete a project; which activities determine that dura-\ntion (i.e., critical path activities); and how much spare \ntime (i.e., float) exists for all the other activities of the \nproject. \n\n\u201cCritical path\u201d is the sequence of dependent tasks that \ndetermines the longest duration of time needed to com-\nplete the project. These tasks drive the schedule and con-\ntinually change, so they must be updated. The critical \npath may encompass only one task or a series of inter-\n\nrelated tasks. It is important to identify the critical path \nand the resources needed to complete the critical tasks \nalong the path if the project is to be completed on time \nand within its resources. As the project progresses, the \ncritical path will change as the critical tasks are com-\npleted or as other tasks are delayed. This evolving critical \npath with its identified tasks needs to be carefully moni-\ntored during the progression of the project.\n\nNetwork scheduling systems help managers accurately \nassess the impact of both technical and resource changes \non the cost and schedule of a project. Cost and technical \nproblems often show up first as schedule problems. Un-\nderstanding the project\u2019s schedule is a prerequisite for \ndetermining an accurate project budget and for tracking \nperformance and progress. Because network schedules \nshow how each activity affects other activities, they assist \nin assessing and predicting the consequences of schedule \nslips or accelerations of an activity on the entire project.\n\nNetwork Schedule Data and Graphical \nFormats\n\nNetwork schedule data consist of:\nActivities and associated tasks; ?\nDependencies among activities (e.g., where an activity  ?\ndepends upon another activity for a receivable);\nProducts or milestones that occur as a result of one or  ?\nmore activities; and\nDuration of each activity. ?\n\nA network schedule contains all four of the above data \nitems. When creating a network schedule, creating \ngraphical formats of these data elements may be a useful \nfirst step in planning and organizing schedule data.\n\nWorkflow Diagrams\nA workflow diagram is a graphical display of the first \nthree data items. Two general types of graphical formats \nare used as shown in Figure 6.1-2. One places activities \non arrows, with products and dependencies at the begin-\nning and end of the arrow. This is the typical format of \nthe Program Evaluation and Review Technique (PERT) \nchart. \n\nThe second format, called precedence diagrams, uses \nboxes to represent activities; dependencies are then \nshown by arrows. The precedence diagram format al-\nlows for simple depiction of the following logical rela-\ntionships:\n\n\n\n116 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nActivity B begins when Activity A begins (start-start). ?\nActivity B begins only after Activity A ends (finish- ?\nstart).\nActivity B ends when Activity A ends (finish-finish). ?\n\nEach of these three activity relationships may be modified \nby attaching a lag (+ or \u2013) to the relationship, as shown \nin Figure 6.1-2. It is possible to summarize a number of \nlow-level activities in a precedence diagram with a single \nactivity. One takes the initial low-level activity and at-\ntaches a summary activity to it using the start-start rela-\ntionship described above. The summary activity is then \nattached to the final low-level activity using the finish-\nstart relationship. The most common relationship used \nin precedence diagrams is the finish-start one. The ac-\ntivity-on-arrow format can represent the identical time-\nprecedence logic as a precedence diagram by creating ar-\ntificial events and activities as needed.\n\nEstablishing a Network Schedule\nScheduling begins with project-level schedule objec-\ntives for delivering the products described in the upper \nlevels of the WBS. To develop network schedules that are \nconsistent with the project\u2019s objectives, the following six \n\nsteps are applied to each element at the lowest available \nlevel of the WBS.\n\nStep 1: Identify activities and dependencies needed to \ncomplete each WBS element. Enough activities should \nbe identified to show exact schedule dependencies be-\ntween activities and other WBS elements. This first step \nis most easily accomplished by:\n\nEnsuring that the WBS model is extended downward  ?\nto describe all significant products including docu-\nments, reports, and hardware and software items.\nFor each product, listing the steps required for its gen- ?\neration and drawing the process as a workflow dia-\ngram.\nIndicating the dependencies among the products, and  ?\nany integration and verification steps within the work \npackage.\n\nStep 2: Identify and negotiate external dependencies. Ex-\nternal dependencies are any receivables from outside of, \nand any deliverables that go outside of, the WBS element. \nNegotiations should occur to ensure that there is agree-\nment with respect to the content, format, and labeling of \nproducts that move across WBS elements so that lower \nlevel schedules can be integrated.\n\nStep 3: Estimate durations of all activities. Assumptions \nbehind these estimates (workforce, availability of facili-\nties, etc.) should be written down for future reference.\n\nStep 4: Enter the data for each WBS element into a sched-\nuling program to obtain a network schedule and an es-\ntimate of the critical path for that element. It is not un-\nusual at this point for some iteration of steps 1 to 4 to \nobtain a satisfactory schedule. Reserve is often added to \ncritical-path activities to ensure that schedule commit-\nments can be met within targeted risk levels.\n\nStep 5: Integrate schedules of lower level WBS elements \nso that all dependencies among elements are correctly \nincluded in a project network. It is important to include \nthe impacts of holidays, weekends, etc., by this point. \nThe critical path for the project is discovered at this step \nin the process.\n\nStep 6: Review the workforce level and funding profile \nover time and make a final set of adjustments to logic \nand durations so that workforce levels and funding levels \nare within project constraints. Adjustments to the logic \nand the durations of activities may be needed to con-\n\nActivity-on-Arrow Diagram\n\nPrecedence Diagram\n\n5 5\n\n5\n5\n\nSS5\n\n10\n\nA\n\n10\n\nB\n\nActivity description, including an action\nand the subject of that action\n\nActivity duration (e.g., days)\n\nMeans that \nActivity B can\nstart 5 days after\nActivity A starts\n\nA1 A2\n\nB1\nB2\n\nActivity A arti?cially divided \ninto two separate activities\n\nActivity duration\n(e.g., days)\n\nActivity description, including an action\nand the subject of that action\n\nFigure 6.1?2 Activity?on?arrow and precedence \ndiagrams for network schedules\n\n\n\n6.1 Technical Planning\n\nNASA Systems Engineering Handbook ? 117\n\nverge to the schedule targets established at the project \nlevel. Adjustments may include adding more activities to \nsome WBS elements, deleting redundant activities, in-\ncreasing the workforce for some activities that are on the \ncritical path, or finding ways to do more activities in par-\nallel, rather than in series. \n\nAgain, it is good practice to have some schedule reserve, \nor float, as part of a risk mitigation strategy. The product \nof these last steps is a feasible baseline schedule for each \nWBS element that is consistent with the activities of all \nother WBS elements. The sum of all of these schedules \nshould be consistent with both the technical scope and \nthe schedule goals of the project. There should be enough \nfloat in this integrated master schedule so that schedule \nand associated cost risk are acceptable to the project and \nto the project\u2019s customer. Even when this is done, time \nestimates for many WBS elements will have been under-\nestimated or work on some WBS elements will not start \nas early as had been originally assumed due to late ar-\nrival of receivables. Consequently, replanning is almost \nalways needed to meet the project\u2019s goals.\n\nReporting Techniques\nSummary data about a schedule is usually described in \ncharts. A Gantt chart is a bar chart that depicts a project \nschedule using start and finish dates of the appropriate \nproduct elements tied to the project WBS of a project. \nSome Gantt charts also show the dependency (i.e., pre-\ncedence and critical path) relationships among activities \nand also current status. A good example of a Gantt chart is \nshown in Figure 6.1-3. (See box on Gantt chart features.) \n\nAnother type of output format is a table that shows the \nfloat and recent changes in float of key activities. For ex-\nample, a project manager may wish to know precisely \nhow much schedule reserve has been consumed by crit-\nical path activities, and whether reserves are being con-\nsumed or are being preserved in the latest reporting \nperiod. This table provides information on the rate of \nchange of schedule reserve.\n\nResource Leveling\nGood scheduling systems provide capabilities to show re-\nsource requirements over time and to make adjustments \nso that the schedule is feasible with respect to resource \nconstraints over time. Resources may include workforce \nlevel, funding profiles, important facilities, etc. The ob-\njective is to move the start dates of tasks that have float to \n\npoints where the resource profile is feasible. If that is not \nsufficient, then the assumed task durations for resource-\nintensive activities should be reexamined and, accord-\ningly, the resource levels changed.\n\nBudgeting\nBudgeting and resource planning involve the establish-\nment of a reasonable project baseline budget and the \ncapability to analyze changes to that baseline resulting \nfrom technical and/or schedule changes. The project\u2019s \nWBS, baseline schedule, and budget should be viewed \nas mutually dependent, reflecting the technical content, \ntime, and cost of meeting the project\u2019s goals and objec-\ntives. The budgeting process needs to take into account \n\nGantt Chart Features\n\nThe Gantt chart shown in Figure 6.1-3 illustrates the \nfollowing desirable features:\n\nA heading that describes the WBS element, iden- ?\ntifies the responsible manager, and provides the \ndate of the baseline used and the date that status \nwas reported.\n\nA milestone section in the main body (lines 1 and 2). ?\n\nAn activity section in the main body. Activity data  ?\nshown includes:\n\n WBS elements (lines 3, 5, 8, 12, 16, and 21); ?\n\nActivities (indented from WBS elements); ?\n\nCurrent plan (shown as thick bars); ?\n\nBaseline plan (same as current plan, or if different,  ?\nrepresented by thin bars under the thick bars);\n\nSlack for each activity (dotted horizontal line be- ?\nfore the milestone on line 12);\n\nSchedule slips from the baseline (dotted hori- ?\nzontal lines after the current plan bars);\n\nThe critical path is shown encompassing lines 18  ?\nthrough 21 and impacting line 24; and\n\nStatus line (dotted vertical line from top to bot- ?\ntom of the main body of the chart) at the date \nthe status was reported.\n\nA legend explaining the symbols in the chart. ?\n\nThis Gantt chart shows only 24 lines, which is a sum-\nmary of the activities currently being worked for this \nWBS element. It is appropriate to tailor the amount of \ndetail reported to those items most pertinent at the \ntime of status reporting.\n\n\n\n118 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nOct Nov Dec Jan Feb Mar Apr May Jun Jul Aug Sep\nACTIVITY\n\n1 Milestones \u2013 Subsystem\n2   \u2013 Assembly\n3 Management\n4  Quarterly Assessments\n5 System Engineering\n6  Assembly Design\n7  Subassembly Requirements\n8 Subassembly #1\n9  Design\n10  Fabricate\n11  Test\n12 Subassembly #2\n13  Design\n14  Fabricate\n15  Test\n16 Subassembly #3\n17  Design\n18  Fabricate Phase 1\n19  Fabricate Phase 2\n20  Test\n21 Integration and Test\n22  Plans\n23  Procedures\n24  Integrate and Test\n\n2006 2007\nFY 2007\n\nSYSTEM (TIER 2)\nSUBSYSTEM (TIER 3)\nASSEMBLY (TIER 4)\n\nEXAMPLE PROJECT Page 1/2\n\nStatus as of: Jan 20, 2007\nRevision Date: Dec 23, 2006\n\nCDR\nCDR\n\nSDR\nPDR\n\nPDR\n\nApproval\nApproval\n\nRec\u2019d Requirements\n\nDeliver\n\nTo Integration & Test\n\nTo Integration & Test\n\nTo Integration & Test\n\nFinal\n\nFixtures\n\nReceive All\nSubassemblies\n\nToday\u2019s Date\n\nApproval\n\nResponsible Manager:\n\nLegend:\n\nFloat, positive or negative, is shown above the activity bars and event symbols.\nThe baselined schedule is shown below revised schedule, if they di?er.\n\nCritical Path\n\nNot Completed\n\nCompleted\n\nEnd of Task Schedule\n\nMajor Milestone\n\nScheduled Period of Performance for Activity\n\nFigure 6.1?3 Gantt chart\n\nwhether a fixed cost cap or cost profile exists. When no \nsuch cap or profile exists, a baseline budget is developed \nfrom the WBS and network schedule. This specifically \ninvolves combining the project\u2019s workforce and other re-\nsource needs with the appropriate workforce rates and \nother financial and programmatic factors to obtain cost \nelement estimates. These elements of cost include:\n\nDirect labor costs, ?\nOverhead costs, ?\nOther direct costs (travel, data processing, etc.), ?\nSubcontract costs, ?\nMaterial costs, ?\nGeneral and administrative costs, ?\n\nCost of money (i.e., interest payments, if applicable), ?\nFee (if applicable), and ?\nContingency. ?\n\nWhen there is a cost cap or a fixed cost profile, there are \nadditional logic gates that must be satisfied before com-\npleting the budgeting and planning process. A determi-\nnation needs to be made whether the WBS and network \nschedule are feasible with respect to mandated cost caps \nand/or cost profiles. If not, it will be necessary to con-\nsider stretching out a project (usually at an increase in \nthe total cost) or descoping the project\u2019s goals and objec-\ntives, requirements, design, and/or implementation ap-\nproach. \n\n\n\n6.1 Technical Planning\n\nNASA Systems Engineering Handbook ? 119\n\nIf a cost cap or fixed cost profile exists, it is important to \ncontrol costs after they have been baselined. An important \naspect of cost control is project cost and schedule status re-\nporting and assessment, methods for which are discussed \nin Section 6.7. Another is cost and schedule risk planning, \nsuch as developing risk avoidance and workaround strate-\ngies. At the project level, budgeting and resource planning \nmust ensure that an adequate level of contingency funds is \nincluded to deal with unforeseen events.\n\nThe maturity of the Life-Cycle Cost Estimate (LCCE) \nshould progress as follows:\n\nPre-Phase A: Initial LCCE (70 percent confidence  ?\nlevel; however, much uncertainty is expected) \nPhase A: Preliminary commitment to LCCE ?\nPhase B: Approve LCCE (70 percent confidence level  ?\nat PDR commitment)\nPhase C, D, and E report variances to LCCE baseline  ?\nusing Earned Value Management (EVM) and LCCE \nupdates\n\nCredibility of the cost estimate is suspect if:\nWBS cost estimates are expressed only in dollars with  ?\nno other identifiable units, indicating that require-\nments are not sufficiently defined for processes and \nresources to be identified.\nThe basis of estimates does not contain sufficient de- ?\ntail for independent verification that work scope and \nestimated cost (and schedule) are reasonable.\nActual costs vary significantly from the LCCE. ?\nWork is performed that was not originally planned,  ?\ncausing cost or schedule variance.\nSchedule and cost earned value performance trends  ?\nreadily indicate unfavorable performance.\n\nPrepare the SEMP and Other Technical Plans\nThe SEMP is the primary, top-level technical manage-\nment document for the project and is developed early \nin the Formulation phase and updated throughout the \nproject life cycle. The SEMP is driven by the type of \nproject, the phase in the project life cycle, and the tech-\nnical development risks and is written specifically for \neach project or project element. While the specific con-\ntent of the SEMP is tailored to the project, the recom-\nmended content is discussed in Appendix J.\n\nThe technical team, working under the overall project \nplan, develops and updates the SEMP as necessary. The \n\ntechnical team works with the project manager to review \nthe content and obtain concurrence. This allows for thor-\nough discussion and coordination of how the proposed \ntechnical activities would impact the programmatic, cost, \nand schedule aspects of the project. The SEMP provides \nthe specifics of the technical effort and describes what \ntechnical processes will be used, how the processes will be \napplied using appropriate activities, how the project will \nbe organized to accomplish the activities, and the cost and \nschedule associated with accomplishing the activities.\n\nThe physical length of a SEMP is not what is important. \nThis will vary from project to project. The plan needs to \nbe adequate to address the specific technical needs of the \nproject. It is a living document that is updated as often as \nnecessary to incorporate new information as it becomes \navailable and as the project develops through Implemen-\ntation. The SEMP should not duplicate other project \ndocuments; however, the SEMP should reference and \nsummarize the content of other technical plans. \n\nThe systems engineer and project manager must iden-\ntify additional required technical plans based on the \nproject scope and type. If plans are not included in the \nSEMP, they should be referenced and coordinated in the \ndevelopment of the SEMP. Other plans, such as system \nsafety and the probabilistic risk assessment, also need \nto be planned for and coordinated with the SEMP. If a \ntechnical plan is a stand-alone, it should be referenced \nin the SEMP. Depending on the size and complexity of \nthe project, these may be separate plans or may be in-\ncluded within the SEMP. Once identified, the plans can \nbe developed, training on these plans established, and \nthe plans implemented. Examples of technical plans in \naddition to the SEMP are listed in Appendix K.\n\nThe SEMP must be developed concurrently with the \nproject plan. In developing the SEMP, the technical ap-\nproach to the project and, hence, the technical aspect of \nthe project life cycle is developed. This determines the \nproject\u2019s length and cost. The development of the pro-\ngrammatic and technical management approaches \nrequires that the key project personnel develop an \nunderstanding of the work to be performed and the re-\nlationships among the various parts of that work. Refer \nto Subsections 6.1.2.1 and 6.1.1.2 on WBSs and network \nscheduling, respectively.\n\nThe SEMP\u2019s development requires contributions from \nknowledgeable programmatic and technical experts from \nall areas of the project that can significantly influence the \n\n\n\n120 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nproject\u2019s outcome. The involvement of recognized ex-\nperts is needed to establish a SEMP that is credible to the \nproject manager and to secure the full commitment of \nthe project team.\n\nRole of the SEMP\nThe SEMP is the rule book that describes to all partici-\npants how the project will be technically managed. The \nNASA field center responsible for the project should \nhave a SEMP to describe how it will conduct its technical \nmanagement, and each contractor should have a SEMP \nto describe how it will manage in accordance with both \nits contract and NASA\u2019s technical management practices. \nEach Center that is involved with the project should also \nhave a SEMP for its part of the project, which would in-\nterface with the project SEMP of the responsible NASA \nCenter, but this lower tier SEMP specifically will address \nthat Center\u2019s technical effort and how it interfaces with \nthe overall project. Since the SEMP is project- and con-\ntract-unique, it must be updated for each significant pro-\ngrammatic change, or it will become outmoded and un-\nused, and the project could slide into an uncontrolled state. \nThe lead NASA field center should have its SEMP devel-\noped before attempting to prepare an initial cost estimate, \nsince activities that incur cost, such as technical risk re-\nduction, need to be identified and described beforehand. \nThe contractor should have its SEMP developed during \nthe proposal process (prior to costing and pricing) be-\ncause the SEMP describes the technical content of the \nproject, the potentially costly risk management activi-\nties, and the verification and validation techniques to be \nused, all of which must be included in the preparation of \nproject cost estimates. The SEMPs from the supporting \nCenters should be developed along with the primary \nproject SEMP. The project SEMP is the senior technical \nmanagement document for the project: all other tech-\nnical plans must comply with it. The SEMP should be \ncomprehensive and describe how a fully integrated engi-\nneering effort will be managed and conducted.\n\nObtain Stakeholder Commitments to Technical \nPlans\nTo obtain commitments to the technical plans by the \nstakeholders, the technical team should ensure that the \nappropriate stakeholders have a method to provide in-\nputs and to review the project planning for implemen-\ntation of stakeholder interests. During Formulation, \nthe roles of the stakeholders should be defined in the \nproject plan and the SEMP. Review of these plans and \n\nthe agreement from the stakeholders of the content of \nthese plans will constitute buy-in from the stakeholders \nin the technical approach. Later in the project life cycle, \nstakeholders may be responsible for delivery of products \nto the project. Initial agreements regarding the respon-\nsibilities of the stakeholders are key to ensuring that the \nproject technical team obtains the appropriate deliveries \nfrom stakeholders.\n\nThe identification of stakeholders is one of the early steps in \nthe systems engineering process. As the project progresses, \nstakeholder expectations are flowed down through the \nLogical Decomposition Process, and specific stakeholders \nare identified for all of the primary and derived require-\nments. A critical part of the stakeholders\u2019 involvement is \nin the definition of the technical requirements. As require-\nments and ConOps are developed, the stakeholders will \nbe required to agree to these products. Inadequate stake-\nholder involvement will lead to inadequate requirements \nand a resultant product that does not meet the stakeholder \nexpectations. Status on relevant stakeholder involvement \nshould be tracked and corrective action taken if stake-\nholders are not participating as planned.\n\nThroughout the project life cycle, communication with \nthe stakeholders and commitment from the stakeholders \nmay be accomplished through the use of agreements. Or-\nganizations may use an Internal Task Agreement (ITA), \na Memorandum of Understanding (MOU), or other \nsimilar documentation to establish the relationship be-\ntween the project and the stakeholder. These agreements \nalso are used to document the customer and provider re-\nsponsibilities for definition of products to be delivered. \nThese agreements should establish the Measures of Ef-\nfectiveness (MOEs) or Measures of Performance (MOPs) \nthat will be used to monitor the progress of activities. Re-\nporting requirements and schedule requirements should \nbe established in these agreements. Preparation of these \nagreements will ensure that the stakeholders\u2019 roles and \nresponsibilities support the project goals and that the \nproject has a method to address risks and issues as they \nare identified.\n\nDuring development of the project plan and the SEMP, \nforums are established to facilitate communication and \ndocument decisions during the life cycle of the project. \nThese forums include meetings, working groups, deci-\nsion panels, and control boards. Each of these forums \nshould establish a charter to define the scope and au-\nthority of the forum and identify necessary voting or \n\n\n\n6.1 Technical Planning\n\nNASA Systems Engineering Handbook ? 121\n\nnonvoting participants. Ad hoc members may be identi-\nfied when the expertise or input of specific stakeholders \nis needed when specific topics are addressed. Ensure that \nstakeholders have been identified to support the forum.\n\nIssue Technical Work Directives\nThe technical team provides technical work directives \nto Cost Account Managers (CAMs). This enables the \nCAMs to prepare detailed plans that are mutually con-\nsistent and collectively address all of the work to be per-\nformed. These plans include the detailed schedules and \nbudgets for cost accounts that are needed for cost man-\nagement and EVM. \n\nIssuing technical work directives is an essential activity \nduring Phase B of a project, when a detailed planning \nbaseline is required. If this activity is not implemented, \nthen the CAMs are often left with insufficient guidance \nfor detailed planning. The schedules and budgets that are \nneeded for EVM will then be based on assumptions and \nlocal interpretations of project-level information. If this \nis the case, it is highly likely that substantial variances \nwill occur between the baseline plan and the work per-\nformed. Providing technical work directives to CAMs \nproduces a more organized technical team. This activity \nmay be repeated when replanning occurs.\n\nThis activity is not limited to systems engineering. This \nis a normal part of project planning wherever there is a \nneed for an accurate planning baseline. \n\nThe technical team will provide technical directives to \nCAMs for every cost account within the SE element of \nthe WBS. These directives may be in any format, but \nshould clearly communicate the following information \nfor each account:\n\nTechnical products expected; ?\nDocuments and technical reporting requirements for  ?\neach cost account;\nCritical events, and specific products expected from a  ?\nparticular CAM in support of this event (e.g., this cost \naccount is expected to deliver a presentation on spe-\ncific topics at the PDR);\nReferences to applicable requirements, policies, and  ?\nstandards;\nIdentification of particular tools that should be used;  ?\nInstructions on how the technical team wants to co- ?\nordinate and review cost account plans before they go \nto project management; and\n\nDecisions that have been made on how work is to be  ?\nperformed and who is to perform it. \n\nCAMs receive these technical directives, along with the \nproject planning guidelines, and prepare cost account \nplans. These plans may be in any format and may have \nvarious names at different Centers, but minimally they \nwill include:\n\nScope of the cost account, which includes: ?\nTechnical products delivered; ?\nOther products developed that will be needed to  ?\ncomplete deliverables (e.g., a Configuration Man-\nagement (CM) system may need development in \norder to deliver the product of a \u201cmanaged configu-\nration\u201d);\nA brief description of the procedures that will be  ?\nfollowed to complete work on these products, such \nas:\n\nProduct X will be prepared in-house, using the  \u2022\nlocal procedure A, which is commonly used in \nOrganization ABC,\nProduct X will be verified/validated in the fol- \u2022\nlowing manner\u2026,\nProduct X will be delivered to the project in the  \u2022\nfollowing manner\u2026,\nProduct X delivery will include the following re- \u2022\nports (e.g., delivery of a CM system to the project \nwould include regular reports on the status of the \nconfiguration, etc.),\nProduct Y will be procured in accordance with  \u2022\nprocurement procedure B.\n\nA schedule attached to this plan in a format com- ?\npatible with project guidelines for schedules. This \nschedule would contain each of the procedures and \ndeliverables mentioned above and provide additional \ninformation on the activity steps of each procedure.\nA budget attached to this plan in a system compat- ?\nible with project guidelines for budgets. This budget \nwould be consistent with the resources needed to ac-\ncomplish the scheduled activities.\nAny necessary agreements and approvals.  ?\n\nIf the project is going to use EVM, then the scope of a \ncost account needs to further identify a number of \u201cwork \npackages,\u201d which are units of work that can be sched-\nuled and given cost estimates. Work packages should be \nbased on completed products to the greatest extent pos-\n\n\n\n122 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nsible, but may also be based on completed procedures \n(e.g., completion of validation). Each work package \nwill have its own schedule and a budget. The budget for \nthis work package becomes part of the Budgeted Cost \nof Work Scheduled (BCWS) in the EVM system. When \nthis unit of work is completed, the project\u2019s earned value \nwill increase by this amount. There may be future work \nin this cost account that is not well enough defined to \nbe described as a set of work packages. For example, \nlaunch operations will be supported by the technical \nteam, but the details of what will be done often have not \nbeen worked out during Phase B. In this case, this future \nwork is called a \u201cplanning package,\u201d which has a high-\nlevel schedule and an overall budget. When this work is \nunderstood better, the planning package will be broken \nup into work packages, so that the EVM system can con-\ntinue to operate during launch operations.\n\nCost account plans should be reviewed and approved by \nthe technical team and by the line manager of the cost \naccount manager\u2019s home organization. Planning guide-\nlines may identify additional review and approval re-\nquirements.\n\nThe planning process described above is not limited to \nsystems engineering. This is the expected process for all \nelements of a flight project. One role that the systems en-\ngineer may have in planning is to verify that the scope of \nwork described in cost account plans across the project \nis consistent with the project WBS dictionary, and that \nthe WBS dictionary is consistent with the architecture \nof the project.\n\nCapture Technical Planning Work Products\nThe work products from the Technical Planning pro-\ncess should be managed using either the Technical \nData Management Process or the Configuration Man-\nagement Process as required. Some of the more impor-\ntant products of technical planning (i.e., the WBS, the \nSEMP, and the schedule, etc.) are kept under configu-\nration control and captured using the CM process. The \nTechnical Data Management Process is used to capture \ntrade studies, cost estimates, technical analyses, reports, \nand other important documents not under formal con-\nfiguration control. Work products, such as meeting \nminutes and correspondence (including e-mail) con-\ntaining decisions or agreements with stakeholders also \nshould be retained and stored in project files for later \nreference.\n\n6.1.1.3 Outputs\nTypical outputs from technical planning activities are:\n\nTechnical work cost estimates, schedules, and re- ?\nsource needs, e.g., funds, workforce, facilities, and \nequipment (to project), within the project resources;\nProduct and process measures needed to assess prog- ?\nress of the technical effort and the effectiveness of \nprocesses (to Technical Assessment Process);\nTechnical planning strategy, WBS, SEMP, and other  ?\ntechnical plans that support implementation of the \ntechnical effort (to all processes; applicable plans to \ntechnical processes);\nTechnical work directives, e.g., work packages or task  ?\norders with work authorization (to applicable tech-\nnical teams); and\nTechnical Planning Process work products needed  ?\nto provide reports, records, and nondeliverable out-\ncomes of process activities (to Technical Data Man-\nagement Process).\n\nThe resulting technical planning strategy would consti-\ntute an outline, or rough draft, of the SEMP. This would \nserve as a starting part for the overall Technical Planning \nProcess after initial preparation is complete. When prep-\narations for technical planning are complete, the tech-\nnical team should have a cost estimate and schedule for \nthe technical planning effort. The budget and schedule to \nsupport the defined technical planning effort can then be \nnegotiated with the project manager to resolve any dis-\ncrepancies between what is needed and what is available. \nThe SEMP baseline needs to be completed. Planning for \nthe update of the SEMP based on programmatic changes \nneeds to be developed and implemented. The SEMP needs \nto be approved by the appropriate level of authority.\n\nThis \u201ctechnical work directives\u201d step produces: (1) plan-\nning directives to cost account managers that result in \n(2) a consistent set of cost account plans. Where EVM \nis called for, it produces (3) an EVM planning baseline, \nincluding a BCWS. \n\n6.1.2 Technical Planning Guidance\n\n6.1.2.1 Work Breakdown Structure\nA work breakdown structure is a hierarchical break-\ndown of the work necessary to complete a project. The \nWBS should be a product-based, hierarchical division \nof deliverable items and associated services. As such, it \n\n\n\n6.1 Technical Planning\n\nNASA Systems Engineering Handbook ? 123\n\nshould contain the project\u2019s Product Breakdown Struc-\nture (PBS) with the specified prime product(s) at the top \nand the systems, segments, subsystems, etc., at succes-\nsive lower levels. At the lowest level are products such as \nhardware items, software items, and information items \n(documents, databases, etc.) for which there is a cogni-\nzant engineer or manager. Branch points in the hierarchy \nshould show how the PBS elements are to be integrated. \nThe WBS is built, in part, from the PBS by adding, at each \nbranch point of the PBS, any necessary service elements, \nsuch as management, systems engineering, Integration \nand Verification (I&V), and integrated logistics support. \nIf several WBS elements require similar equipment or \nsoftware, then a higher level WBS element might be de-\nfined from the system level to perform a block buy or a \ndevelopment activity (e.g., system support equipment). \nFigure 6.1-4 shows the relationship between a system, a \nPBS, and a WBS. In summary, the WBS is a combination \nof the PBS and input from the system level. The system \nlevel is incorporated to capture and integrate similarities \nacross WBS elements.\n\nA project WBS should be carried down to the cost ac-\ncount level appropriate to the risks to be managed. The \nappropriate level of detail for a cost account is deter-\nmined by management\u2019s desire to have visibility into \ncosts, balanced against the cost of planning and report-\ning. Contractors may have a Contract WBS (CWBS) that \nis appropriate to their need to control costs. A summary \nCWBS, consisting of the upper levels of the full CWBS, \nis usually included in the project WBS to report costs to \nthe contracting organization. WBS elements should be \nidentified by title and by a numbering system that per-\nforms the following functions:\n\nIdentifies the level of the WBS element, ?\nIdentifies the higher level element into which the  ?\nWBS element will be integrated, and\nShows the cost account number of the element. ?\n\nA WBS should also have a companion WBS dictionary \nthat contains each element\u2019s title, identification number, \nobjective, description, and any dependencies (e.g., re-\nceivables) on other WBS elements. This dictionary pro-\nvides a structured project description that is valuable for \norienting project members and other interested parties. \nIt fully describes the products and/or services expected \nfrom each WBS element. This subsection provides some \ntechniques for developing a WBS and points out some \nmistakes to avoid.\n\nRole of the WBS\nThe technical team should receive planning guidelines \nfrom the project office. The technical team should pro-\nvide the project office with any appropriate tailoring or \nexpansion of the systems engineering WBS element, and \nhave project-level concurrence on the WBS and WBS \ndictionary before issuing technical work directives.\n\nA product-based WBS is the organizing structure for:\nProject and technical planning and scheduling. ?\nCost estimation and budget formulation. (In partic- ?\nular, costs collected in a product-based WBS can be \ncompared to historical data. This is identified as a pri-\nmary objective by DOD standards for WBSs.)\n\nFigure 6.1?4 Relationship between a system, a \nPBS, and a WBS\n\nThe whole does more\nthan the sum of the parts.\n\nThe whole takes more work\nthan the sum of the parts.\n\nSubsystem A\nSYSTEM\n\nComponents\n(subsystems)\nheld together\n\nby \u201cglue\u201d\n(integration)\n\nPBS\nShows the components\n\nthat form the system\n\nSystem\n\nA B C D\n\nThe individual\nsystem components\n\nWBS\nAll work components\nnecessary to produce\n\na complete system\n\nSystem\n\nA B C D\n\nWork to produce\nthe individual\n\nsystem components\n\nWork to integrate the\ncomponents into a system\n\nMgmt ILSI&VSE\n\nSubsystem B\nSubsystem C\n\nSubsystem D\n\n\n\n124 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nDefining the scope of statements of work and specifi- ?\ncations for contract efforts.\nProject status reporting, including schedule, cost,  ?\nworkforce, technical performance, and integrated \ncost/schedule data (such as earned value and esti-\nmated cost at completion).\nPlans, such as the SEMP, and other documentation  ?\nproducts, such as specifications and drawings.\n\nIt provides a logical outline and vocabulary that de-\nscribes the entire project, and integrates information in \na consistent way. If there is a schedule slip in one ele-\nment of a WBS, an observer can determine which other \nWBS elements are most likely to be affected. Cost im-\npacts are more accurately estimated. If there is a design \nchange in one element of the WBS, an observer can de-\ntermine which other WBS elements will most likely be \naffected, and these elements can be consulted for poten-\ntial adverse impacts.\n\nTechniques for Developing the WBS\n\nDeveloping a successful project WBS is likely to require \nseveral iterations through the project life cycle since it \nis not always obvious at the outset what the full extent \nof the work may be. Prior to developing a preliminary \nWBS, there should be some development of the system \narchitecture to the point where a preliminary PBS can be \ncreated. The PBS and associated WBS can then be devel-\noped level by level from the top down. In this approach, \na project-level systems engineer finalizes the PBS at the \nproject level and provides a draft PBS for the next lower \nlevel. The WBS is then derived by adding appropriate \nservices such as management and systems engineering to \nthat lower level. This process is repeated recursively until \na WBS exists down to the desired cost account level. An \nalternative approach is to define all levels of a complete \nPBS in one design activity and then develop the com-\nplete WBS. When this approach is taken, it is necessary \nto take great care to develop the PBS so that all products \nare included and all assembly/I&V branches are correct. \nThe involvement of people who will be responsible for \nthe lower level WBS elements is recommended.\n\nCommon Errors in Developing a WBS \n\nThere are three common errors found in WBSs.\nError 1: ?  The WBS describes functions, not products. \nThis makes the project manager the only one formally \nresponsible for products.\n\nError 2: ?  The WBS has branch points that are not \nconsistent with how the WBS elements will be in-\ntegrated. For instance, in a flight operations system \nwith a distributed architecture, there is typically \nsoftware associated with hardware items that will be \nintegrated and verified at lower levels of a WBS. It \nwould then be inappropriate to separate hardware \nand software as if they were separate systems to be \nintegrated at the system level. This would make it \ndifficult to assign accountability for integration and \nto identify the costs of integrating and testing com-\nponents of a system.\nError 3: ?  The WBS is inconsistent with the PBS. This \nmakes it possible that the PBS will not be fully imple-\nmented and generally complicates the management \nprocess. \n\nSome examples of these errors are shown in Figure 6.1-5. \nEach one prevents the WBS from successfully performing \nits roles in project planning and organizing. These errors \nare avoided by using the WBS development techniques \ndescribed above.\n\nCommon to both the project management and systems \nengineering disciplines is the requirement for organizing \nand managing a system throughout its life cycle within \na systematic and structured framework, reflective of the \nwork to be performed and the associated cost, schedule, \ntechnical, and risk data to be accumulated, summarized, \nand reported. (See NPR 7120.5.)\n\nA key element of this framework is a hierarchical, \nproduct-oriented WBS. Derived from both the physical \nand system architectures, the WBS provides a system-\natic, logical approach for defining and translating ini-\ntial mission goals and technical concepts into tangible \nproject goals, system products, and life-cycle support (or \nenabling) functions.\n\nWhen appropriately structured and used in conjunction \nwith sound engineering principles, the WBS supplies a \ncommon framework for subdividing the total project \ninto clearly defined, product-oriented work compo-\nnents, logically related and sequenced according to hier-\narchy, schedule, and responsibility assignment. \n\nThe composition and level of detail required in the WBS \nhierarchy is determined by the project management and \ntechnical teams based on careful consideration of the \nproject\u2019s size and the complexity, constraints, and risk \nassociated with the technical effort. The initial WBS will \n\n\n\n6.1 Technical Planning\n\nNASA Systems Engineering Handbook ? 125\n\nprovide a structured framework for conceptualizing and \ndefining the program/project objectives and for trans-\nlating the initial concepts into the major systems, com-\nponent products, and services to be developed, pro-\nduced, and/or obtained. As successive levels of detail \nare defined, the WBS hierarchy will evolve to reflect a \ncomprehensive, complete view of both the total project \neffort and each system or end product to be realized \nthroughout the project\u2019s life cycle. \n\nDecomposition of the major deliverables into unique, \ntangible product or service elements should continue to \na level representative of how each WBS element will be \nplanned and managed. Whether assigned to in-house or \ncontractor organizations, these lower WBS elements will \nbe subdivided into subordinate tasks and activities and \naggregated into the work packages and control accounts \nutilized to populate the project\u2019s cost plans, schedules, \nand performance metrics. \n\nAt a minimum, the WBS should reflect the major system \nproducts and services to be developed and/or procured, \nthe enabling (support) products and services, and any \nhigh-cost and/or high-risk product elements residing at \nlower levels in the hierarchy.1 The baseline WBS config-\nuration will be documented as part of the program plan \nand utilized to structure the SEMP. The cost estimates \nand the WBS dictionary are maintained throughout the \nproject\u2019s life cycle to reflect the project\u2019s current scope. \n\nThe preparation and approval of three key program/\nproject documents, the Formulation Authorization Doc-\nument (FAD), the program commitment agreement, and \nthe program/project plans are significant contributors to \nearly WBS development.\n\nThe initial contents of these documents will establish the \npurpose, scope, objectives, and applicable agreements \nfor the program of interest and will include a list of ap-\nproved projects, control plans, management approaches, \nand any commitments and constraints identified.\n\nThe technical team selects the appropriate system design \nprocesses to be employed in the top-down definition of \neach product in the system structure. Subdivision of the \nproject and system architecture into smaller, more man-\nageable components will provide logical summary points \nfor assessing the overall project\u2019s accomplishments and \nfor measuring cost and schedule performance. \n\nOnce the initial mission goals and objectives have evolved \ninto the build-to or final design, the WBS will be refined \nand updated to reflect the evolving scope and architec-\nture of the project and the bottom-up realization of each \nproduct in the system structure.\n\nThroughout the applicable life-cycle phases, the WBS \nand WBS dictionary will be updated to reflect the proj-\nect\u2019s current scope and to ensure control of high-risk and \ncost/schedule performance issues.\n\n6.1.2.2 Cost Definition and Modeling\nThis subsection deals with the role of cost in the systems \nanalysis and engineering process, how to measure it, \nhow to control it, and how to obtain estimates of it. The \nreason costs and their estimates are of great importance \n\n1IEEE Standard 1220, Section C.3, \u201cThe system products \nand life cycle enabling products should be jointly engineered \nand once the enabling products and services are identified, \nshould be treated as systems in the overall system hierarchy.\u201d\n\nFigure 6.1?5 Examples of WBS development \nerrors\n\nError 1: Functions Without Products\n\nError 3: Inconsistency With PBS\n\nError 2: Inappropriate Branches\n\nProject\n\nManagement Engineering Fabrication Veri?cation\n\nDistributed\nInformation System\n\nHardwareSoftware\n\nTransmitter TWT Ampli?er\n\nSubsystem Subsystem\n\nTransmitter\n\nTWT Ampli?er\n\nWBS PBS\n\n\n\n126 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nin systems engineering goes back to a principal objec-\ntive of systems engineering: fulfilling the system\u2019s goals \nin the most cost-effective manner. The cost of each al-\nternative should be one of the most important outcome \nvariables in trade studies performed during the systems \nengineering process. \n\nOne role, then, for cost estimates is in helping to choose \nrationally among alternatives. Another is as a control \nmechanism during the project life cycle. Cost measures \nproduced for project life-cycle reviews are important in \ndetermining whether the system goals and objectives \nare still deemed valid and achievable, and whether con-\nstraints and boundaries are worth maintaining. These \nmeasures are also useful in determining whether system \ngoals and objectives have properly flowed down through \nto the various subsystems. \n\nAs system designs and ConOps mature, cost estimates \nshould mature as well. At each review, cost estimates \nneed to be presented and compared to the funds likely \nto be available to complete the project. The cost estimates \npresented at early reviews must be given special attention \nsince they usually form the basis for the initial cost com-\nmitment for the project. The systems engineer must be \nable to provide realistic cost estimates to the project man-\nager. In the absence of such estimates, overruns are likely \nto occur, and the credibility of the entire system develop-\nment process, both internal and external, is threatened.\n\nLife?Cycle Cost and Other Cost Measures\n\nA number of questions need to be addressed so that costs \nare properly treated in systems analysis and engineering. \nThese questions include:\n\nWBS Hierarchies for Systems\n\nIt is important to note that while product-oriented in nature, the standard WBS mandated for NASA space flight proj-\nects in NPR 7120.5 approaches WBS development from a project and not a system perspective. The WBS mandated re-\nflects the scope of a major Agency project and, therefore, is structured to include the development, operation, and dis-\nposal of more than one major system of interest during the project\u2019s normal life cycle.\n\nWBS hierarchies for NASA\u2019s space flight projects will include high-level system products, such as payload, spacecraft, \nand ground systems, and enabling products and services, such as project management, systems engineering, and ed-\nucation. These standard product elements have been established to facilitate alignment with the Agency\u2019s accounting, \nacquisition, and reporting systems.\n\nUnlike the project-view WBS approach described in NPR 7120.5, creation of a technical WBS focuses on the develop-\nment and realization of both the overall end product and each subproduct included as a lower level element in the \noverall system structure. \n\nNPR 7123.1, NASA Systems Engineering Processes and Requirements mandates a standard, systematic technical approach \nto system or end-product development and realization. Utilizing a building-block or product-hierarchy approach, the \nsystem architecture is successively defined and decomposed into subsystems (elements performing the operational \nfunctions of the system) and associated and interrelated subelements (assemblies, components, parts, and enabling \nlife-cycle products).\n\nThe resulting hierarchy or family-product tree depicts the entire system architecture in a PBS. Recognized by Govern-\nment and industry as a \u201cbest practice,\u201d utilization of the PBS and its building-block configuration facilitates both the ap-\nplication of NPR 7123.1\u2019s 17 common technical processes at all levels of the PBS structure and the definition and realiza-\ntion of successively lower level elements of the system\u2019s hierarchy. \n\nDefinition and application of the work effort to the PBS structure yields a series of functional subproducts or \u201cchildren\u201d \nWBS models. The overall parent or system WBS model is realized through the rollup of successive levels of these prod-\nuct-based, subelement WBS models. \n\nEach WBS model represents one unique unit or functional end product in the overall system configuration and, when \nrelated by the PBS into a hierarchy of individual models, represents one functional system end product or \u201cparent\u201d WBS \nmodel. \n\n(See NPR 7120.5, NASA Space Flight Program and Project Management Requirements.)\n\n\n\n6.1 Technical Planning\n\nNASA Systems Engineering Handbook ? 127\n\nWhat costs should be counted? ?\nHow should costs occurring at different times be  ?\ntreated?\nWhat about costs that cannot easily be measured in  ?\ndollars?\n\nWhat Costs Should Be Counted \nThe most comprehensive measure of the cost of an al-\nternative is its life-cycle cost. According to NPR 7120.5, \na system\u2019s life-cycle cost is, \u201cthe total of the direct, indi-\nrect, recurring, nonrecurring, and other related expenses \nincurred, or estimated to be incurred, in the design, de-\nvelopment, verification, production, operation, mainte-\nnance, support, and disposal of a project. The life-cycle \ncost of a project or system can also be defined as the total \ncost of ownership over the project or system\u2019s life cycle \nfrom Formulation through Implementation. It includes \nall design, development, deployment, operation and \nmaintenance, and disposal costs.\u201d\n\nCosts Occurring Over Time \nThe life-cycle cost combines costs that typically occur over \na period of several years. To facilitate engineering trades \nand comparison of system costs, these real year costs are \ndeescalated to constant year values. This removes the \nimpact of inflation from all estimates and allows ready \ncomparison of alternative approaches. In those instances \nwhere major portfolio architectural trades are being con-\nducted, it may be necessary to perform formal cost ben-\nefit analyses or evaluate leasing versus purchase alterna-\ntives. In those trades, engineers and cost analysts should \nfollow the guidance provided in Office of Management \nand Budget (OMB) Circular A-94 on rate of return and \nnet present value calculation in comparing alternatives. \n\nDifficult?to?Measure Costs \nIn practice, estimating some costs poses special prob-\nlems. These special problems, which are not unique to \nNASA systems, usually occur in two areas: (1) when al-\nternatives have differences in the irreducible chances of \nloss of life, and (2) when externalities are present. Two \nexamples of externalities that impose costs are pollution \ncaused by some launch systems and the creation of or-\nbital debris. Because it is difficult to place a dollar figure \non these resource uses, they are generally called \u201cincom-\nmensurable costs.\u201d The general treatment of these types \nof costs in trade studies is not to ignore them, but instead \nto keep track of them along with other costs. If these ele-\n\nments are part of the trade space, it is generally advisable \nto apply Circular A-94 approaches to those trades.\n\nControlling Life?Cycle Costs\nThe project manager/systems engineer must ensure that \nthe probabilistic life-cycle cost estimate is compatible \nwith NASA\u2019s budget and strategic priorities. The current \npolicy is that projects are to submit budgets sufficient to \nensure a 70 percent probability of achieving the objec-\ntives within the proposed resources. Project managers \nand systems engineers must establish processes to esti-\nmate, assess, monitor, and control the project\u2019s life-cycle \ncost through every phase of the project. \n\nEarly decisions in the systems engineering process tend to \nhave the greatest effect on the resultant system life-cycle \ncost. Typically, by the time the preferred system archi-\ntecture is selected, between 50 and 70 percent of the sys-\ntem\u2019s life-cycle cost has been locked in. By the time a pre-\nliminary system design is selected, this figure may be as \nhigh as 90 percent. This presents a major dilemma to the \nsystems engineer, who must lead this selection process. \nJust at the time when decisions are most critical, the state \nof information about the alternatives is least certain. Un-\ncertainty about costs is a fact of systems engineering, \nand that uncertainty must be accommodated by com-\nplete and careful analysis of the project risks and provi-\nsion of sufficient margins (cost, technical, and schedule) \nto ensure success. There are a number of estimating tech-\nniques to assist the systems engineer and project man-\nager in providing for uncertainty and unknown require-\nments. Additional information on these techniques can \nbe found in the NASA Cost Estimating Handbook.\n\nThis suggests that efforts to acquire better information \nabout the life-cycle cost of each alternative early in the \nproject life cycle (Phases A and B) potentially have very \nhigh payoffs. The systems engineer needs to identify the \nprincipal life-cycle cost drivers and the risks associated \nwith the system design, manufacturing, and operations. \nConsequently, it is particularly important with such a \nsystem to bring in the specialty engineering disciplines \nsuch as reliability, maintainability, supportability, and \noperations engineering early in the systems engineering \nprocess, as they are essential to proper life-cycle cost es-\ntimation. \n\nOne mechanism for controlling life-cycle cost is to estab-\nlish a life-cycle cost management program as part of the \nproject\u2019s management approach. (Life-cycle cost man-\n\n\n\n128 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nagement has sometimes been called \u201cdesign-to-life-cycle \ncost.\u201d) Such a program establishes life-cycle cost as a de-\nsign goal, perhaps with subgoals for acquisition costs or \noperations and support costs. More specifically, the ob-\njectives of a life-cycle cost management program are to:\n\nIdentify a common set of ground rules and assump- ?\ntions for life-cycle cost estimation;\nManage to a cost baseline and maintain traceability to  ?\nthe technical baseline with documentation for subse-\nquent cost changes;\nEnsure that best-practice methods, tools, and models  ?\nare used for life-cycle cost analysis;\nTrack the estimated life-cycle cost throughout the  ?\nproject life cycle; and, most important\nIntegrate life-cycle cost considerations into the design  ?\nand development process via trade studies and formal \nchange request assessments.\n\nTrade studies and formal change request assessments \nprovide the means to balance the effectiveness and life-\ncycle cost of the system. The complexity of integrating \nlife-cycle cost considerations into the design and devel-\nopment process should not be underestimated, but nei-\nther should the benefits, which can be measured in terms \nof greater cost-effectiveness. The existence of a rich set \nof potential life-cycle cost trades makes this complexity \neven greater. \n\nCost?Estimating Methods\n\nVarious cost-estimating methodologies are utilized \nthroughout a program\u2019s life cycle. These include para-\nmetric, analogous, and engineering (grassroots). \n\nParametric: ?  Parametric cost models are used in the \nearly stages of project development when there is lim-\nited program and technical definition. Such models \ninvolve collecting relevant historical data at an aggre-\ngated level of detail and relating it to the area to be es-\ntimated through the use of mathematical techniques \nto create cost-estimating relationships. Normally, less \ndetail is required for this approach than for other \nmethods. \nAnalogous: ?  This is based on most new programs \noriginated or evolved from existing programs or \nsimply representing a new combination of existing \ncomponents. It uses actual costs of similar existing or \npast programs and adjusts for complexity, technical, \nor physical differences to derive the new system esti-\n\nmate. This method would be used when there is insuf-\nficient actual cost data to use as a basis for a detailed \napproach but there is a sufficient amount of program \nand technical definition.\nEngineering (Grassroots):  ? These bottom-up esti-\nmates are the result of rolling up the costs estimated \nby each organization performing work described in \nthe WBS. Properly done, grassroots estimates can be \nquite accurate, but each time a \u201cwhat if \u201d question is \nraised, a new estimate needs to be made. Each change \nof assumptions voids at least part of the old estimate. \nBecause the process of obtaining grassroots estimates \nis typically time consuming and labor intensive, the \nnumber of such estimates that can be prepared during \ntrade studies is in reality severely limited. \n\nThe type of cost estimating method used will depend on \nthe adequacy of program definition, level of detail re-\nquired, availability of data, and time constraints. For ex-\nample, during the early stages of a program, a conceptual \nstudy considering several options would dictate an esti-\nmating method requiring no actual cost data and lim-\nited program definition on the systems being estimated. \nA parametric model would be a sound approach at this \npoint. Once a design is baselined and the program is \nmore adequately defined, an analogy approach becomes \nappropriate. As detailed actual cost data are accumu-\nlated, a grassroots methodology is used.\n\nMore information on cost-estimating methods and the \ndevelopment of cost estimates can be found in the NASA \nCost Estimating Handbook.\n\nIntegrating Cost Model Results for a Complete \nLife?Cycle Cost Estimate \n\nA number of parametric cost models are available for \ncosting NASA systems. A list of the models currently in \nuse may be found in an appendix in the NASA Cost Esti-\nmating Handbook. Unfortunately, none alone is sufficient \nto estimate life-cycle cost. Assembling an estimate of life-\ncycle cost often requires that several different models \n(along with the other two techniques) be used together. \nWhether generated by parametric models, analogous, \nor grassroots methods, the estimated cost of the hard-\nware element must frequently be \u201cwrapped\u201d or have fac-\ntors applied to estimate the costs associated with man-\nagement, systems engineering, test, etc., of the systems \nbeing estimated. The NASA full-cost factors also must \nbe applied separately. \n\n\n\n6.1 Technical Planning\n\nNASA Systems Engineering Handbook ? 129\n\nTo integrate the costs being estimated by these dif-\nferent models, the systems engineer should ensure \nthat the inputs to and assumptions of the models are \nconsistent, that all relevant life-cycle cost components \nare covered, and that the phasing of costs is correct. \nEstimates from different sources are often expressed \nin different year constant dollars which must be com-\nbined. Appropriate inflation factors must be applied \nto enable construction of a total life-cycle cost esti-\nmate in real year dollars. Guidance on the use of in-\nflation rates for new projects and for budget submis-\nsions for ongoing projects can be found in the annual \nNASA strategic guidance. \n\nCost models frequently produce a cost estimate for the \nfirst unit of a hardware item, but where the project re-\nquires multiple units a learning curve can be applied to \nthe first unit cost to obtain the required multiple-unit \nestimate. Learning curves are based on the concept that \nresources required to produce each additional unit de-\ncline as the total number of units produced increases. \nThe learning curve concept is used primarily for unin-\nterrupted manufacturing and assembly tasks, which are \nhighly repetitive and labor intensive. The major premise \nof learning curves is that each time the product quantity \ndoubles, the resources (labor hours) required to produce \nthe product will reduce by a determined percentage of \nthe prior quantity resource requirements. The two types \nof learning curve approaches are unit curve and cumula-\ntive average curve. The systems engineer can learn more \nabout the calculation and use of learning curves in the \nNASA Cost Estimating Handbook.\n\nModels frequently provide a cost estimate of the total \nacquisition effort without providing a recommended \nphasing of costs over the life cycle. The systems engineer \ncan use a set of phasing algorithms based on the typical \nramping-up and subsequent ramping-down of acqui-\nsition costs for that type of project if a detailed project \nschedule is not available to form a basis for the phasing \nof the effort. A normal distribution curve, or beta curve, \nis one type of function used for spreading parametrically \nderived cost estimates and for R&D contracts where costs \nbuild up slowly during the initial phases and then esca-\nlate as the midpoint of the contract approaches. A beta \ncurve is a combination of percent spent against percent \ntime elapsed between two points in time. More about \nbeta curves can be found in an appendix of the NASA \nCost Estimating Handbook.\n\nAlthough parametric cost models for space systems are \nalready available, their proper use usually requires a con-\nsiderable investment in learning how to appropriately \nutilize the models. For projects outside of the domains \nof these existing cost models, new cost models may be \nneeded to support trade studies. Efforts to develop these \nmodels need to begin early in the project life cycle to en-\nsure their timely application during the systems engi-\nneering process. Whether existing models or newly cre-\nated ones are used, the SEMP and its associated life-cycle \ncost management plan should identify which (and how) \nmodels are to be used during each phase of the project \nlife cycle.\n\n6.1.2.3 Lessons Learned \nNo section on technical planning guidance would be \ncomplete without the effective integration and incorpo-\nration of the lessons learned relevant to the project. \n\nSystems Engineering Role in Lessons Learned \nSystems engineers are the main users and contributors \nto lessons learned systems. A lesson learned is knowl-\nedge or understanding gained by experience\u2014either \na successful test or mission or a mishap or failure. Sys-\ntems engineers compile lessons learned to serve as his-\ntorical documents, requirements\u2019 rationales, and other \nsupporting data analysis. Systems engineering practitio-\nners collect lessons learned during program and project \nplans, key decision points, life-cycle phases, systems en-\ngineering processes and technical reviews. Systems en-\ngineers\u2019 responsibilities include knowing how to utilize, \nmanage, create, and store lessons learned and knowledge \nmanagement best practices.\n\nUtilization of Lessons Learned Best Practice\nLessons learned are important to future programs, proj-\nects, and processes because they show hypotheses and \nconclusive insights from previous projects or processes. \nPractitioners determine how previous lessons from pro-\ncesses or tasks impact risks to current projects and im-\nplement those lessons learned that improve design and/\nor performance. \n\nTo pull in lessons learned at the start of a project or task:\nSearch the NASA Lessons Learned Information  ?\nSystem (LLIS) database using keywords of interest \nto the new program or project. The process for re-\ncording lessons learned is explained in NPR 7120.6, \n\n\n\n130 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nLessons Learned Process. In addition, other organi-\nzations doing similar work may have publicly avail-\nable databases with lessons learned. For example, the \nChemical Safety Board has a good series of case study \nreports on mishaps.\nSupporting lessons from each engineering discipline  ?\nshould be reflected in the program and project plans. \nEven if little information was found, the search for les-\nsons learned can be documented. \nCompile lessons by topic and/or discipline. ?\nReview and select knowledge gained from particular  ?\nlessons learned.\nDetermine how these lessons learned may represent  ?\npotential risk to the current program or project.\nIncorporate knowledge gained into the project data- ?\nbase for risk management, cost estimate, and any \nother supporting data analysis. \n\nAs an example, a systems engineer working on the con-\ncept for an instrument for a spacecraft might search the \nlessons learned database using the keywords \u201cenviron-\nment,\u201d \u201cmishap,\u201d or \u201cconfiguration management.\u201d One of \nthe lessons learned that search would bring up is #1514. \nThe lesson was from Chandra. A rebaseline of the pro-\ngram in 1992 removed two instruments, changed Chan-\ndra\u2019s orbit from low Earth to high elliptical, and simpli-\nfied the thermal control concept from the active control \nrequired by one of the descoped instruments to pas-\nsive \u201ccold-biased\u201d surface plus heaters. This change in \nthermal control concept mandated silver Teflon thermal \ncontrol surfaces. The event driving the lesson was a se-\nvere spacecraft charging and an electrostatic discharge \nenvironment. The event necessitated an aggressive elec-\ntrostatic discharge test and circuit protection effort that \ncost over $1 million, according to the database. The \nTeflon thermal control surfaces plus the high elliptical \norbit created the electrostatic problem. Design solutions \nfor one environment were inappropriate in another envi-\nronment. The lesson learned was that any orbit modifica-\ntions should trigger a complete new iteration of the sys-\ntems engineering processes starting from requirements \n\ndefinition. Rebaselining a program should take into ac-\ncount change in the natural environment before new de-\nsign decisions are made. This lesson would be valuable \nto keep in mind when changes occur to baselines on the \nprogram currently being worked on.\n\nManagement of Lessons Learned Best Practice\n\nCapturing lessons learned is a function of good manage-\nment practice and discipline. Too often lessons learned \nare missed because they should have been developed and \nmanaged within, across, or between life-cycle phases. \nThere is a tendency to wait until resolution of a situa-\ntion to document a lesson learned, but the unfolding of \na problem at the beginning is valuable information and \nhard to recreate later. It is important to document a lesson \nlearned as it unfolds, particularly as resolution may not \nbe reached until a later phase. Since detailed lessons are \noften hard for the human mind to recover, waiting until a \ntechnical review or the end of a project to collect the les-\nsons learned hinders the use of lessons and the evolution \nof practice. A mechanism for managing and leveraging \nlessons as they occur, such as monthly lessons learned \nbriefings or some periodic sharing forums, facilitates in-\ncorporating lessons into practice and carrying lessons \ninto the next phase.\n\nAt the end of each life-cycle phase, practitioners should \nuse systems engineering processes and procedural tasks \nas control gate cues. All information passed across con-\ntrol gates must be managed in order to successfully enter \nthe next phase, process, or task. \n\nThe systems engineering practitioner should make sure \nall lessons learned in the present phase are concise and \nconclusive. Conclusive lessons learned contain series of \nevents that formulate abstracts and driving events. Irres-\nolute lessons learned may be rolled into the next phase \nto await proper supporting evidence. Project managers \nand the project technical team are to make sure lessons \nlearned are recorded in the Agency database at the end \nof all life-cycle phases, major systems engineering pro-\ncesses, key decision points, and technical reviews.\n\n\n\nNASA Systems Engineering Handbook ? 131\n\n6.2 Requirements Management\n\nRequirements management activities apply to the man-\nagement of all stakeholder expectations, customer re-\nquirements, and technical product requirements down \nto the lowest level product component requirements \n(hereafter referred to as expectations and requirements). \nThe Requirements Management Process is used to: \n\nManage the product requirements identified, base- ?\nlined, and used in the definition of the WBS model \nproducts during system design; \nProvide bidirectional traceability back to the top WBS  ?\nmodel requirements; and \nManage the changes to established requirement base- ?\nlines over the life cycle of the system products.\n\n6.2.1 Process Description\nFigure 6.2-1 provides a typical flow diagram for the Re-\nquirements Management Process and identifies typical \ninputs, outputs, and activities to consider in addressing \nrequirements management. \n\n6.2.1.1 Inputs\nThere are several fundamental inputs to the Require-\nments Management Process.\n\nRequirements and stakeholder expectations are iden- ?\ntified during the system design processes, primarily \nfrom the Stakeholder Expectation Definition Process \nand the Technical Requirements Definition Process. \nThe Requirements Management Process must be pre- ?\npared to deal with requirement change requests that \ncan be generated at any time during the project life \ncycle or as a result of reviews and assessments as part \nof the Technical Assessment Process. \nTPM estimation/evaluation results from the Tech- ?\nnical Assessment Process provide an early warning of \n\nFigure 6.2?1 Requirements Management Process\n\nExpectations and \nRequirements to Be\n\nManaged\n\nFrom system design processes \n\nFrom Product Verification\nand Product Validation\n\nProcesses\n\nTo Technical Data\nManagement Process\n\nTo Configuration\nManagement  Process\n\nPrepare to conduct\nrequirements management\n\nConduct requirements\nmanagement\n\nConduct expectations and \nrequirements traceability\n\nManage expectations\nand requirements changes\n\nCapture work products from\nrequirements management\n\nactivities\n\nRequirements Change\nRequests\n\nTPM Estimation/\nEvaluation Results\n\nRequirements\nDocuments\n\nApproved Changes to\nRequirements Baselines\n\nRequirements\nManagement Work\n\nProducts\n\nFrom project and Technical\nAssessment Process\n\nFrom Technical\nAssessment Process\n\nProduct Veri?cation and\nProduct Validation Results\n\nNote: Requirements can be generated from nonob-\nvious stakeholders and may not directly support the \ncurrent mission and its objectives, but instead pro-\nvide an opportunity to gain additional benefits or \ninformation that can support the Agency or the Na-\ntion. Early in the process, the systems engineer can \nhelp identify potential areas where the system can be \nused to collect unique information that is not directly \nrelated to the primary mission. Often outside groups \nare not aware of the system goals and capabilities un-\ntil it is almost too late in the process.\n\n\n\n132 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nthe adequacy of a design in satisfying selected critical \ntechnical parameter requirements. Variances from \nexpected values of product performance may trigger \nchanges to requirements.\nProduct verification and product validation results from  ?\nthe Product Verification and Product Validation Pro-\ncesses are mapped into the requirements database with \nthe goal of verifying and validating all requirements.\n\n6.2.1.2 Process Activities\nThe Requirements Management Process involves man-\naging all changes to expectations and requirements base-\nlines over the life of the product and maintaining bidi-\nrectional traceability between stakeholder expectations, \ncustomer requirements, technical product requirements, \nproduct component requirements, design documents, \nand test plans and procedures. The successful manage-\nment of requirements involves several key activities:\n\nEstablish a plan for executing requirements manage- ?\nment. \nReceive requirements from the system design processes  ?\nand organize them in a hierarchical tree structure.\nEstablish bidirectional traceability between require- ?\nments. \nValidate requirements against the stakeholder expec- ?\ntations, the mission objectives and constraints, the op-\nerational objectives, and the mission success criteria. \nDefine a verification method for each requirement. ?\nBaseline requirements. ?\nEvaluate all change requests to the requirements base- ?\nline over the life of the project and make changes if \napproved by change board.\nMaintain consistency between the requirements, the  ?\nConOps, and the architecture/design and initiate cor-\nrective actions to eliminate inconsistencies.\n\nRequirements Traceability\nAs each requirement is documented, its bidirectional \ntraceability should be recorded. Each requirement should \nbe traced back to a parent/source requirement or expec-\ntation in a baselined document or identify the require-\nment as self-derived and seek concurrence on it from the \nnext higher level requirements sources. Examples of self-\nderived requirements are requirements that are locally \nadopted as good practices or are the result of design de-\ncisions made while performing the activities of the Log-\nical Decomposition and Design Solution Processes. \n\nThe requirements should be evaluated, independently \nif possible, to ensure that the requirements trace is cor-\nrect and that it fully addresses its parent requirements. \nIf it does not, some other requirement(s) must complete \nfulfillment of the parent requirement and be included in \nthe traceability matrix. In addition, ensure that all top-\nlevel parent document requirements have been allocated \nto the lower level requirements. If there is no parent for \na particular requirement and it is not an acceptable self-\nderived requirement, it should be assumed either that \nthe traceability process is flawed and should be redone \nor that the requirement is \u201cgold plating\u201d and should be \neliminated. Duplication between levels must be resolved. \nIf a requirement is simply repeated at a lower level and it \nis not an externally imposed constraint, perhaps the re-\nquirement does not belong at the higher level. Require-\nments traceability is usually recorded in a requirements \nmatrix. (See Appendix D.)\n\nDefinitions\n\nTraceability: A discernible association between two \nor more logical entities such as requirements, system \nelements, verifications, or tasks. \n\nBidirectional traceability: An association between \ntwo or more logical entities that is discernible in ei-\nther direction (i.e., to and from an entity).\n\nRequirements Validation\nAn important part of requirements management is the \nvalidation of the requirements against the stakeholder \nexpectations, the mission objectives and constraints, the \noperational objectives, and the mission success criteria. \nValidating requirements can be broken into three steps:\n\nAre Requirements Written Correctly:1.  Identify and \ncorrect requirements \u201cshall\u201d statement format errors \nand editorial errors.\nAre Requirements Technically Correct: 2. A few \ntrained reviewers from the technical team identify \nand remove as many technical errors as possible \nbefore having all the relevant stakeholders review \nthe requirements. The reviewers should check that \nthe requirement statements (1) have bidirectional \ntraceability to the baselined stakeholder expecta-\ntions; (2) were formed using valid assumptions; and \n(3) are essential to, and consistent with, designing \nand realizing the appropriate product solution form \n\n\n\n6.2 Requirements Management\n\nNASA Systems Engineering Handbook ? 133\n\nthat will satisfy the applicable product-line life-cycle \nphase success criteria. \nDo Requirements Satisfy Stakeholders: 3. All relevant \nstakeholder groups identify and remove defects. \n\nRequirements validation results are often a deciding \nfactor in whether to proceed with the next process of Log-\nical Decomposition or Design Solution Definition. The \nproject team should be prepared to: (1) demonstrate that \nthe project requirements are complete and understand-\nable; (2) demonstrate that prioritized evaluation criteria \nare consistent with requirements and the operations and \nlogistics concepts; (3) confirm that requirements and \nevaluation criteria are consistent with stakeholder needs; \n(4) demonstrate that operations and architecture con-\ncepts support mission needs, goals, objectives, assump-\ntions, guidelines, and constraints; and (5) demonstrate \nthat the process for managing change in requirements is \nestablished, documented in the project information re-\npository, and communicated to stakeholders.\n\nManaging Requirement Changes\nThroughout Phases A and B, changes in requirements \nand constraints will occur. It is impera tive that all changes \nbe thoroughly evaluated to determine the impacts on the \narchitecture, design, interfaces, ConOps, and higher and \nlower level requirements. Performing functional and \nsensitivity analyses will en sure that the requirements are \nrealistic and evenly allocated. Rigorous requirements \nverification and validation ensure that the requirements \ncan be satisfied and conform to mission objectives. All \nchanges must be subjected to a review and approval cycle \nto maintain traceability and to ensure that the impacts \nare fully assessed for all parts of the system.\n\nOnce the requirements have been validated and re-\nviewed in the System Requirements Review they are \nplaced under formal configuration control. Thereafter, \nany changes to the requirements must be approved by \nthe Configuration Control Board (CCB). The systems \nengineer, project manager, and other key engineers usu-\nally participate in the CCB approval processes to assess \nthe impact of the change including cost, performance, \nprogrammatic, and safety. \n\nThe technical team should also ensure that the approved \nrequirements are communicated in a timely manner to \nall relevant people. Each project should have already es-\ntablished the mechanism to track and disseminate the \nlatest project information. Further information on Con-\n\nfiguration Management (CM) can be found in Sec-\ntion 6.5. \n\nKey Issues for Requirements Management\n\nRequirements Changes\nEffective management of requirements changes requires \na process that assesses the impact of the proposed \nchanges prior to approval and implementation of the \nchange. This is normally accomplished through the use \nof the Configuration Management Process. In order for \nCM to perform this function, a baseline configuration \nmust be documented and tools used to assess impacts \nto the baseline. Typical tools used to analyze the change \nimpacts are as follows:\n\nPerformance Margins: ?  This tool is a list of key perfor-\nmance margins for the system and the current status \nof the margin. For example, the propellant perfor-\nmance margin will provide the necessary propellant \navailable versus the propellant necessary to complete \nthe mission. Changes should be assessed for their im-\npact to performance margins.\nCM Topic Evaluators List:  ? This list is developed by \nthe project office to ensure that the appropriate per-\nsons are evaluating the changes and providing im-\npacts to the change. All changes need to be routed to \nthe appropriate individuals to ensure that the change \nhas had all impacts identified. This list will need to be \nupdated periodically.\nRisk System and Threats List: ?  The risk system can \nbe used to identify risks to the project and the cost, \nschedule, and technical aspects of the risk. Changes \nto the baseline can affect the consequences and like-\nlihood of identified risk or can introduce new risk to \nthe project. A threats list is normally used to identify \nthe costs associated with all the risks for the project. \nProject reserves are used to mitigate the appropriate \nrisk. Analyses of the reserves available versus the \nneeds identified by the threats list assist in the priori-\ntization for reserve use.\n\nThe process for managing requirements changes needs \nto take into account the distribution of information re-\nlated to the decisions made during the change process. \nThe Configuration Management Process needs to com-\nmunicate the requirements change decisions to the af-\nfected organizations. During a board meeting to approve \na change, actions to update documentation need to be \nincluded as part of the change package. These actions \n\n\n\n134 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nshould be tracked to ensure that affected documentation \nis updated in a timely manner.\n\nFeedback to the Requirements Baseline\nDuring development of the system components, it will \nbe necessary to provide feedback to the requirements. \nThis feedback is usually generated during the product \ndesign, validation, and verification processes. The feed-\nback to the project will include design implementation \nissues that impact the interfaces or operations of the \nsystem. In many cases, the design may introduce con-\nstraints on how the component can be operated, main-\ntained, or stored. This information needs to be commu-\nnicated to the project team to evaluate the impact to the \naffected system operation or architecture. Each system \ncomponent will optimize the component design and op-\neration. It is the systems engineering function to evaluate \nthe impact of this optimization at the component level to \nthe optimization of the entire system. \n\nRequirements Creep\n\u201cRequirements creep\u201d is the term used to describe the \nsubtle way that requirements grow imperceptibly during \nthe course of a project. The tendency for the set of re-\nquirements is to relentlessly increase in size during the \ncourse of development, resulting in a system that is more \nexpensive and complex than originally intended. Often \nthe changes are quite innocent and what appear to be \nchanges to a system are really enhancements in disguise. \n\nHowever, some of the requirements creep involves truly \nnew requirements that did not exist, and could not have \nbeen anticipated, during the Technical Requirements \nDefinition Process. These new requirements are the re-\nsult of evolution, and if we are to build a relevant system, \nwe cannot ignore them. \n\nThere are several techniques for avoiding or at least min-\nimizing requirements creep:\n\nIn the early requirements definition phase, flush out  ?\nthe conscious, unconscious, and undreamt-of re-\nquirements that might otherwise not be stated.\nEstablish a strict process for assessing requirement  ?\nchanges as part of the Configuration Management \nProcess. \nEstablish official channels for submitting change re- ?\nquests. This will determine who has the authority to \ngenerate requirement changes and submit them for-\nmally to the CCB (e.g., the contractor-designated rep-\n\nresentative, project technical leads, customer/science \nteam lead, or user). \nMeasure the functionality of each requirement change  ?\nrequest and assess its impact on the rest of the system. \nCompare this impact with the consequences of not \napproving the change. What is the risk if the change \nis not approved?\nDetermine if the proposed change can be accommo- ?\ndated within the fiscal and technical resource budgets. \nIf it cannot be accommodated within the established \nresource margins, then the change most likely should \nbe denied. \n\n6.2.1.3 Outputs\nTypical outputs from the requirements management ac-\ntivities are:\n\nRequirements Documents: ?  Requirements documents \nare submitted to the Configuration Management Pro-\ncess when the requirements are baselined. The official \ncontrolled versions of these documents are generally \nmaintained in electronic format within the require-\nments management tool that has been selected by the \nproject. In this way they are linked to the requirements \nmatrix with all of its traceable relationships.\nApproved Changes to the Requirements Baselines: ?  \nApproved changes to the requirements baselines are \nissued as an output of the Requirements Management \nProcess after careful assessment of all the impacts of \nthe requirements change across the entire product \nor system. A single change can have a far-reaching \nripple effect which may result in several requirement \nchanges in a number of documents.\nVarious Requirements Management Work Prod- ?\nucts: Requirements management work products are \nany reports, records, and undeliverable outcomes of \nthe Requirements Management Process. For example, \nthe bidirectional traceability status would be one of \nthe work products that would be used in the verifica-\ntion and validation reports.\n\n6.2.2 Requirements Management \nGuidance\n\n6.2.2.1 Requirements Management Plan\nThe technical team should prepare a plan for perform-\ning the requirements management activities. This plan is \nnormally part of the SEMP but also can stand alone. The \nplan should:\n\n\n\n6.2 Requirements Management\n\nNASA Systems Engineering Handbook ? 135\n\nIdentify the relevant stakeholders who will be involved  ?\nin the Requirements Management Process (e.g., those \nwho may be affected by, or may affect, the product as \nwell as the processes).\nProvide a schedule for performing the requirements  ?\nmanagement procedures and activities.\nAssign responsibility, authority, and adequate re- ?\nsources for performing the requirements manage-\nment activities, developing the requirements manage-\nment work products, and providing the requirements \nmanagement services defined in the activities (e.g., \nstaff, requirements management database tool, etc.).\nDefine the level of configuration management/data  ?\nmanagement control for all requirements manage-\nment work products. \nIdentify the training for those who will be performing  ?\nthe requirements management activities.\n\n6.2.2.2 Requirements Management Tools\nFor small projects and products, the requirements can \nusually be managed using a spreadsheet program. How-\never, the larger programs and projects require the use \nof one of the available requirements management tools. \nIn selecting a tool, it is important to define the project\u2019s \nprocedure for specifying how the requirements will be \n\norganized in the requirements management database \ntool and how the tool will be used. It is possible, given \nmodern requirements management tools, to create a \nrequirements management database that can store and \nsort requirements data in multiple ways according to the \nparticular needs of the technical team. The organization \nof the database is not a trivial exercise and has conse-\nquences on how the requirements data can be viewed for \nthe life of the project. Organize the database so that it \nhas all the views into the requirements information that \nthe technical team is likely to need. Careful consider-\nation should be given to how flowdown of requirements \nand bidirectional traceability will be represented in the \ndatabase. Sophisticated requirements management data-\nbase tools also have the ability to capture numerous re-\nquirement attributes in the tools\u2019 requirements matrix, \nincluding the requirements traceability and allocation \nlinks. For each requirement in the requirements matrix, \nthe verification method(s), level, and phase are docu-\nmented in the verification requirements matrix housed \nin the requirements management database tool (e.g., the \ntool associates the attributes of method, level, and phase \nwith each requirement). It is important to make sure that \nthe requirements management database tool is compat-\nible with the verification and validation tools chosen for \nthe project.\n\n\n\n136 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nThe management and control of interfaces is crucial to \nsuccessful programs or projects. Interface management \nis a process to assist in controlling product develop-\nment when efforts are divided among parties (e.g., Gov-\nernment, contractors, geographically diverse technical \nteams, etc.) and/or to define and maintain compliance \namong the products that must interoperate.\n\n6.3.1 Process Description\nFigure 6.3-1 provides a typical flow diagram for the In-\nterface Management Process and identifies typical in-\nputs, outputs, and activities to consider in addressing \ninterface management.\n\n6.3.1.1 Inputs\nTypical inputs needed to understand and address inter-\nface management would include the following:\n\nSystem Description: ?  This allows the design of the \nsystem to be explored and examined to determine \nwhere system interfaces exist. Contractor arrange-\nments will also dictate where interfaces are needed.\nSystem Boundaries: ?  Document physical boundaries, \ncomponents, and/or subsystems, which are all drivers \nfor determining where interfaces exist.\nOrganizational Structure: ?  Decide which organiza-\ntion will dictate interfaces, particularly when there is \nthe need to jointly agree on shared interface param-\n\n6.3 Interface Management\n\neters of a system. The program and project WBS will \nalso provide interface boundaries.\nBoards Structure: ?  The SEMP should provide insight \ninto organizational interface responsibilities and drive \nout interface locations.\nInterface Requirements: ?  The internal and external \nfunctional and physical interface requirements devel-\noped as part of the Technical Requirements Defini-\ntion Process for the product(s).\nInterface Change Requests: ?  These include changes \nresulting from program or project agreements or \nchanges on the part of the technical team as part of \nthe Technical Assessment Process.\n\n6.3.1.2 Process Activities\nDuring project Formulation, the ConOps of the product \nis analyzed to identify both external and internal inter-\nfaces. This analysis will establish the origin, destination, \nstimuli, and special characteristics of the interfaces that \nneed to be documented and maintained. As the system \nstructure and architecture emerges, interfaces will be \nadded and existing interfaces will be changed and must \nbe maintained. Thus, the Interface Management Process \nhas a close relationship to other areas, such as require-\nments definition and configuration management during \nthis period. Typically, an Interface Working Group \n(IWG) establishes communication links between those \n\nFrom project and\nTechnical Assessment\n\nProcess\n\nFrom user or program and\nsystem design processes \n\nTo Technical Data\nManagement Process\n\nTo Con?guration\nManagement Process \n\nInterface\nRequirements \n\nInterface Control\nDocuments \n\nApproved Interface\nRequirement\n\nChanges\n\nInterface Management\nWork Products\n\nPrepare or update interface\nmanagement procedures \n\nConduct interface management during\nsystem design activities for each WBS-\n\nlike model in the system structure\n\nConduct interface control \n\nConduct interface management during\nproduct integration activities  \n\nCapture work products from interface\nmanagement activities \n\nInterface\nChanges \n\nFigure 6.3?1 Interface Management Process\n\n\n\n6.3 Interface Management\n\nNASA Systems Engineering Handbook ? 137\n\nresponsible for interfacing systems, end products, en-\nabling products, and subsystems. The IWG has the re-\nsponsibility to ensure accomplishment of the planning, \nscheduling, and execution of all interface activities. An \nIWG is typically a technical team with appropriate tech-\nnical membership from the interfacing parties (e.g., the \nproject, the contractor, etc.). \n\nDuring product integration, interface management ac-\ntivities would support the review of integration and as-\nsembly procedures to ensure interfaces are properly \nmarked and compatible with specifications and interface \ncontrol documents. The interface management process \nhas a close relationship to verification and validation. In-\nterface control documentation and approved interface \nrequirement changes are used as inputs to the Product \nVerification Process and the Product Validation Process, \nparticularly where verification test constraints and inter-\nface parameters are needed to set the test objectives and \ntest plans. Interface requirements verification is a critical \naspect of the overall system verification.\n\n6.3.1.3 Outputs\nTypical outputs needed to capture interface management \nwould include interface control documentation. This is \nthe documentation that identifies and captures the inter-\nface information and the approved interface change re-\nquests. Types of interface documentation include the In-\nterface Requirements Document (IRD), Interface Control \nDocument/Drawing (ICD), Interface Definition Docu-\nment (IDD), and Interface Control Plan (ICP). These out-\nputs will then be maintained and approved using the Con-\nfiguration Management Process and become a part of the \noverall technical data package for the project.\n\n6.3.2 Interface Management Guidance\n\n6.3.2.1 Interface Requirements Document\nAn interface requirement defines the functional, perfor-\nmance, electrical, environmental, human, and physical \nrequirements and constraints that exist at a common \nboundary between two or more functions, system ele-\nments, configuration items, or systems. Interface require-\nments include both logical and physical interfaces. They \ninclude, as necessary, physical measurements, defini-\ntions of sequences of energy or information transfer, and \nall other significant interactions between items. For ex-\nample, communication interfaces involve the movement \nand transfer of data and information within the system, \nand between the system and its environment. Proper \n\nevaluation of communications requirements involves \ndefinition of both the structural components of commu-\nnications (e.g., bandwidth, data rate, distribution, etc.) \nand content requirements (what data/information is be-\ning communicated, what is being moved among the sys-\ntem components, and the criticality of this information \nto system functionality). Interface requirements can be \nderived from the functional allocation if function inputs \nand outputs have been defined. For example: \n\nIf function F1 outputs item A to function F2, and  ?\nFunction F1 is allocated to component C1, and  ?\nFunction F2 is allocated to component C2,  ?\nThen there is an implicit requirement that the inter- ?\nface between components C1 and C2 pass item A, \nwhether item A is a liquid, a solid, or a message con-\ntaining data.\n\nThe IRD is a document that defines all physical, func-\ntional, and procedural interface requirements between \ntwo or more end items, elements, or components of a \nsystem and ensures project hardware and software com-\npatibility. An IRD is composed of physical and func-\ntional requirements and constraints imposed on hard-\nware configuration items and/or software configuration \nitems. The purpose of the IRD is to control the interfaces \nbetween interrelated components of the system under \ndevelopment, as well as between the system under de-\nvelopment and any external systems (either existing or \nunder development) that comprise a total architecture. \nInterface requirements may be contained in the SRD \nuntil the point in the development process where the in-\ndividual interfaces are determined. IRDs are useful when \nseparate organizations are developing components of the \nsystem or when the system must levy requirements on \nother systems outside program/project control. During \nboth Phase A and Phase B, multiple IRDs are drafted for \ndifferent levels of interfaces. By SRR, draft IRDs would be \ncomplete for system-to-external-system interfaces (e.g., \nthe shuttle to the International Space Station), and seg-\nment-to-segment interfaces (e.g., the shuttle to the launch \npad). An IRD generic outline is described in Appendix L.\n\n6.3.2.2 Interface Control Document or \nInterface Control Drawing\n\nAn interface control document or drawing details the \nphysical interface between two system elements, in-\ncluding the number and types of connectors, electrical \nparameters, mechanical properties, and environmental \nconstraints. The ICD identifies the design solution to the \n\n\n\n138 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\ninterface requirement. ICDs are useful when separate \norganizations are developing design solutions to be ad-\nhered to at a particular interface. \n\n6.3.2.3 Interface Definition Document \nAn IDD is a unilateral document controlled by the end-\nitem provider, and it basically provides the details of the \ninterface for a design solution that is already established. \nThis document is sometimes referred to as a \u201cone-sided \nICD.\u201d The user of the IDD is provided connectors, elec-\ntrical parameters, mechanical properties, environmental \nconstraints, etc., of the existing design. The user must \nthen design the interface of the system to be compatible \nwith the already existing design interface. \n\n6.3.2.4 Interface Control Plan\nAn ICP should be developed to address the process for \ncontrolling identified interfaces and the related interface \ndocumentation. Key content for the ICP is the list of in-\nterfaces by category and who owns the interface. The \nICP should also address the configuration control forum \nand mechanisms to implement the change process (e.g., \nPreliminary Interface Revision Notice (PIRN)/Interface \nRevision Notice (IRN)) for the documents.\n\nTypical Interface Management Checklist\n\nUse the generic outline provided when developing  ?\nthe IRD. Define a \u201creserved\u201d placeholder if a para-\ngraph or section is not applicable.\n\nEnsure that there are two or more specifications  ?\nthat are being used to serve as the parent for the \nIRD specific requirements. \n\nEnsure that \u201cshall\u201d statements are used to define  ?\nspecific requirements. \n\nEach organization must approve and sign the IRD.  ?\n\nA control process must be established to manage  ?\nchanges to the IRD. \n\nCorresponding ICDs are developed based upon the  ?\nrequirements in the IRD. \n\nConfirm connectivity between the interface re- ?\nquirements and the Product Verification and Prod-\nuct Validation Processes.\n\nDefine the SEMP content to address interface man- ?\nagement.\n\nEach major program or project should include an  ?\nICP to describe the how and what of interface man-\nagement products.\n\n\n\nNASA Systems Engineering Handbook ? 139\n\nThe Technical Risk Management Process is one of the \ncrosscutting technical management processes. Risk is de-\nfined as the combination of (1) the probability that a pro-\ngram or project will experience an undesired event and \n(2) the consequences, impact, or severity of the unde-\nsired event, were it to occur. The undesired event might \ncome from technical or programmatic sources (e.g., a \ncost overrun, schedule slippage, safety mishap, health \nproblem, malicious activities, environmental impact, \n\n6.4 Technical Risk Management\n\nor failure to achieve a needed scientific or technolog-\nical objective or success criterion). Both the probability \nand consequences may have associated uncertainties. \nTechnical risk management is an organized, systematic \nrisk-informed decisionmaking discipline that proac-\ntively identifies, analyzes, plans, tracks, controls, com-\nmunicates, documents, and manages risk to increase \nthe likelihood of achieving project goals. The Technical \nRisk Management Process focuses on project objectives, \n\nKey Concepts in Technical Risk Management \n\nRisk: ?  Risk is a measure of the inability to achieve overall program objectives within defined cost, schedule, and tech-\nnical constraints and has two components: (1) the probability of failing to achieve a particular outcome and (2) the \nconsequences/impacts of failing to achieve that outcome. \n\nCost Risk: ?  This is the risk associated with the ability of the program/project to achieve its life-cycle cost objectives and \nsecure appropriate funding. Two risk areas bearing on cost are (1) the risk that the cost estimates and objectives are \nnot accurate and reasonable and (2) the risk that program execution will not meet the cost objectives as a result of a \nfailure to handle cost, schedule, and performance risks.\n\nSchedule Risk:  ? Schedule risks are those associated with the adequacy of the time estimated and allocated for the de-\nvelopment, production, implementation, and operation of the system. Two risk areas bearing on schedule risk are (1) \nthe risk that the schedule estimates and objectives are not realistic and reasonable and (2) the risk that program exe-\ncution will fall short of the schedule objectives as a result of failure to handle cost, schedule, or performance risks.\n\nTechnical Risk:  ? This is the risk associated with the evolution of the design and the production of the system of inter-\nest affecting the level of performance necessary to meet the stakeholder expectations and technical requirements. \nThe design, test, and production processes (process risk) influence the technical risk and the nature of the product as \ndepicted in the various levels of the PBS (product risk).\n\nProgrammatic Risk: ?  This is the risk associated with action or inaction from outside the project, over which the proj-\nect manager has no control, but which may have significant impact on the project. These impacts may manifest \nthemselves in terms of technical, cost, and/or schedule. This includes such activities as: International Traffic in Arms \nRequirements (ITAR), import/export control, partner agreements with other domestic or foreign organizations, con-\ngressional direction or earmarks, Office of Management and Budget direction, industrial contractor restructuring, ex-\nternal organizational changes, etc.\n\nHazard Versus Risk:  ? Hazard is distinguished from risk. A hazard represents a potential for harm, while risk includes con-\nsideration of not only the potential for harm, but also the scenarios leading to adverse outcomes and the likelihood of \nthese outcomes. In the context of safety, \u201crisk\u201d considers the likelihood of undesired consequences occurring.\n\nProbabilistic Risk Assessment (PRA): ?  PRA is a scenario-based risk assessment technique that quantifies the likeli-\nhoods of various possible undesired scenarios and their consequences, as well as the uncertainties in the likelihoods \nand consequences. Traditionally, design organizations have relied on surrogate criteria such as system redundancy \nor system-level reliability measures, partly because the difficulties of directly quantifying actual safety impacts, as op-\nposed to simpler surrogates, seemed insurmountable. Depending on the detailed formulation of the objectives hi-\nerarchy, PRA can be applied to quantify Technical Performance Measures (TPMs) that are very closely related to fun-\ndamental objectives (e.g., Probability of Loss of Crew (P(LOC))). PRA focuses on the development of a comprehensive \nscenario set, which has immediate application to identify key and candidate contributors to risk. In all but the simplest \nsystems, this requires the use of models to capture the important scenarios, to assess consequences, and to system-\natically quantify scenario likelihoods. These models include reliability models, system safety models, simulation mod-\nels, performance models, and logic models.\n\n\n\n140 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nbringing to bear an analytical basis for risk management \ndecisions and the ensuing management activities, and a \nframework for dealing with uncertainty.\n\nStrategies for risk management include transferring per-\nformance risk, eliminating the risk, reducing the likeli-\nhood of undesired events, reducing the negative effects \nof the risk (i.e., reducing consequence severity), reducing \nuncertainties if warranted, and accepting some or all of \nthe consequences of a particular risk. Once a strategy \nis selected, technical risk management ensures its suc-\ncessful implementation through planning and imple-\nmentation of the risk tracking and controlling activities. \nTechnical risk management focuses on risk that relates \nto technical performance. However, management of \ntechnical risk has an impact on the nontechnical risk by \naffecting budget, schedule, and other stakeholder expec-\ntations. This discussion of technical risk management is \napplicable to technical and nontechnical risk issues, but \nthe focus of this section is on technical risk issues.\n\n6.4.1 Process Description\nFigure 6.4-1 provides a typical flow diagram for the Tech-\nnical Risk Management Process and identifies typical in-\nputs, activities, and outputs to consider in addressing \ntechnical risk management. \n\n6.4.1.1 Inputs\n\nThe following are typical inputs to technical risk man-\nagement:\n\nPlans and Policies: ?  Risk management plan, risk re-\nporting requirements, systems engineering manage-\nment plan, form of technical data products, and policy \ninput to metrics and thresholds.\nTechnical Inputs: ?  Technical performance measures, \nprogram alternatives to be assessed, technical issues, \nand current program baseline.\nInputs Needed for Risk Analysis of Alternatives: ?  \nDesign information and relevant experience data.\n\n6.4.1.2 Process Activities\n\nTechnical risk management is an iterative process that con-\nsiders activity requirements, constraints, and priorities to:\n\nIdentify and assess the risks associated with the im- ?\nplementation of technical alternatives;\nAnalyze, prioritize, plan, track and control risk and  ?\nthe implementation of the selected alternative; \nPlan, track, and control the risk and the implementa- ?\ntion of the selected alternative; \nImplement contingency action plans as triggered; ?\n\nFigure 6.4?1 Technical Risk Management Process\n\nFrom Technical\nAssessment and\n\nDecision Analysis\nProcesses  \n\nFrom project and all\ntechnical processes\n\nTo project and Technical\nData Management\n\nProcess \n\nTo Technical Data\nManagement Process\n\nTo Technical\nPlanning Process \n\nTechnical Risk Status\nMeasurements \n\nTechnical Risk\nReporting\n\nRequirements \n\nTechnical Risk\nIssues\n\nTechnical Risk\nMitigation and/or\n\nContingency Actions \n\nTechnical Risk\nReports \n\nWork Products of\nTechnical Risk\nManagement \n\nFrom project and\nTechnical Assessment\n\nProcess \n\nFrom project\n\nProject Risk\nManagement Plan \n\nPrepare a strategy to conduct technical\nrisk management \n\nIdentify technical risks\n\nPrepare for technical risk mitigation\n\nConduct technical risk assessment\n\nMonitor the status of each technical\nrisk periodically \n\nImplement technical risk mitigation and\ncontingency action plans as triggered \n\nCapture work products from technical\nrisk management activities\n\n\n\n6.4 Technical Risk Management\n\nNASA Systems Engineering Handbook ? 141\n\nCommunicate, deliberate, and document work prod- ?\nucts and the risk; and\nIterate with previous steps in light of new information  ?\nthroughout the life cycle.\n\n6.4.1.3 Outputs\nFollowing are key technical risk outputs from activities:\n\nPlans and Policies: ?  Baseline-specific plan for tracking \nand controlling risk\nTechnical Outputs: ?  Technical risk mitigation or con-\ntingency actions and tracking results, status findings, \nand emergent issues\nOutputs from Risk Analysis of Alternatives:  ? Identi-\nfied, analyzed, prioritized, and assigned risk; and risk \nanalysis updates\n\n6.4.2 Technical Risk Management Guidance\nA widely used conceptu-\nalization of risk is the sce-\nnarios, likelihoods, and con-\nsequences concept as shown \nin Figures 6.4-2 and 6.4-3. \n\nThe scenarios, along with \nconsequences, likelihoods, \nand associated uncertain-\nties, make up the complete \nrisk triplet (risk as a set of \ntriplets\u2014scenarios, likeli-\nhoods, consequences). The \n\ntriplet concept applies in principle to all risk types, and \nincludes the information needed for quantifying simpler \nmeasures, such as expected consequences. Estimates of \nexpected consequences (probability or frequency multi-\nplied by consequences) alone do not adequately inform \ntechnical decisions. Scenario-based analyses provide \nmore of the information that risk-informed decisions \nneed. For example, a rare but severe risk contributor \nmay warrant a response different from that warranted \nby a frequent, less severe contributor, even though both \nhave the same expected consequences. In all but the sim-\nplest systems, this requires the use of detailed models to \ncapture the important scenarios, to assess consequences, \nand to systematically quantify scenario likelihoods. For \nadditional information on probabilistic risk assessments, \nrefer to NPR 8705.3, Probabilistic Risk Assessment Proce-\ndures Guide for NASA Managers and Practitioners.\n\nFigure 6.4?2 Scenario?based modeling of hazards\n\nAccident Mitigation Layers\n\nSafety Adverse\nConsequence \n\nHazards\n\nSystem does not\nlimit the severity\nof consequence\n\nAccident Prevention Layers  \n\nInitiating\nEvent\n\nAccident\n(Mishap) \n\nSystem does not\ncompensate\n\n(failure of controls)\n\nHazards\n\nRISK ? \n\nStructure of Scenario\nLikelihood and\nIts Uncertainty\n\nConsequence Severity\nand Its Uncertainty\n\nStructure of Scenario\nLikelihood and\nIts Uncertainty\n\nConsequence Severity\nand Its Uncertainty\n\nStructure of Scenario\nLikelihood and\nIts Uncertainty\n\nConsequence Severity\nand Its Uncertainty\n\nFigure 6.4?3 Risk as a set of triplets \n\n\n\n142 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\n6.4.2.1 Role of Continuous Risk Management \nin Technical Risk Management \n\nContinuous Risk Management (CRM) is a widely used \ntechnique within NASA, initiated at the beginning and \ncontinuing throughout the program life cycle to mon-\nitor and control risk. It is an iterative and adaptive pro-\ncess, which promotes the successful handling of risk. \nEach step of the paradigm builds on the previous step, \nleading to improved designs and processes through the \nfeedback of information generated. Figure 6.4-4 suggests \nthis adaptive feature of CRM. \n\nA brief overview of CRM is provided below for reference:\nIdentify: ?  Identify program risk by identifying sce-\nnarios having adverse consequences (deviations from \nprogram intent). CRM addresses risk related to safety, \ntechnical performance, cost, schedule, and other risk \nthat is specific to the program. \nAnalyze: ?  Estimate the likelihood and consequence \ncomponents of the risk through analysis, including \nuncertainty in the likelihoods and consequences, and \nthe timeframes in which risk mitigation actions must \nbe taken.\nPlan: ?  Plan the track and control actions. Decide what \nwill be tracked, decision thresholds for corrective ac-\ntion, and proposed risk control actions. \nTrack: ?  Track program observables relating to TPMs \n(performance data, schedule variances, etc.), mea-\nsuring how close the program performance is com-\npared to its plan.\nControl:  ? Given an emergent risk issue, execute the \nappropriate control action and verify its effectiveness.\nCommunicate, Deliberate, and Document:  ? This is \nan element of each of the previous steps. Focus on un-\n\nderstanding and communicating all risk information \nthroughout each program phase. Document the risk, \nrisk control plans, and closure/acceptance rationale. \nDeliberate on decisions throughout the CRM process.\n\n6.4.2.2 The Interface Between CRM and Risk-\nInformed Decision Analysis\n\nFigure 6.4-5 shows the interface between CRM and risk-\ninformed decision analysis. (See Subsection 6.8.2 for \nmore on the Decision Analysis Process.) The following \nsteps are a risk-informed Decision Analysis Process:\n\nFormulate the objectives hierarchy and TPMs.1. \nPropose and identify decision alternatives. Alterna-2. \ntives from this process are combined with the alter-\nnatives identified in the other systems engineering \nprocesses, including design solution, verification, \nand validation as well as production.\nPerform risk analysis and rank decision alterna-3. \ntives.\nEvaluate and recommend decision alternative.4. \nTrack the implementation of the decision.5. \n\nThese steps support good decisions by focusing first on \nobjectives, next on developing decision alternatives with \nthose objectives clearly in mind, and using decision al-\nternatives that have been developed under other systems \nengineering processes. The later steps of the decision \nanalysis interrelate heavily with the Technical Risk Man-\nagement Process, as indicated in Figure 6.4-5.\n\nThe risk analysis of decision alternatives (third box) not \nonly guides selection of a preferred alternative, it also car-\nries out the \u201cidentify\u201d and \u201canalyze\u201d steps of CRM. Selec-\ntion of a preferred alternative is based in part on an un-\nderstanding of the risks associated with that alternative. \nAlternative selection is followed immediately by a plan-\nning activity in which key implementation aspects are \naddressed, namely, risk tracking and control, including \nrisk mitigation if necessary. Also shown conceptually on \nFigure 6.4-5 is the interface between risk management \nand other technical and programmatic processes.\n\nRisk Analysis, Performing Trade Studies and \nRanking\nThe goal of this step is to carry out the kinds and amounts \nof analysis needed to characterize the risk for two pur-\nposes: ranking risk alternatives, and performing the \n\u201cidentify\u201d and \u201canalyze\u201d steps of CRM. \n\nFigure 6.4?4 Continuous risk management\n\nCommunicate,\nDeliberate, and\n\nDocument\n\n \n\n    \n   C\n\non\ntr\n\nol\n     \n\n       \n   Identify                  Analyze \n\n            Track             Pl\nan\n\n     \n    \n\n    \n    \n\n    \n    \n\n    \n    \n\n    \n     \n\n   \n\n\n\n6.4 Technical Risk Management\n\nNASA Systems Engineering Handbook ? 143\n\nTo support ranking, trade studies may be performed. \nTPMs that can affect the decision outcome are quanti-\nfied including uncertainty as appropriate. \n\nTo support the \u201cidentify\u201d and \u201canalyze\u201d steps of CRM, \nthe risk associated with the preferred alternative is ana-\nlyzed in detail. Refer to Figure 6.4-6. Risk analysis can \ntake many forms, ranging from qualitative risk identifi-\ncation (essentially scenarios and consequences, without \nperforming detailed quantification of likelihood using \ntechniques such as Failure Mode and Effects Analysis \n(FMEA) and fault trees), to highly quantitative methods \nsuch as PRA. The analysis stops when the technical \ncase is made; if simpler, more qualitative methods suf-\nfice, then more detailed methods need not be applied. \nThe process is then identified, planned for, and continu-\nously checked. Selection and application of appropriate \nmethods is discussed as follows.\n\n6.4.2.3 Selection and Application of \nAppropriate Risk Methods\n\nThe nature and context of the problem, and the specific \nTPMs, determine the methods to be used. In some proj-\nects, qualitative methods are adequate for making deci-\nsions; in others, these methods are not precise enough to \nappropriately characterize the magnitude of the problem, \nor to allocate scarce risk reduction resources. The tech-\nnical team needs to decide whether risk identification \nand judgment-based characterization are adequate, or \nwhether the improved quantification of TPMs through \nmore detailed risk analysis is justified. In making that de-\ntermination, the technical team must balance the cost of \nrisk analysis against the value of the additional informa-\ntion to be gained. The concept of \u201cvalue of information\u201d \nis central to making the determination of what analysis \nis appropriate and to what extent uncertainty needs to be \nquantified. \n\nCRM Process\n\nTechnical Risk Management\n\nFormulation of Objectives\nHierarchy and TPMs\n\nRisk-Informed Decision\nAnalysis\n\nDecisionmaking\nand\n\nImplementation\nof Decision\nAlternative\n\nProposing and/or\nIdentifying Decision\n\nAlternatives\n\nRisk Analysis of Decision\nAlternatives, Performing\n\nTrade Studies and Ranking\n\nDeliberating and\nRecommending a\n\nDecision Alternative\n\nRisk Analysis of Decision\nAlternatives, Performing\n\nTrade Studies and Ranking\n\nStakeholder\nExpectation,\n\nRequirements\nDefinition/\n\nManagement\n\nDesign Solution,\nTechnical Planning\n\nDesign Solution,\nTechnical Planning,\n\nDecision Analysis\n\nTechnical Planning,\nDecision Analysis\n\nDecision Analysis,\nLessons Learned,\n\nKnowledge\nManagement \n\nTracking and Controlling\nPerformance Deviations\n\nCommunicate,\nDeliberate, and\n\nDocument\n\n \n\n    \n   C\n\non\ntr\n\nol\n     \n\n       \n   Identify                  Analyze \n\n            Track             Pl\nan\n\n     \n    \n\n    \n    \n\n    \n    \n\n    \n    \n\n    \n     \n\n   \n\nFigure 6.4?5 The interface between CRM and risk?informed decision analysis\n\n\n\n144 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nA review of the lessons learned files, data, and reports \nfrom previous similar projects can produce insights and \ninformation for hazard identification on a new project. \nThis includes studies from similar systems and historical \ndocuments, such as mishap files and near-miss reports. \nThe key to applying this technique is in recognizing \nwhat aspects of the old projects and the current project \nare analogous, and what data from the old projects are \nrelevant to the current project. In some cases the use of \nquantitative methods can compensate for limited avail-\nability of information because these techniques pull the \nmost value from the information that is available.\n\nTypes of Risk\nAs part of selecting appropriate risk analysis methods, \nit is useful to categorize types of risk. Broadly, risk can \nbe related to cost, schedule, and technical performance. \nMany other categories exist, such as safety, organiza-\ntional, management, acquisition, supportability, polit-\nical, and programmatic risk, but these can be thought of \nas subsets of the broad categories. For example, program-\nmatic risk refers to risk that affects cost and/or schedule, \nbut not technical. \n\nIn the early stages of a risk analysis, it is typically nec-\nessary to screen contributors to risk to determine the \ndrivers that warrant more careful analysis. For this pur-\npose, conservative bounding approaches may be ap-\npropriate. Overestimates of risk significance will be \ncorrected when more detailed analysis is performed. \nHowever, it can be misleading to allow bounding esti-\nmates to drive risk ranking. For this reason, analysis will \ntypically iterate on a problem, beginning with screening \nestimates, using these to prioritize subsequent analysis, \nand moving on to a more defensible risk profile based on \ncareful analysis of significant contributors. This is part of \nthe iteration loop shown in Figure 6.4-6.\n\nQualitative Methods\nCommonly used qualitative methods accomplish the \nfollowing:\n\nHelp identify scenarios that are potential risk contrib- ?\nutors,\nProvide some input to more quantitative methods,  ?\nand\nSupport judgment-based quantification of TPMs. ?\n\nIs the\nranking/\n\ncomparison\nrobust?\n\nAnalyze\n\nPreliminary Risk\n& Performance\nMeasurement\n\nResults\n\nIdentify\n\nIdentify\n\nAnalyze\n\nDecision\nAlternatives for\n\nAnalysis \n\nIteration\n\nNet\nbene?cial to reduce\n\nuncertainty?\n\nDeliberation and\nRecommending a\n\nDecision Alternative \n\nYes\n\nNo\n\nNo\n\nScoping &\nDetermination of\n\nMethods to Be\nUsed \n\nAdditional Uncertainty Reduction if Necessary per Stakeholders\n\nRisk &\nPerformance\n\nMeasurement\nResults\n\nRisk Analysis\nTechniques \n\nSp\nec\n\ntr\num\n\n o\nf\n\nA\nva\n\nila\nbl\n\ne\nTe\n\nch\nni\n\nqu\nes\n\nExamples of Decisions \n? Architecture A vs. Architecture B vs. Architecture C ? Making changes to existing systems\n? Extending the life of existing systems ? Responding to operational occurrences in real time\n? Contingency Plan A vs. Contingency Plan B ? Technology A vs. Technology B\n? Changing requirements ? Prioritization\n? Launch or no launch\n\nQualitative\nTechniques \n\nQuantitative\nTechniques \n\nFigure 6.4?6 Risk analysis of decision alternatives\n\n\n\n6.4 Technical Risk Management\n\nNASA Systems Engineering Handbook ? 145\n\nThese qualitative methods are discussed briefly below.\n\nRisk Matrices\n\u201cNxM\u201d (most commonly 5x5) risk matrices provide as-\nsistance in managing and communicating risk. (See Fig-\nure 6.4-7.) They combine qualitative and semi-quanti-\ntative measures of likelihood with similar measures of \nconsequences. The risk matrix is not an assessment tool, \nbut can facilitate risk discussions. Specifically, risk ma-\ntrices help to:\n\nTrack the status and effects of risk-handling efforts,  ?\nand\nCommunicate risk status information.  ?\n\nWhen ranking risk, it is important to use a common \nmethodology. Different organizations, and sometimes \nprojects, establish their own format. This can cause con-\n\nfusion and miscommunication. So before using a rank-\ning system, the definitions should be clearly established \nand communicated via a legend or some other method. \nFor the purposes of this handbook, a definition widely \nused by NASA, other Government organizations, and \nindustry is provided.\n\nLow (Green) Risk:  ? Has little or no potential for in-\ncrease in cost, disruption of schedule, or degrada-\ntion of performance. Actions within the scope of the \nplanned program and normal management attention \nshould result in controlling acceptable risk.\nModerate (Yellow) Risk:  ? May cause some increase in \ncost, disruption of schedule, or degradation of per-\n\nExample Sources of Risk\n\nIn the \u201cidentify\u201d activity, checklists such as this can \nserve as a reminder to analysts regarding areas in \nwhich risks have been identified previously. \n\nUnrealistic schedule estimates or allocation ?\n\nUnrealistic cost estimates or budget allocation ?\n\nInadequate staffing or skills ?\n\nUncertain or inadequate contractor capability ?\n\nUncertain or inadequate vendor capability ?\n\nInsufficient production capacity ?\n\nOperational hazards ?\n\nIssues, hazards, and vulnerabilities that could ad- ?\nversely affect the program\u2019s technical effort\n\nUnprecedented efforts without estimates ?\n\nPoorly defined requirements ?\n\nNo bidirectional traceability of requirements ?\n\nInfeasible design ?\n\nInadequate configuration management ?\n\nUnavailable technology ?\n\nInadequate test planning ?\n\nInadequate quality assurance ?\n\nRequirements prescribing nondevelopmental prod- ?\nucts too low in the product tree\n\nLack of concurrent development of enabling prod- ?\nucts for deployment, training, production, opera-\ntions, support, or disposal\n\nLi\nke\n\nlih\noo\n\nd\n\nConsequences\n1\n\n5\n\n4\n\n3\n\n1\n\n2\n\nHIGH\n\nLOW\n\n2 3 54\n\nMODERATE\n\nFigure 6.4?7 Risk matrix\n\nLimitations of Risk Matrices\n\nInteraction between risks is not considered. Each  ?\nrisk is mapped onto the matrix individually. (These \nrisks can be related to each item using FMECA or a \nfault tree.)\n\nInability to deal with aggregate risks (i.e., total risk). ?\n\nInability to represent uncertainties. A risk is as- ?\nsumed to exist within one likelihood range and \nconsequence range, both of which are assumed to \nbe known.\n\nFixed tradeoff between likelihood and conse- ?\nquence. Using the standardized 5x5 matrix, the sig-\nnificance of different levels of likelihood and conse-\nquence are fixed and unresponsive to the context \nof the program.\n\n\n\n146 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nformance. Special action and management attention \nmay be required to handle risk.\nHigh (Red) Risk:  ? Likely to cause significant increase \nin cost, disruption of schedule, or degradation of per-\nformance. Significant additional action and high-pri-\nority management attention will be required to handle \nrisk.\n\nFMECAs, FMEAs, and Fault Trees\nFMEA; Failure Modes, Effects, and Criticality Analysis \n(FMECA); and fault trees are methodologies designed to \nidentify potential failure modes for a product or process, \nto assess the risk associated with those failure modes, to \nrank the issues in terms of importance, and to identify \nand carry out corrective actions to address the most se-\nrious concerns. These methodologies focus on the hard-\nware components as well as processes that make up the \nsystem. According to MIL-STD-1629, Failure Mode and \nEffects Analysis, FMECA is an ongoing procedure by \nwhich each potential failure in a system is analyzed to \ndetermine the results or effects thereof on the system, \nand to classify each potential failure mode according to \nits consequence severity. A fault tree evaluates the com-\nbinations of failures that can lead to the top event of in-\nterest. (See Figure 6.4-8.)\n\nQuantitative and Communication Methods\n\nPRA is a comprehensive, structured, and logical analysis \nmethod aimed at identifying and assessing risks in com-\nplex technological systems for the purpose of cost-effec-\ntively improving their safety and performance. \n\nRisk management involves prevention of (reduction of \nthe frequency of) adverse scenarios (ones with undesir-\nable consequences) and promotion of favorable scenarios. \nThis requires understanding the elements of adverse sce-\nnarios so that they can be prevented and the elements of \nsuccessful scenarios so that they can be promoted. \n\nPRA quantifies risk metrics. \u201cRisk metric\u201d refers to the \nkind of quantities that might appear in a decision model: \nsuch things as the frequency or probability of conse-\nquences of a specific magnitude or perhaps expected \nconsequences. Risk metrics of interest for NASA include \nprobability of loss of vehicle for some specific mission \ntype, probability of mission failure, and probability of \nlarge capital loss. Figures of merit such as system failure \nprobability can be used as risk metrics, but the phrase \n\u201crisk metric\u201d ordinarily suggests a higher level, more \n\nconsequence-oriented figure of merit. The resources \nneeded for PRA are justified by the importance of the \nconsequences modeled or until the cost in time and re-\nsources of further analysis is no longer justified by the \nexpected benefits. \n\nThe NASA safety and risk directives determine the scope \nand the level of rigor of the risk assessments. NPR 8715.3, \nNASA General Safety Program Requirements assigns the \nproject a priority ranking based on its consequence cate-\ngory and other criteria. NPR 8705.5, Probabilistic Risk As-\n\nSingular event\n\nEvent that results from a\ncombination of singular\nevents\n\n\u201cAnd\u201d logic symbol\n\n\u201cOr\u201d logic symbol\n\nLegend:\n\nT\n\nE?\n\nA\n\nC\n\nI\n\nF\n\nG H\n\nBasic\nFailure Event\n\nIntermediate\nFailure\nEvent\n\nTop\nEvent\n\nLeak Not Detected\n\nController Fails\n\nPressure\nTransducer 1\nFails\n\nE?\n\nB E?\n\nE?\n\nE?\n\nFigure 6.4?8 Example of a fault tree\n\n\n\n6.4 Technical Risk Management\n\nNASA Systems Engineering Handbook ? 147\n\nsessment (PRA) Procedures for NASA Programs and Proj-\nects then determines the scope and the level of rigor and \ndetails for the assessment based on the priority ranking \nand the level of design maturity.\n\nQuantification\nTPMs are quantified for each alternative and used to \nquantify an overall performance index or an overall \nmeasure of effectiveness for each alternative. These re-\nsults are then used for ranking alternatives.\n\nBounding approaches are often used for initial screening \nof possible risk contributors. However, realistic assess-\nments must ultimately be performed on the risk drivers. \nBounding approaches are inappropriate for ranking al-\nternatives because they bias each TPM in which they are \napplied, and are very difficult to do at a quantitatively \nconsistent level from one analysis to the next. \n\nBecause different tools employ different simplifications \nand approximations, it is difficult to compare analysis \nresults in a consistent manner if they are based on dif-\nferent tools or done by different analysts. These sources \nof inconsistency need to be considered when the work \nis planned and when the results are applied. Vetting risk \nand TPM results with these factors in mind is one benefit \nof deliberation (discussed below).\n\nConsideration of Uncertainty Reduction \nMeasures \n\nIn some cases, the preliminary ranking of alternatives \nwill not be robust. A \u201crobust\u201d ranking is one that is not \nsensitive to small changes in model parameters or as-\n\nsumptions. As an example, suppose that differences in \nTPMs of different decision alternatives are sufficiently \nsmall that variations of key parameters within the stated \nuncertainty bounds could change the ranking. This \ncould arise in a range of decision situations, including \narchitecture decisions and risk management decisions \nfor a given architecture. In the latter case, the alternatives \nresult in different risk mitigation approaches. \n\nIn such cases, it may be worthwhile to invest in work to \nreduce uncertainties. Quantification of the \u201cvalue of infor-\nmation\u201d can help the decisionmaker determine whether \nuncertainty reduction is an efficient use of resources.\n\nDeliberation and Recommendation of Decision \nAlternative\n\nDeliberation \nDeliberation is recommended in order to make use of \ncollective wisdom to promote selection of an alterna-\ntive for actual implementation, or perhaps, in the case \nof complex and high-stakes decisions, to recommend a \nfinal round of trade studies or uncertainty reduction ef-\nforts, as suggested by the analysis arrow in Figure 6.4-9.\n\nCapturing the Preferred Alternative and the \nBasis for Its Selection\n\nDepending on the level at which this methodology is \nbeing exercised (project level, subtask level, etc.), the \ntechnical team chooses an alternative, basing the choice \non deliberation to the extent appropriate. The decision \nitself is made by appropriate authority inside of the sys-\ntems engineering processes. The purpose of calling out \n\nFigure 6.4?9 Deliberation\n\nAnalysis\n\nNo No\n\nYes\n\nPresent\npreliminary\n\nresults to\nstakeholders \n\nNeed for\nadditional\n\nuncertainty\nreduction? \n\nYes\n\nDecisionmaking and\nimplementation of\ndecision alternative\n\nNeed to\nrefine/adjust\n\ndecision\nalternatives? \n\nRisk and TPM\nResults \n\nSelection\n? Furnish recommendation\n\nto decisionmaker\n? Capture basis\n? Develop/update risk\n\nmanagement plan\n? Refine metrics and develop\n\nmonitoring thresholds\n\n\n\n148 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nthis step is to emphasize that key information about the \nalternative needs to be captured and that this key infor-\nmation includes the perceived potential program vulner-\nabilities that are input to the \u201cplanning\u201d activity within \nCRM. By definition, the selection of the alternative is \nbased at least in part on the prospective achievement of \ncertain values of the TPMs. For purposes of monitoring \nand implementation, these TPM values help to define \nsuccess, and are key inputs to the determination of mon-\nitoring thresholds.\n\nPlanning Technical Risk Management of the \nSelected Alternative\n\nAt this point, a single alternative has been chosen. Dur-\ning analysis, the risk of each alternative will have been \nevaluated for purposes of TPM quantification, but de-\ntailed risk management plans will not have been drawn \nup. At this stage, detailed planning for technical risk \nmanagement of the selected alternative takes place and \na formal risk management plan is drafted. In the plan-\nning phase: \n\nProvisional decisions are made on risk control actions  ?\n(eliminate, mitigate, research, watch, or accept);\nObservables are determined for use in measurement  ?\nof program performance;\nThresholds are determined for the observables such  ?\nthat nonexceedance of the thresholds indicates satis-\nfactory program performance;\nProtocols are determined that guide how often observ- ?\nables are to be measured, what to do when a threshold \nis exceeded, how often to update the analyses, deci-\nsion authority, etc.; and\nResponsibility for the risk tracking is assigned. ?\n\nGeneral categories of risk control actions from NPR \n8000.4, Risk Management Procedural Requirements are \nsummarized here. Each identified and analyzed risk can \nbe managed in one of five ways:\n\nEliminate the risk, ?\nMitigate the risk, ?\nResearch the risk, ?\nWatch the risk, or ?\nAccept the risk. ?\n\nSteps should be taken to eliminate or mitigate the risk if \nit is well understood and the benefits realized are com-\nmensurate with the cost. Benefits are determined using \nthe TPMs from the program\u2019s objectives hierarchy. The \n\nconsequences of mitigation alternatives need to be ana-\nlyzed to ensure that they do not introduce unwarranted \nnew contributions to risk.\n\nIf mitigation is not justified, other activities are consid-\nered. Suppose that there is substantial uncertainty re-\ngarding the risk. For example, there may be uncertainty \nin the probability of a scenario or in the consequences. \nThis creates uncertainty in the benefits of mitigation, \nsuch that a mitigation decision cannot be made with \nconfidence. In this case, research may be warranted to \nreduce uncertainty and more clearly indicate an appro-\npriate choice for the control method. Research is only an \ninterim measure, eventually leading either to risk mitiga-\ntion or to acceptance.\n\nIf neither risk mitigation nor research is justified and the \nconsequence associated with the risk is small, then it may \nneed to be accepted. The risk acceptance process con-\nsiders the likelihood and the severity of consequences. \nNPR 8000.4 delineates the program level with authority \nto accept risk and requires accepted risk to be reviewed \nperiodically (minimum of every 6 months) to ensure \nthat conditions and assumptions have not changed re-\nquiring the risk acceptance to be reevaluated. These re-\nviews should take the form of quantitative and qualita-\ntive analyses, as appropriate.\n\nThe remaining cases are those in which neither risk miti-\ngation nor research are justified, and the consequence as-\nsociated with the risk is large. If there is large uncertainty \nin the risk, then it may need to be watched. This allows \nthe uncertainty to reduce naturally as the program pro-\ngresses and knowledge accumulates, without a research \nprogram targeting that risk. As with research, watching \nis an interim measure, eventually leading either to risk \nmitigation or to acceptance, along guidelines previously \ncited.\n\nEffective Planning\nThe balance of this subsection is aimed primarily at en-\nsuring that the implementation plan for risk monitoring \nis net beneficial.\n\nA good plan has a high probability of detecting signifi-\ncant deviations from program intent in a timely fashion, \nwithout overburdening the program. In order to ac-\ncomplish this, a portfolio of observables and thresholds \nneeds to be identified. Selective plan implementation \nthen checks for deviations of actual TPM values from \n\n\n\n6.4 Technical Risk Management\n\nNASA Systems Engineering Handbook ? 149\n\nplanned TPM values, and does so in a way that adds net \nvalue by not overburdening the project with reporting \nrequirements. Elements of the plan include financial and \nprogress reporting requirements, which are somewhat \npredetermined, and additional program-specific observ-\nables, audits, and program reviews.\n\nThe selection of observables and thresholds should have \nthe following properties:\n\nMeasurable parameters (direct measurement of the  ?\nparameter or of related parameters that can be used \nto calculate the parameter) exist to monitor system \nperformance against clearly defined, objective thresh-\nolds;\nThe monitoring program is set up so that, when a  ?\nthreshold is exceeded, it provides timely indication of \nperformance issues; and\nThe program burden associated with the activity is  ?\nthe minimum needed to satisfy the above.\n\nFor example, probability of loss of a specific mission \ncannot be directly measured, but depends on many \nquantities that can be measured up to a point, such as \nlower level reliability and availability metrics.\n\nMonitoring protocols are established to clarify require-\nments, assign responsibility, and establish intervals for \nmonitoring. The results of monitoring are collected and \nanalyzed, and responses are triggered if performance \nthresholds are exceeded. These protocols also determine \nwhen the analyses must be updated. For example, tech-\nnical risk management decisions should be reassessed \nwith analysis if the goals of the program change. Due \nto the long lead time required for the high-technology \nproducts required by NASA programs, program require-\nments often change before the program completes its \nlife cycle. These changes may include technical require-\nments, budget or schedule, risk tolerance, etc.\n\nTracking and Controlling Performance \nDeviations\n\nAs shown in Figure 6.4-10, tracking is the process by \nwhich parameters are observed, compiled, and reported \naccording to the risk management plan. Risk mitigation/\ncontrol is triggered when a performance threshold is ex-\nceeded, when risk that was assumed to be insignificant \nis found to be significant, or when risk that was not ad-\ndressed during the analyses is discovered. Control may \nalso be required if there are significant changes to the \n\nprogram. The need to invoke risk control measures in \nlight of program changes is determined in the risk man-\nagement plan. Alternatives are proposed and analyzed, \nand a preferred alternative is chosen based on the perfor-\nmance of the alternatives with respect to the TPM, sen-\nsitivity and uncertainty analyses, and deliberation by the \nstakeholders. The new preferred alternative is then sub-\njected to planning, tracking, and control.\n\nDuring the planning phase, control alternatives were \nproactively conceived before required. Once a threshold \nis triggered, a risk control action (as described in Sub-\nsection 6.4.2.3) is required. At this point, there may be \nconsiderably more information available to the decision-\nmaker than existed when the control alternatives were \nproposed. Therefore, new alternatives or modifications \nof existing alternatives should be considered in addition \nto the existing alternatives by iterating this technical risk \nmanagement process. \n\nFigure 6.4-11 shows an example of tracking and control-\nling performance by tracking TPM margins against pre-\ndetermined thresholds. At a point in time corresponding \nwith the vertical break, the TPM\u2019s margin is less than \nthe required margin. At this point, the alternative was \nchanged, such that the margin and margin requirement \nincreased.\n\nThresholds or Other \n\nMetrics and Thresholds\nFrom Plan\n\nExecution of\nChosen Decision\n\nAlternative \n\nIdentify\n\nIterate continuous\nrisk management\n\nprocess\n\nIntervene if\nperformance\n\nthresholds are\nexceeded \n\nControl\n\nTrack\n\nMonitor\nperformance\n\nbased on TPMs and\ndecision rules \n\nFigure 6.4?10 Performance monitoring and \ncontrol of deviations\n\n\n\n150 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nTechnical risk management is not exited until the pro-\ngram terminates, although the level of activity varies \naccording to the current position of the activity in the \nlife cycle. The main outputs are the technical risk re-\nports, including risk associated with proposed alter-\nnatives, risk control alternatives, and decision support \ndata. Risk control alternatives are fed back to technical \nplanning as more information is learned about the al-\nternatives\u2019 risk. This continues until the risk manage-\nment plan is established. This learning process also pro-\nduces alternatives, issues, or problems and supporting \ndata that are fed back into the project. Once a project \nbaseline is chosen, technical risk management focuses \non measuring the deviation of project risk from this \nbaseline, and driving decision support requests based \non these measurements. \n\nMargin\n\nOriginal MarginRequirement\nNew Margin Requirement\n\nTP\nM\n\n M\nar\n\ngi\nn\n\n+\n\n-\n\n0\n\nCu\nrr\n\nen\nt D\n\nat\ne\n\nTimet\n\nRe\npl\n\nan\nni\n\nng\n O\n\ncc\nur\n\nre\nd \n\nH\ner\n\ne\n\nDiscontinuity\nInduced by Shift\n\nto New Allocation\n\nFigure 6.4?11 Margin management method\n\n\n\nNASA Systems Engineering Handbook ? 151\n\nConfiguration Management is a management discipline \napplied over the product\u2019s life cycle to provide visibility \ninto and to control changes to performance and func-\ntional and physical characteristics. CM ensures that the \nconfiguration of a product is known and reflected in \nproduct information, that any product change is benefi-\ncial and is effected without adverse consequences, and \nthat changes are managed.\n\nCM reduces technical risks by ensuring correct product \nconfigurations, distinguishes among product versions, \nensures consistency between the product and informa-\ntion about the product, and avoids the embarrassment of \nstakeholder dissatisfaction and complaint. NASA adopts \nthe CM principles as defined by ANSI/EIA 649, NASA \nmethods of implementation as defined by NASA CM \nprofessionals, and as approved by NASA management. \n\nWhen applied to the design, fabrication/assembly, system/\nsubsystem testing, integration, operational and sustaining \nactivities of complex technology items, CM represents the \n\u201cbackbone\u201d of the enterprise structure. It instills disci-\npline and keeps the product attributes and documenta-\n\ntion consistent. CM enables all stakeholders in the tech-\nnical effort, at any given time in the life of a product, to \nuse identical data for development activities and decision-\nmaking. CM principles are applied to keep the documen-\ntation consistent with the approved engineering, and to \nensure that the product conforms to the functional and \nphysical requirements of the approved design.\n\n6.5.1 Process Description\nFigure 6.5-1 provides a typical flow diagram for the Config-\nuration Management Process and identifies typical inputs, \noutputs, and activities to consider in addressing CM. \n\n6.5.1.1 Inputs\nThe required inputs for this process are:\n\nCM plan, ?\nWork products to be controlled, and ?\nProposed baseline changes. ?\n\n6.5.1.2 Process Activities\nThere are five elements of CM (see Figure 6.5-2):\n\n6.5 Configuration Management \n\nFrom project\n\nFrom Requirements\nManagement and Interface\n\nManagement Processes\n\nTo Technical Data\nManagement Process\n\nTo applicable\ntechnical processes \n\nTo project and\nTechnical Data\n\nManagement Process\n\nFrom Stakeholder\nExpectation De?nition,\n\nLogical Decomposition, and\nTechnical Planning Processes\n\nExpectation, \nRequirements, and\n\nInterface Documents \n\nEngineering Change\nProposals\n\nProject Con?guration\nManagement Plan \n\nList of Con?guration\nItems Under\n\nControl\n\nCon?guration\nManagement\n\nWork Products \n\nCurrent\nBaselines\n\nCon?guration\nManagement\n\nReports \n\nApproved\nRequirements\n\nBaseline Changes\n\nPrepare a strategy to conduct\ncon?guration  management\n\nIdentify baseline to be under\ncon?guration control \n\nMaintain the status of con?guration\ndocumentation \n\nManage con?guration change control\n\nConduct con?guration audits\n\nCapture work products from\ncon?guration management activities Designated\n\nCon?guration Items\nto Be Controlled\n\nFigure 6.5?1 CM Process\n\n\n\n152 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nConfiguration planning and management ?\nConfiguration identification, ?\nConfiguration change management, ?\nConfiguration Status Accounting (CSA), and ?\nConfiguration verification. ?\n\nConfiguration Identification\nConfiguration identification is the systematic process of \nselecting, organizing, and stating the product attributes. \nIdentification requires unique identifiers for a product \nand its configuration documentation. The CM activity \n\nassociated with identifica-\ntion includes selecting the \nConfiguration Items (CIs), \ndetermining CIs\u2019 associated \nconfiguration documenta-\ntion, determining the ap-\npropriate change control \nauthority, issuing unique \nidentifiers for both CIs \nand CI documentation, re-\nleasing configuration docu-\nmentation, and establishing \nconfiguration baselines.\n\nNASA has four baselines, each of which defines a dis-\ntinct phase in the evolution of a product design. The \nbaseline identifies an agreed-to description of attri-\nbutes of a CI at a point in time and provides a known \nconfiguration to which changes are addressed. Base-\nlines are established by agreeing to (and documenting) \nthe stated definition of a CI\u2019s attributes. The approved \n\u201ccurrent\u201d baseline defines the basis of the subsequent \nchange. The system specification is typically finalized \nfollowing the SRR. The functional baseline is estab-\nlished at the SDR and will usually transfer to NASA\u2019s \ncontrol at that time.\n\nThe four baselines (see Figure 6.5-3) normally controlled \nby the program, project, or Center are the following:\n\nFunctional Baseline: ?  The functional baseline is the \napproved configuration documentation that describes \na system\u2019s or top-level CI\u2019s performance requirements \n(functional, interoperability, and interface character-\nistics) and the verification required to demonstrate \nthe achievement of those specified characteristics. The \nfunctional baseline is controlled by NASA.\nAllocated Baseline: ?  The allocated baseline is the ap-\nproved performance-oriented configuration docu-\nmentation for a CI to be developed that describes the \nfunctional and interface characteristics that are al-\nlocated from a higher level requirements document \nor a CI and the verification required to demonstrate \nachievement of those specified characteristics. The al-\nlocated baseline extends the top-level performance \n\nConfiguration\nVerification\n\nConfiguration\nStatus\n\nAccounting\n\nConfiguration\nChange\n\nManagement\n\nConfiguration\nIdentification\n\nConfiguration\nPlanning and\nManagement\n\nCONFIGURATION\nMANAGEMENT\n\nFigure 6.5?2 Five elements of configuration management\n\nCM Planning and Management\nCM planning starts at a program\u2019s or project\u2019s inception. \nThe CM office must carefully weigh the value of priori-\ntizing resources into CM tools or into CM surveillance \nof the contractors. Reviews by the Center Configuration \nManagement Organization (CMO) are warranted and \nwill cost resources and time, but the correction of sys-\ntemic CM problems before they erupt into losing config-\nuration control are always preferable to explaining why \nincorrect or misidentified parts are causing major prob-\nlems in the program/project.\n\nOne of the key inputs to preparing for CM implemen-\ntation is a strategic plan for the project\u2019s complete CM \nprocess. This is typically contained in a CM plan. See Ap-\npendix M for an outline of a typical CM plan.\n\nThis plan has both internal and external uses: \nInternal: ?  It is used within the project office to guide, \nmonitor, and measure the overall CM process. It de-\nscribes both the CM activities planned for future ac-\nquisition phases and the schedule for implementing \nthose activities.\nExternal: ?  The CM plan is used to communicate the \nCM process to the contractors involved in the pro-\ngram. It establishes consistent CM processes and \nworking relationships.\n\nThe CM plan may be a stand-alone document, or it may \nbe combined with other program planning documents. \nIt should describe the criteria for each technical baseline \ncreation, technical approvals, and audits.\n\n\n\n6.5 Configuration Management\n\nNASA Systems Engineering Handbook ? 153\n\nrequirements of the functional baseline to sufficient \ndetail for initiating manufacturing or coding of a CI. \nThe allocated baseline is usually controlled by the de-\nsign organization until all design requirements have \nbeen verified. The allocated baseline is typically estab-\nlished at the successful completion of the PDR. Prior \nto CDR, NASA normally reviews design output for \nconformance to design requirements through incre-\nmental deliveries of engineering data. NASA control \nof the allocated baseline occurs through review of the \nengineering deliveries as data items.\nProduct Baseline:  ? The product baseline is the ap-\nproved technical documentation that describes the \nconfiguration of a CI during the production, fielding/\ndeployment, and operational support phases of its life \n\ncycle. The established product baseline is controlled as \ndescribed in the configuration management plan that \nwas developed during Phase A. The product baseline \nis typically established at the completion of the CDR. \nThe product baseline describes:\n\nDetailed physical or form, fit, and function charac- ?\nteristics of a CI;\nThe selected functional characteristics designated  ?\nfor production acceptance testing; and\nThe production acceptance test requirements. ?\n\nAs-Deployed Baseline:  ? The as-deployed baseline \noccurs at the ORR. At this point, the design is con-\nsidered to be functional and ready for flight. All \nchanges will have been incorporated into the docu-\nmentation.\n\nMission Need\nStatement\n\nFUNCTIONAL \nBASELINE\n\nMajor Architecture\nAspects of Design\nComplete\n\nALLOCATED\nBASELINE\n\nImplementation\nAspects of Design\nComplete \n\nPRODUCT\nBASELINE\n\nRealization Aspects of \nDesign Complete; Fabrication\nand Text Complete \n\nAS-DEPLOYED\nBASELINE\n\nOperational Capability\nDemonstrate \n\nMDR\n\nConcept\n\nPartial\nAnalysis \n\nEngineering\nItems \n\nSystem Spec\n\nComplete\nAnalysis \n\nManuals\n\nSDR\nSRR\n\nPDR\n\nORR\n\nSAR\n\nCDR\n\nQuali?cation\nItems \n\nProduct\nSystem \n\nEnd\nItems \n\nProcedures\n\nProgram\nPlan\n\nPlan\n\nDesign\nDisclosure\n\nSegment Spec\n\nPrime Item\nDesign-to-Spec\n\nEnd Item\nDesign-to-Spec\n\nEnd Item\nBuild-to-Spec\n\nEnd Item\nDesign-to-Spec\n\nEnd Item\nDesign-to-Spec\n\nSegment Spec Segment Spec\n\nPrime Item\nDesign-to-Spec\n\nPrime Item\nDesign-to-Spec\n\nFigure 6.5?3 Evolution of technical baseline\n\n\n\n154 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nConfiguration Change Management\nConfiguration change management is a process to \nmanage approved designs and the implementation of \napproved changes. Configuration change management \nis achieved via the systematic proposal, justification, and \nevaluation of proposed changes, followed by incorpora-\ntion of approved changes and verification of implemen-\ntation. Implementing configuration change manage-\nment in a given program requires unique knowledge \nof the program objectives and requirements. The first \nstep establishes a robust and well-disciplined internal \nNASA Configuration Control Board (CCB) system, \nwhich is chaired by someone with program change au-\nthority. CCB members represent the stakeholders with \nauthority to commit the team they represent. The second \nstep creates configuration change management surveil-\nlance of the contractor\u2019s activity. The CM office advises \nthe NASA program or project manager to achieve a bal-\nanced configuration change management implementa-\ntion that suits the unique program/project situation. See \nFigure 6.5-4 for an example of a typical configuration \nchange management control process.\n\nConfiguration Status Accounting\nConfiguration Status Accounting (CSA) is the recording \nand reporting of configuration data necessary to manage \nCIs effectively. An effective CSA system provides timely \nand accurate configuration information such as:\n\nComplete current and historical configuration docu- ?\nmentation and unique identifiers. \nStatus of proposed changes, deviations, and waivers  ?\nfrom initiation to implementation. \nStatus and final disposition of identified discrepancies  ?\nand actions identified during each configuration audit. \n\nSome useful purposes of the CSA data include:\nAn aid for proposed change evaluations, change deci- ?\nsions, investigations of design problems, warranties, \nand shelf-life calculations.\nHistorical traceability. ?\nSoftware trouble reporting. ?\nPerformance measurement data.  ?\n\nThe following are critical functions or attributes to con-\nsider if designing or purchasing software to assist with \nthe task of managing configuration.\n\nAbility to share data real time with internal and ex- ?\nternal stakeholders securely;\nVersion control and comparison (track history of an  ?\nobject or product);\nSecure user checkout and check in; ?\nTracking capabilities for gathering metrics (i.e., time,  ?\ndate, who, time in phases, etc.);\nWeb based; ?\nNotification capability via e-mail; ?\nIntegration with other databases or legacy systems; ?\nCompatible with required support contractors and/or  ?\nsuppliers (i.e., can accept data from a third party as \nrequired);\nIntegration with drafting and modeling programs as  ?\nrequired;\nProvide neutral format viewer for users; ?\nLicense agreement allows for multiple users within an  ?\nagreed-to number;\nWorkflow and life-cycle management; ?\nLimited customization; ?\nMigration support for software upgrades; ?\nUser friendly; ?\nConsideration for users with limited access; ?\n\nTypes of Configuration Change \nManagement Changes\n\nEngineering Change: ?  An engineering change is \nan iteration in the baseline (draft or established). \nChanges can be major or minor. They may or may \nnot include a specification change. Changes affect-\ning an external interface must be coordinated and \napproved by all stakeholders affected.\n\nA \u201cmajor\u201d change is a change to the baseline con- ?\nfiguration documentation that has significant im-\npact (i.e., requires retrofit of delivered products \nor affects the baseline specification, cost, safety, \ncompatibility with interfacing products, or oper-\nator, or maintenance training).\n\nA \u201dminor\u201d change corrects or modifies configura- ?\ntion documentation or processes without impact \nto the interchangeability of products or system \nelements in the system structure.\n\nWaiver: ?  A waiver is a documented agreement in-\ntentionally releasing a program or project from \nmeeting a requirement. (Some Centers use devia-\ntions prior to Implementation and waivers during \nImplementation.) Authorized waivers do not con-\nstitute a change to a baseline.\n\n\n\n6.5 Configuration Management\n\nNASA Systems Engineering Handbook ? 155\n\nAbility to attach standard format files from desktop ?\nWorkflow capability (i.e., route a CI as required based  ?\non a specific set of criteria); and\nCapable of acting as the one and only source for re- ?\nleased information.\n\nConfiguration Verification\nConfiguration verification is accomplished by inspecting \ndocuments, products, and records; reviewing proce-\ndures, processes, and systems of operations to verify that \nthe product has achieved its required performance re-\n\nFigure 6.5?4 Typical change control process\n\nORIGINATOR\n\nCONFIGURATION\nMANAGEMENT\n\nORGANIZATION EVALUATORS\nRESPONSIBLE\n\nOFFICE\n\nCONFIGURATION\nCONTROL\n\nBOARD ACTIONEES\n\n7.\nChair signs final\n\ndirective\n\n9.\nUpdates\n\ndocument,\nhardware, or\n\nsoftware\n\n11.\nReleases\n\ndocument per\ndirective\n\nStores and\ndistributes as\n\nrequired\n\n10.\nPerforms final\n\ncheck on\ndocuments\n\n1.\nPrepares\n\nchange request \nand sends to\nconfiguration\nmanagement\norganization\n\n3.\nEvaluate\nchange\npackage\n\n4.\nConsolidates\nevaluations\n\nFormulates\nrecommended\n\ndisposition\n\nPresents to CCB\n\n2.\nChecks format\n\nand content\n\nAssigns number\nand enters into \n\nCSA\n\nDetermines\nevaluators\n\n(with originator)\n\nSchedules CCB\ndate if required\n\nPrepares CCB\nagenda\n\n6.\nFinalizes CCB\n\ndirective\n\nUpdates CSA\n\nCreates CCB\nminutes\n\nTracks action\nitems\n\n5.\nChair\n\ndispositions\nchange request\n\nChair assigns\naction items\nif required\n\n8.\nComplete\n\nassigned actions\n\n\n\n156 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nquirements and functional attributes; and verifying that \nthe product\u2019s design is documented. This is sometimes \ndivided into functional and physical configuration au-\ndits. (See Section 6.7 for more on technical reviews.)\n\n6.5.1.3 Outputs\nNPR 7120.5 defines a project\u2019s life cycle in progressive \nphases. Beginning with Pre-Phase A, these steps in turn \nare grouped under the headings of Formulation and Im-\nplementation. Approval is required to transition between \nthese phases. Key Decision Points (KDPs) define transi-\ntions between the phases. CM plays an important role in \n\ndetermining whether a KDP has been met. Major out-\nputs of CM are procedures, approved baseline changes, \nconfiguration status, and audit reports.\n\n6.5.2 CM Guidance\n6.5.2.1 What Is the Impact of Not Doing CM?\nThe impact of not doing CM may result in a project be-\ning plagued by confusion, inaccuracies, low productiv-\nity, and unmanageable configuration data. During the \nColumbia accident investigation, the Columbia Acci-\ndent Investigation Board found inconsistencies related \nto the hardware and the documentation with \u201cunincor-\n\nWarning Signs/Red Flags (How Do You Know When You\u2019re in Trouble?)\n\nGeneral warning signs of an improper implementation of CM include the following:\n\nFailure of program to define the \u201ctop-level\u201d technical requirement (\u201cWe don\u2019t need a spec\u201d). ?\n\nFailure of program to recognize the baseline activities that precede and follow design reviews. ?\n\nProgram office reduces the time to evaluate changes to one that is impossible for engineering, SMA, or other CCB  ?\nmembers to meet.\n\nProgram office declares \u201cthere will be no dissent in the record\u201d for CCB documentation. ?\n\nContract is awarded without CM requirements concurred with by CMO supporting the program office. ?\n\nRedlines used inappropriately on production floor to keep track of changes to design. ?\n\nMaterial Review Board does not know the difference between critical, major, and minor nonconformances and the  ?\nappropriate classification of waivers.\n\nDrawings are not of high quality and do not contain appropriate notes to identify critical engineering items for con- ?\nfiguration control or appropriate tolerancing.\n\nVendors do not understand the implication of submitting waivers to safety requirements as defined in engineering. ?\n\nSubcontractors/vendors change engineering design without approval of integrating contractor, do not know how to  ?\ncoordinate and write an engineering change request, etc.\n\nManufacturing tooling engineering does not keep up with engineering changes that affect tooling concepts. Manu- ?\nfacturing tools lose configuration control and acceptability for production.\n\nVerification data cannot be traced to released part number and specification that apply to verification task. ?\n\nOperational manuals and repair instructions cannot be traced to latest released part number and repair drawings that  ?\napply to repair/modification task.\n\nMaintenance and ground support tools and equipment cannot be traced to latest released part number and specifi- ?\ncation that applies to equipment.\n\nParts and items cannot be identified due to improper identification markings. ?\n\nDigital closeout photography cannot be correlated to the latest release engineering. ?\n\nNASA is unable to verify the latest released engineering through access to the contractor\u2019s CM Web site. ?\n\nTools required per installation procedures do not match the fasteners and nuts and bolts used in the design of CIs. ?\n\nCIs do not fit into their packing crates and containers due to losing configuration control in the design of the ship- ?\nping and packing containers.\n\nSupporting procurement/fabrication change procedures do not adequately involve approval by originating engi- ?\nneering organization.\n\n\n\n6.5 Configuration Management\n\nNASA Systems Engineering Handbook ? 157\n\nporated documentation changes\u201d that led to failure. No \nCM issues were cited as a cause of the accident. The \nusual impact of not implementing CM can be described \nas \u201closing configuration control.\u201d Within NASA, this \nhas resulted in program delays and engineering issues, \nespecially in fast prototyping developments (X-37 Pro-\ngram) where schedule has priority over recording what \nis being done to the hardware. If CM is implemented \nproperly, discrepancies identified during functional and \nphysical configuration audits will be addressed. The fol-\nlowing impacts are possible and have occurred in the \npast:\n\nMission failure and loss of property and life due to im- ?\nproperly configured or installed hardware or software,\nMission failure to gather mission data due to improp- ?\nerly configured or installed hardware or software,\nSignificant mission delay incurring additional cost  ?\ndue to improperly configured or installed hardware \nor software, and\nSignificant mission costs or delay due to improperly  ?\ncertified parts or subsystems due to fraudulent veri-\nfication data.\n\nIf CM is not implemented properly, problems may occur \nin manufacturing, quality, receiving, procurement, etc. \nThe user will also experience problems if ILS data are \nnot maintained. Using a shared software system that \ncan route and track tasks provides the team with the re-\nsources necessary for a successful project. \n\n6.5.2.2 When Is It Acceptable to Use Redline \nDrawings?\n\n\u201cRedline\u201d refers to the control process of marking up \ndrawings and documents during design, fabrication, pro-\nduction, and testing that are found to contain errors or in-\naccuracies. Work stoppages could occur if the documents \nwere corrected through the formal change process. \n\nAll redlines require the approval of the responsible hard-\nware manager and quality assurance manager at a min-\nimum. These managers will determine whether redlines \nare to be incorporated into the plan or procedure.\n\nThe important point is that each project must have a \ncontrolled procedure for redlines that specifies redline \nprocedures and approvals.\n\nRedlines Were identified as One of the Major Causes of the NOAA N?Prime Mishap\n\nExcerpts from the NOAA N-Prime Mishap Investigation Final Report:\n\n\u201cSeveral elements contributed to the NOAA N-PRIME incident, the most significant of which were the lack of proper \nTOC [Turn Over Cart] verification, including the lack of proper PA [Product Assurance] witness, the change in schedule \nand its effect on the crew makeup, the failure of the crew to recognize missing bolts while performing the interface sur-\nface wipe down, the failure to notify in a timely fashion or at all the Safety, PA, and Government representatives, and the \nimproper use of procedure redlines leading to a difficult-to-follow sequence of events. The interplay of the several el-\nements allowed a situation to exist where the extensively experienced crew was not focusing on the activity at hand. \nThere were missed opportunities that could have averted this mishap.\n\n\u201cIn addition, the operations team was utilizing a heavily redlined procedure that required considerable \u2018jumping\u2019 from \nstep to step, and had not been previously practiced. The poorly written procedure and novel redlines were precondi-\ntions to the decision errors made by the RTE [Responsible Test Engineer].\n\n\u201cThe I&T [Integration and Test] supervisors allowed routine poor test documentation and routine misuse of procedure \nredlines.\n\n\u201cKey processes that were found to be inadequate include those that regulate operational tempo, operations planning, pro-\ncedure development, use of redlines, and GSE [Ground Support Equipment] configurations. For instance, the operation \nduring which the mishap occurred was conducted using extensively redlined procedures. The procedures were essentially \nnew at the time of the operation\u2014that is, they had never been used in that particular instantiation in any prior operation. \nThe rewritten procedure had been approved through the appropriate channels even though such an extensive use of red-\nlines was unprecedented. Such approval had been given without hazard or safety analyses having been performed.\u201d\n\n\n\n158 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nThe Technical Data Management Process is used to plan \nfor, acquire, access, manage, protect, and use data of a tech-\nnical nature to support the total life cycle of a system. Data \nManagement (DM) includes the development, deployment, \noperations and support, eventual retirement, and retention \nof appropriate technical, to include mission and science, \ndata beyond system retirement as required by NPR 1441.1, \nNASA Records Retention Schedules. \n\nDM is illustrated in Figure 6.6-1. Key aspects of DM for \nsystems engineering include:\n\nApplication of policies and procedures for data iden- ?\ntification and control,\nTimely and economical acquisition of technical data, ?\nAssurance of the adequacy of data and its protection, ?\nFacilitating access to and distribution of the data to  ?\nthe point of use,\nAnalysis of data use, ?\nEvaluation of data for future value to other programs/ ?\nprojects, and\nProcess access to information written in legacy software. ?\n\n6.6.1 Process Description\nFigure 6.6-1 provides a typical flow diagram for the Tech-\nnical Data Management Process and identifies typical \ninputs, outputs, and activities to consider in addressing \ntechnical data management. \n\n6.6.1.1 Inputs\nInputs include technical data, regardless of the form or \nmethod of recording and whether the data are generated \n\nby the contractor or Government during the life cycle of \nthe system being developed. Major inputs to the Techni-\ncal Data Management Process include: \n\nProgram DM plan, ?\nData products to be managed, and ?\nData requests. ?\n\n6.6.1.2  Process Activities\nEach Center is responsible for policies and procedures \nfor technical DM. NPR 7120.5 and NPR 7123.1 define \nthe need to manage data, but leave specifics to the indi-\nvidual Centers. However, NPR 7120.5 does require that \nDM planning be provided as either a section in the pro-\ngram/project plan or as a separate document. The pro-\ngram or project manager is responsible for ensuring that \nthe data required are captured and stored, data integrity \nis maintained, and data are disseminated as required. \n\nOther NASA policies address the acquisition and storage \nof data and not just the technical data used in the life \ncycle of a system. \n\nRole of Data Management Plan\n\nThe recommended procedure is that the DM plan be a \nseparate plan apart from the program/project plan. DM \nissues are usually of sufficient magnitude to justify a sep-\narate plan. The lack of specificity in Agency policy and \nprocedures provides further justification for more de-\ntailed DM planning. The plan should cover the following \nmajor DM topics:\n\n6.6 Technical Data Management\n\nFigure 6.6?1 Technical Data Management Process\n\nFrom all technical\nprocesses and contractors \n\nTo all technical processes\nand contractors \n\nTo project and all\ntechnical processes \n\nTechnical Data\nProducts to Be\n\nManaged \n\nTechnical Data\nRequests \n\nTechnical Data\nElectronic Exchange\n\nFormats \n\nForm of Technical\nData Products \n\nDelivered Technical\nData \n\nFrom project and all\ntechnical processes \n\nPrepare for technical data\nmanagement implementation \n\nCollect and store required\ntechnical data \n\nProvide technical data to authorized\nparties\n\nMaintain stored technical data\n\n\n\n6.6 Technical Data Management\n\nNASA Systems Engineering Handbook ? 159\n\nIdentification/definition of data requirements for all  ?\naspects of the product life cycle.\nControl procedures\u2014receipt, modification, review,  ?\nand approval.\nGuidance on how to access/search for data for users. ?\nData exchange formats that promote data reuse and  ?\nhelp to ensure that data can be used consistently \nthroughout the system, family of systems, or system \nof systems.\nData rights and distribution limitations such as ex- ?\nport-control Sensitive But Unclassified (SBU).\nStorage and maintenance of data, including master  ?\nlists where documents and records are maintained \nand managed.\n\nTechnical Data Management Key Considerations \n\nSubsequent activities collect, store, and maintain techni-\ncal data and provide it to authorized parties as required. \nSome considerations that impact these activities for im-\nplementing Technical Data Management include:\n\nRequirements relating to the flow/delivery of data to  ?\nor from a contractor should be specified in the tech-\nnical data management plan and included in the Re-\nquest for Proposal (RFP) and contractor agreement. \nNASA should not impose changes on existing con- ?\ntractor data management systems unless the program \ntechnical data management requirements, including \ndata exchange requirements, cannot otherwise be \nmet. \nResponsibility for data inputs into the technical data  ?\nmanagement system lies solely with the originator or \ngenerator of the data. \nThe availability/access of technical data will lie with  ?\nthe author, originator, or generator of the data in con-\njunction with the manager of the technical data man-\nagement system. \nThe established availability/access description and list  ?\nshould be baselined and placed under configuration \ncontrol.\nFor new programs, a digital generation and delivery  ?\nmedium is desired. Existing programs must weigh the \ncost/benefit trades of digitizing hard copy data.\n\nGeneral Data Management Roles\n\nThe Technical Data Management Process provides the \nbasis for applying the policies and procedures to iden-\n\ntify and control data requirements; to responsively and \neconomically acquire, access, and distribute data; and to \nanalyze data use. \n\nAdherence to DM principles/rules enables the sharing, \nintegration, and management of data for performing \ntechnical efforts by Government and industry, and en-\nsures that information generated from managed tech-\nnical data satisfies requests or meets expectations.\n\nThe Technical Data Management Process has a leading \nrole in capturing and organizing technical data and pro-\nviding information for the following uses: \n\nIdentifying, gathering, storing and maintaining the  ?\nwork products generated by other systems engi-\nneering technical and technical management pro-\ncesses as well as the assumptions made in arriving at \nthose work products;\nEnabling collaboration and life-cycle use of system  ?\nproduct data;\nCapturing and organizing technical effort inputs, as  ?\nwell as current, intermediate, and final outputs;\nData correlation and traceability among require- ?\nments, designs, solutions, decisions, and rationales;\nDocumenting engineering decisions, including pro- ?\ncedures, methods, results, and analyses;\nFacilitating technology insertion for affordability im- ?\nprovements during reprocurement and post-produc-\ntion support; and \nSupporting other technical management and tech- ?\nnical processes, as needed.\n\nData Identification/Definition \n\nEach program/project determines data needs during the \nlife cycle. Data types may be defined in standard docu-\nments. Center and Agency directives sometimes specify \ncontent of documents and are appropriately used for \nin-house data preparation. The standard description is \nmodified to suit program/project-specific needs, and \nappropriate language is included in SOWs to imple-\nment actions resulting from the data evaluation. \u201cData \nsuppliers\u201d may be a contractor, academia, or the Gov-\nernment. Procurement of data from an outside supplier \nis a formal procurement action that requires a procure-\nment document; in-house requirements may be han-\ndled in a less formal method. Below are the different \ntypes of data that might be utilized within a program/\nproject:\n\n\n\n160 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nData ?\n\u201cData\u201d is defined in general as \u201crecorded informa- ?\ntion regardless of the form or method of recording.\u201d \nHowever, the terms \u201cdata\u201d and \u201cinformation\u201d are \nfrequently used interchangeably. To be more pre-\ncise, data generally must be processed in some \nmanner to generate useful, actionable information.\n\u201cData,\u201d as used in SE DM, includes technical data;  ?\ncomputer software documentation; and representa-\ntion of facts, numbers, or data of any nature that can \nbe communicated, stored, and processed to form \ninformation required by a contract or agreement to \nbe delivered to, or accessed by, the Government. \nData include that associated with system develop- ?\nment, modeling and simulation used in develop-\nment or test, test and evaluation, installation, parts, \nspares, repairs, usage data required for product sus-\ntainability, and source and/or supplier data. \nData specifically not included in Technical Data  ?\nManagement would be data relating to general \nNASA workforce operations information, com-\nmunications information (except where related to \na specific requirement), financial transactions, per-\nsonnel data, transactional data, and other data of a \npurely business nature. \n\nData Call: ?  Solicitation from Government stake-\nholders (specifically Integrated Product Team (IPT) \nleads and functional managers) identifies and justifies \ntheir data requirements from a proposed contracted \nprocurement. Since data provided by contractors have \na cost to the Government, a data call (or an equivalent \nactivity) is a common control mechanism used to en-\nsure that the requested data are truly needed. If ap-\nproved by the data call, a description of each data item \nneeded is then developed and placed on contract.\nInformation: ?  Information is generally considered as \nprocessed data. The form of the processed data is de-\npendent on the documentation, report, review for-\nmats, or templates that are applicable.\nTechnical Data Package: ?  A technical data package is \na technical description of an item adequate for sup-\nporting an acquisition strategy, production, engi-\nneering, and logistics support. The package defines \nthe required design configuration and procedures to \nensure adequacy of item performance. It consists of \nall applicable items such as drawings, associated lists, \nspecifications, standards, performance requirements, \nquality assurance provisions, and packaging details.\n\nTechnical Data Management System: ?  The strategies, \nplans, procedures, tools, people, data formats, data \nexchange rules, databases, and other entities and de-\nscriptions required to manage the technical data of a \nprogram.\n\nInappropriate Uses of Technical Data\n\nExamples of inappropriate uses of technical data in-\nclude:\n\nUnauthorized disclosure of classified data or data  ?\notherwise provided in confidence;\n\nFaulty interpretation based on incomplete, out-of- ?\ncontext, or otherwise misleading data; and\n\nUse of data for parts or maintenance procurement  ?\nfor which at least Government purpose rights have \nnot been obtained.\n\nWays to help prevent inappropriate use of technical \ndata include the following:\n\nEducate stakeholders on appropriate data use and ?\n\nControl access to data. ?\n\nInitial Data Management System Structure \nWhen setting up a DM system, it is not necessary to \nacquire (that is, to purchase and take delivery of) all \ntechnical data generated on a project. Some data may \nbe stored in other locations with accessibility pro-\nvided on a need-to-know basis. Data should be pur-\nchased only when such access is not sufficient, timely, \nor secure enough to provide for responsive life-cycle \nplanning and system maintenance. Data calls are a \ncommon control mechanism to help address this \nneed.\n\nData Management Planning \nPrepare a technical data management strategy. This  ?\nstrategy can document how the program data man-\nagement plan will be implemented by the technical ef-\nfort or, in the absence of such a program-level plan, \nbe used as the basis for preparing a detailed technical \ndata management plan, including:\n\nItems of data that will be managed according to  ?\nprogram or organizational policy, agreements, or \nlegislation; \nThe data content and format;  ?\n\n\n\n6.6 Technical Data Management\n\nNASA Systems Engineering Handbook ? 161\n\nA framework for data flow within the program  ?\nand to/from contractors including the language(s) \nto be employed in technical effort information ex-\nchanges;\nTechnical data management responsibilities and  ?\nauthorities regarding the origin, generation, cap-\nture, archiving, security, privacy, and disposal of \ndata products; \nEstablishing the rights, obligations, and commit- ?\nments regarding the retention of, transmission of, \nand access to data items; and\nRelevant data storage, transformation, transmis- ?\nsion, and presentation standards and conventions \nto be used according to program or organizational \npolicy, agreements, or legislative constraints.\n\nObtain strategy/plan commitment from relevant  ?\nstakeholders. \nPrepare procedures for implementing the technical  ?\ndata management strategy for the technical effort \nand/or for implementing the activities of the technical \ndata management plan. \nEstablish a technical database(s) to use for technical  ?\ndata maintenance and storage or work with the pro-\ngram staff to arrange use of the program database(s) \nfor managing technical data. \nEstablish data collection tools, as appropriate to the  ?\ntechnical data management scope and available re-\nsources. (See Section 7.3.) \nEstablish electronic data exchange interfaces in accor- ?\ndance with international standards/agreements and \napplicable NASA standards.\nTrain appropriate stakeholders and other technical  ?\npersonnel in the established technical data manage-\nment strategy/plan, procedures, and data collection \ntools, as applicable.\nExpected outcomes:  ?\n\nA strategy and/or plan for implementing technical  ?\ndata management;\nEstablished procedures for performing planned  ?\nTechnical Data Management activities;\nMaster list of managed data and its classification by  ?\ncategory and use;\nData collection tools established and available; and ?\nQualified technical personnel capable of conducting  ?\nestablished technical data management procedures \nand using available data collection tools.\n\nKey Considerations for Planning Data \nManagement and for Tool Selection\n\nAll data entered into the technical data management  ?\nsystem or delivered to a requester from the databases \nof the system should have traceability to the author, \noriginator, or generator of the data.\nAll technical data entered into the technical data  ?\nmanagement system should carry objective evidence \nof current status (for approval, for agreement, for in-\nformation, etc.), version/control number, and date.\nThe technical data management approach should be  ?\ncovered as part of the program\u2019s SEMP.\nTechnical data expected to be used for reprocurement  ?\nof parts, maintenance services, etc., might need to be \nreviewed by the Center\u2019s legal counsel.\n\nCareful consideration should be taken when planning \nthe data access and storage of data that will be generated \nfrom a project or program. If a system or tool is needed, \nmany times the CM tool can be used with less formality. \nIf a separate tool is required to manage the data, refer to \nthe section below for some best practices when evalu-\nating a data management tool. Priority must be placed \non being able to access the data and ease of inputting the \ndata. Second priority should be the consideration of the \nvalue of the specific data to current project/program, fu-\nture programs/projects, NASA\u2019s overall efficiency, and \nuniqueness to NASA\u2019s engineering knowledge.\n\nThe following are critical functions or attributes to con-\nsider if designing or purchasing software to assist with \nthe task of managing data:\n\nAbility to share data with internal and external stake- ?\nholders securely;\nVersion control and comparison, to track history of  ?\nan object or product;\nSecure user updating; ?\nAccess control down to the file level; ?\nWeb based; ?\nAbility to link data to CM system or elements; ?\nCompatible with required support contractors and/or  ?\nsuppliers, i.e., can accept data from a third party as \nrequired;\nIntegrate with drafting and modeling programs as re- ?\nquired;\nProvide neutral format viewer for users; ?\nLicense agreement allows for multiuser seats; ?\n\n\n\n162 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nWorkflow and life-cycle management is a suggested  ?\noption;\nLimited customization; ?\nMigration support between software version up- ?\ngrades;\nUser friendly; ?\nStraightforward search capabilities; and ?\nAbility to attach standard format files from desktop. ?\n\nValue of Data\nStorage of engineering data needs to be planned at the \nbeginning of a program or project. Some of the data \ntypes will fall under the control of NPR 1441.1, Records \nRetention Schedules; those that do not will have to be ad-\ndressed. It is best to evaluate all data that will be pro-\nduced and decide how long it is of value to the program \nor project or to NASA engineering as a whole. There are \nfour basic questions to ask when evaluating data\u2019s value:\n\nDo the data describe the product/system that is being  ?\ndeveloped or built?\nAre the data required to accurately produce the  ?\nproduct/system being developed or built? \nDo the data offer insight for similar future programs  ?\nor projects?\nDo the data hold key information that needs to be  ?\nmaintained in NASA\u2019s knowledge base for future en-\ngineers to use or kept as a learning example?\n\nTechnical Data Capture Tasks\nTable 6.6-1 defines the tasks required to capture tech-\nnical data.\n\nProtection for Data Deliverables\nAll data deliverables should include distribution state-\nments and procedures to protect all data that contain \ncritical technology information, as well as to ensure that \nlimited distribution data, intellectual property data, or \nproprietary data are properly handled during systems \nengineering activities. This injunction applies whether \nthe data are hard copy or digital. \n\nAs part of overall asset protection planning, NASA has \nestablished special procedures for the protection of \nCritical Program Information (CPI). CPI may include \ncomponents; engineering, design, or manufacturing \nprocesses; technologies; system capabilities and vulner-\n\nabilities; and any other information that gives a system \nits distinctive operational capability.\n\nCPI protection should be a key consideration for the \nTechnical Data Management effort and is part of the \nasset protection planning process, as shown in Appen-\ndix Q.\n\n6.6.1.3 Outputs\nOutputs include timely, secure availability of needed data \nin various representations to those authorized to receive \nit. Major outputs from the Technical Data Management \nProcess include (refer to Figure 6.6-1):\n\nTechnical data management procedures, ?\nData representation forms, ?\nData exchange formats, and ?\nRequested data/information delivered. ?\n\n6.6.2 Technical Data Management \nGuidance\n\n6.6.2.1 Data Security and ITAR\nNASA generates an enormous amount of informa-\ntion, much of which is unclassified/nonsensitive in na-\nture with few restrictions on its use and dissemination. \nNASA also generates and maintains Classified National \nSecurity Information (CNSI) under a variety of Agency \nprograms, projects, and through partnerships and col-\nlaboration with other Federal agencies, academia, and \nprivate enterprises. SBU markings requires the author, \ndistributor, and receiver to keep control of the sensitive \n\nData Collection Checklist\n\nHave the frequency of collection and the points  ?\nin the technical and technical management pro-\ncesses when data inputs will be available been de-\ntermined? \n\nHas the timeline that is required to move data from  ?\nthe point of origin to storage repositories or stake-\nholders been established? \n\nWho is responsible for the input of the data?  ?\n\nWho is responsible for data storage, retrieval, and  ?\nsecurity? \n\nHave necessary supporting tools been developed  ?\nor acquired?\n\n\n\n6.6 Technical Data Management\n\nNASA Systems Engineering Handbook ? 163\n\ndocument and data or pass the control to an established \ncontrol process. Public release is prohibited, and a docu-\nment/data marked as such must be transmitted by secure \nmeans. Secure means are encrypted e-mail, secure fax, \nor person-to-person tracking. WebEx is a nonsecure en-\nvironment. Standard e-mail is not permitted to transmit \nSBU documents and data. A secure way to send SBU in-\nformation via e-mail is using the Public Key Infrastructure \n(PKI) to transmit the file(s). PKI is a system that manages \nkeys to lock and unlock computer data. The basic purpose \n\nof PKI is to enable you to share your data keys with other \npeople in a secure manner. PKI provides desktop secu-\nrity, as well as security for desktop and network applica-\ntions, including electronic and Internet commerce.\n\nData items such as detailed design data (models, draw-\nings, presentations, etc.), limited rights data, source se-\nlection data, bid and proposal information, financial \ndata, emergency contingency plans, and restricted com-\nputer software are all examples of SBU data. Items that \n\nTable 6.6?1 Technical Data Tasks\n\nDescription Tasks Expected Outcomes\n\nTechnical \ndata capture\n\nCollect and store inputs and technical effort outcomes from the technical \nand technical management processes, including:\n\nresults from technical assessments;  ?\n\ndescriptions of methods, tools, and metrics used;  ?\n\nrecommendations, decisions, assumptions, and impacts of technical  ?\nefforts and decisions; \n\nlessons learned;  ?\n\ndeviations from plan;  ?\n\nanomalies and out-of-tolerances relative to requirements; and  ?\n\nother data for tracking requirements ?\n\nPerform data integrity checks on collected data to ensure compliance with \ncontent and format as well as technical data check to ensure there are no \nerrors in specifying or recording the data.\n\nReport integrity check anomalies or variances to the authors or generators of \nthe data for correction.\n\nPrioritize, review, and update data collection and storage procedures as part \nof regularly scheduled maintenance.\n\nSharable data needed to \nperform and control the \ntechnical and technical \nmanagement processes is \ncollected and stored.\n\nStored data inventory.\n\nTechnical \ndata mainte-\nnance\n\nImplement technical management roles and responsibilities with technical \ndata products received.\n\nManage database(s) to ensure that collected data have proper quality and \nintegrity; and are properly retained, secure, and available to those with \naccess authority.\n\nPeriodically review technical data management activities to ensure consis-\ntency and identify anomalies and variances.\n\nReview stored data to ensure completeness, integrity, validity, availability, \naccuracy, currency, and traceability.\n\nPerform technical data maintenance, as required.\n\nIdentify and document significant issues, their impacts, and changes made \nto technical data to correct issues and mitigate impacts.\n\nMaintain, control, and prevent the stored data from being used inappropri-\nately.\n\nStore data in a manner that enables easy and speedy retrieval.\n\nMaintain stored data in a manner that protects the technical data against \nforeseeable hazards, e.g., fire, flood, earthquake, etc.\n\nRecords of technical data \nmaintenance.\n\nTechnical effort data, \nincluding captured work \nproducts, contractor-\ndelivered documents \nand acquirer-provided \ndocuments, are controlled \nand maintained.\n\nStatus of data stored is \nmaintained, to include: \nversion description, \ntimeline, and security \nclassification.\n\n (continued)\n\n\n\n164 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nare deemed SBU must be clearly marked in accordance \nwith NPR 1600.1, NASA Security Program Procedural Re-\nquirements. Data or items that cannot be directly marked, \nsuch as computer models and analyses, must have an at-\ntached copy of NASA Form 1686 that indicates the entire \npackage is SBU data. Documents are required to have a \nNASA Form 1686 as a cover sheet. SBU documents and \ndata should be safeguarded. Some examples of ways to \nsafeguard SBU data are: access is limited on a need-to-\nknow basis, items are copy controlled, items are attended \nwhile being used, items are properly marked (document \nheader, footer, and NASA Form 1686), items are stored \nin locked containers or offices and secure servers, trans-\nmitted by secure means, and destroyed by approved \n\nmethods (shredding, etc.). For more information on \nSBU data, see NPR 1600.1.\n\nInternational Traffic in Arms Regulations (ITAR) im-\nplement the Arms Export Control Act, and contain the \nUnited States Munitions List (USML). The USML lists \narticles, services, and related technical data that are \ndesignated as \u201cdefense articles\u201d and \u201cdefense services,\u201d \npursuant to Sections 38 and 47(7) of the Arms Export \nControl Act. The ITAR is administered by the U.S. De-\npartment of State. \u201cTechnical data\u201d as defined in the \nITAR does not include information concerning general \nscientific, mathematical, or engineering principles com-\nmonly taught in schools, colleges, and universities or in-\nformation in the public domain (as that term is defined \n\nDescription Tasks Expected Outcomes\n\nTechnical \ndata/ \ninformation \ndistribution\n\nMaintain an information library or reference index to provide technical data \navailability and access instructions. \n\nReceive and evaluate requests to determine data requirements and delivery \ninstructions.\n\nProcess special requests for technical effort data or information according to \nestablished procedures for handling such requests.\n\nEnsure that required and requested data are appropriately distributed to \nsatisfy the needs of the acquirer and requesters in accordance with the \nagreement, program directives, and technical data management plans and \nprocedures.\n\nEnsure that electronic access rules are followed before database access is \nallowed or any requested data are electronically released/transferred to the \nrequester.\n\nProvide proof of correctness, reliability, and security of technical data \nprovided to internal and external recipients.\n\nAccess information (e.g., \navailable data, access \nmeans, security proce-\ndures, time period for \navailability, and personnel \ncleared for access) is read-\nily available.\n\nTechnical data are \nprovided to authorized \nrequesters in the appropri-\nate format, with the ap-\npropriate content, and by a \nsecure mode of delivery, as \napplicable.\n\nData \nmanagement \nsystem \nmaintenance\n\nImplement safeguards to ensure protection of the technical database and of \nen route technical data from unauthorized access or intrusion.\n\nEstablish proof of coherence of the overall technical data set to facilitate \neffective and efficient use.\n\nMaintain, as applicable, backups of each technical database.\n\nEvaluate the technical data management system to identify collection and \nstorage performance issues and problems; satisfaction of data users; risks \nassociated with delayed or corrupted data, unauthorized access, or surviv-\nability of information from hazards such as fire, flood, earthquake, etc. \n\nReview systematically the technical data management system, including the \ndatabase capacity, to determine its appropriateness for successive phases of \nthe Defense Acquisition Framework.\n\nRecommend improvements for discovered risks and problems:\n\nHandle risks identified as part of technical risk management.  ?\n\nControl recommended changes through established program change  ?\nmanagement activities.\n\nCurrent technical data \nmanagement system.\n\nTechnical data are ap-\npropriately and regularly \nbacked up to prevent data \nloss. \n\nTable 6.6?1 Technical Data Tasks (continued)\n\n\n\n6.6 Technical Data Management\n\nNASA Systems Engineering Handbook ? 165\n\nin 22 CFR 120.11). It also does not include basic mar-\nketing information on function and purpose or general \nsystem descriptions. For purposes of the ITAR, the fol-\nlowing definitions apply:\n\n\u201cDefense Article\u201d (22 CFR 120.6):  ? A defense article \nis any item or technical data on the USML. The term \nincludes technical data recorded or stored in any phys-\nical form, models, mockups, or other items that reveal \ntechnical data directly relating to items designated in \nthe USML. Examples of defense articles included on \nthe USML are (1) launch vehicles, including their spe-\ncifically designed or modified components, parts, ac-\ncessories, attachments, and associated equipment; \n(2) remote sensing satellite systems, including ground \ncontrol stations for telemetry, tracking, and control \nof such satellites, as well as passive ground stations if \nsuch stations employ any cryptographic items con-\ntrolled on the USML or employ any uplink command \ncapability; and (3) all components, parts, accessories, \nattachments, and associated equipment (including \n\nground support equipment) that is specifically de-\nsigned, modified, or configured for such systems. (See \n22 CFR 121.1 for the complete listing.) \n\u201cTechnical Data\u201d (22 CFR 120.10): ?  Technical data \nare information required for the design, development, \nproduction, manufacture, assembly, operation, repair, \ntesting, maintenance, or modification of defense ar-\nticles. This includes information in the form of blue-\nprints, drawings, photographs, plans, instructions, \nand documentation. \nClassified Information Relating to Defense Articles  ?\nand Defense Services: Classified information is cov-\nered by an invention secrecy order (35 U.S.C. 181 et \nseq.; 35 CFR Part 5). \nSoftware Directly Related to Defense Articles: ?  Con-\ntrolled software includes, but is not limited to, system \nfunctional design, logic flow, algorithms, application \nprograms, operating systems, and support software \nfor design, implementation, test, operations, diag-\nnosis, and repair related to defense articles. \n\n\n\n166 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nTechnical assessment is the crosscutting process used to help \nmonitor technical progress of a program/project through \nPeriodic Technical Reviews (PTRs). It also provides status \ninformation to support assessing system design, product re-\nalization, and technical management decisions. \n\n6.7.1 Process Description\nFigure 6.7-1 provides a typical flow diagram for the \nTechnical Assessment Process and identifies typical in-\nputs, outputs, and activities to consider in addressing \ntechnical assessment.\n\n6.7.1.1 Inputs\nTypical inputs needed for the Technical Assessment Pro-\ncess would include the following:\n\nTechnical Plans ? : These are the planning documents \nthat will outline the technical reviews/assessment \nprocess as well as identify the technical product/pro-\ncess measures that will be tracked and assessed to de-\ntermine technical progress. Examples of these plans \nwill be the SEMP, review plans, and EVM plan.\n\nTechnical Measures ? : These are the identified tech-\nnical measures that will be tracked to determine tech-\nnical progress. These measures are also referred to as \nMOEs, MOPs, and TPMs.\nReporting Requirements ? : These are the requirements \non the methodology in which the status of the tech-\nnical measures will be reported in regard to risk, cost, \nschedule, etc. The methodology and tools used for re-\nporting the status will be established on a project-by-\nproject basis.\n\n6.7.1.2 Process Activities\nAs outlined in Figure 6.7-1, the technical plans (e.g., \nSEMP, review plans) provide the initial inputs into the \nTechnical Assessment Process. These documents will \noutline the technical reviews/assessment approach as \nwell as identify the technical measures that will be tracked \nand assessed to determine technical progress. An impor-\ntant part of the technical planning is determining what is \nneeded in time, resources, and performance to complete \na system that meets desired goals and objectives. Project \n\nTo Decision Analysis Process\n\nAnalysis Support \nRequests\n\nWork Products From\nTechnical Assessment \n\nTo Technical Data\nManagement Process \n\nTo Technical Planning, \nRequirements Management, and \n\nInterface Management Processes  \n\nCorrective Action\nRecommendations\n\nTechnical Cost and Schedule\nStatus Reports\n\nProduct Measurements\n\nDecision Support\nRecommendations and\n\nImpacts \n\nFrom Product Veri?cation and\nProduct Validation Processes \n\nFrom Decision Analysis Process \n\nProduct and Process\nMeasures \n\nTechnical Plans\n\nFrom Technical\nPlanning Process\n\nFrom project\n\nRisk Reporting\nRequirements\n\nTechnical Review\nReports \n\nTo project and Technical Data\nManagement Process \n\nAssessment Results/\nFindings \n\nTo Technical Planning, \nTechnical Risk Management, and \n\nRequirements Management Processes\n\nPrepare strategy for conducting\ntechnical assessments\n\nAssess technical work productivity \n(measure progress against plans)\n\nConduct horizontal and vertical progress\ntechnical reviews\n\nAssess technical product quality\n(measure progress against requirements)\n\nCapture work products from technical\nassessment activities\n\nFigure 6.7?1 Technical Assessment Process\n\n6.7 Technical Assessment\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 167\n\nmanagers need visibility into the progress of those plans \nin order to exercise proper management control. Typical \nactivities in determining progress against the identified \ntechnical measures will include status reporting and as-\nsessing the data. Status reporting will identify where the \nproject stands in regard to a particular technical measure. \nAssessing will analytically convert the output of the status \nreporting into a more useful form from which trends can \nbe determined and variances from expected results can be \nunderstood. Results of the assessment activity will then \nfeed into the Decision Analysis Process (see Section 6.8) \nwhere potential corrective action is necessary.\n\nThese activities together form the feedback loop depicted \nin Figure 6.7-2. \n\nRegular, periodic (e.g., monthly) tracking of the tech-\nnical measures is recommended, although some mea-\nsures should be tracked more often when there is rapid \nchange or cause for concern. Key reviews, such as PDRs \nand CDRs, are points at which technical measures and \ntheir trends should be carefully scrutinized for early \nwarning signs of potential problems. Should there be in-\ndications that existing trends, if allowed to continue, will \nyield an unfavorable outcome, corrective action should \nbegin as soon as practical. Subsection 6.7.2.2 provides \nadditional information on status reporting and assess-\nment techniques for costs and schedules (including \nEVM), technical performance, and systems engineering \nprocess metrics.\n\nThe measures are predominantly assessed during the \nprogram and project technical reviews. Typical activities \nperformed for technical reviews include (1) identifying, \nplanning, and conducting phase-to-phase technical re-\nviews; (2) establishing each review\u2019s purpose, objec-\ntive, and entry and success criteria; (3) establishing the \nmakeup of the review team; and (4) identifying and re-\nsolving action items resulting from the review. Subsec-\ntion 6.7.2.1 summarizes the types of technical reviews \ntypically conducted on a program/project and the role \nof these reviews in supporting management decision \nprocesses. It also identifies some general principles for \nholding reviews, but leaves explicit direction for exe-\ncuting a review to the program/project team to define. \n\nThe process of executing technical assessment has close \nrelationships to other areas, such as risk management, \ndecision analysis, and technical planning. These areas \nmay provide input into the Technical Assessment Pro-\ncess or be the benefactor of outputs from the process.\n\n6.7.1.3 Outputs\nTypical outputs of the Technical Assessment Process \nwould include the following:\n\nAssessment Results, Findings, and Recommenda- ?\ntions: This is the collective data on the established \nmeasures from which trends can be determined and \nvariances from expected results can be understood. \nResults will then feed into the Decision Analysis Pro-\ncess where potential corrective action is necessary.\nTechnical Review Reports/Minutes: ?  This is the col-\nlective information coming out of each review that \ncaptures the results, recommendations, and actions in \nregard to meeting the review\u2019s success criteria.\n\n(Re-)\nPlanning\n\nStatus\nReporting Assessing\n\nDecision-\nmaking\n\nStatus Not OK\n\nStatus OK\n\nExecute\n\nFigure 6.7?2 Planning and status reporting \nfeedback loop\n\nThis loop takes place on a continual basis throughout \nthe project life cycle. This loop is applicable at each level \nof the project hierarchy. Planning data, status reporting \ndata, and assessments flow up the hierarchy with ap-\npropriate aggregation at each level; decisions cause ac-\ntions to be taken down the hierarchy. Managers at each \nlevel determine (consistent with policies established at \nthe next higher level of the project hierarchy) how of-\nten, and in what form, reporting data and assessments \nshould be made. In establishing these status reporting \nand assessment requirements, some principles of good \npractice are:\n\nUse an agreed-upon set of well-defined technical  ?\nmeasures. (See Subsection 6.7.2.2.)\nReport these technical measures in a consistent format  ?\nat all project levels.\nMaintain historical data for both trend identification  ?\nand cross-project analyses.\nEncourage a logical process of rolling up technical mea- ?\nsures (e.g., use the WBS for project progress status).\nSupport assessments with quantitative risk measures. ?\nSummarize the condition of the project by using  ?\ncolor-coded (red, yellow, and green) alert zones for all \ntechnical measures. \n\n\n\n168 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\n6.7.2 Technical Assessment Guidance\n\n6.7.2.1 Reviews, Audits, and Key Decision Points \nTo gain a general understanding of the various tech-\nnical reviews called out in Agency policy (e.g., NPR \n7120.5 and NPR 7123.1), we need to examine the intent \nof the policy within each of the above-mentioned doc-\numents. These reviews inform the decision authority. \nNPR 7120.5\u2019s primary focus is to inform the decision au-\nthority as to the readiness of a program/project to pro-\nceed into the next phase of the life cycle. This is done for \neach milestone review and is tied to a KDP throughout \nthe life cycle. For KDP/milestone reviews, external in-\ndependent reviewers known as Standing Review Board \n(SRB) members evaluate the program/project and, in the \nend, report their findings to the decision authority. For a \nprogram or project to prepare for the SRB, the technical \nteam must conduct their own internal peer review pro-\ncess. This process typically includes both informal and \nformal peer reviews at the subsystem and system level. \nThis handbook attempts to provide sufficient insight and \nguidance into both policy documents so that practitio-\nners can understand how they are to be successfully in-\ntegrated; however, the main focus in this handbook will \nbe on the internal review process.\n\nThe intent and policy for reviews, audits, and KDPs \nshould be developed during Phase A and defined in the \nprogram/project plan. The specific implementation of \nthese activities should be consistent with the types of re-\nviews and audits described in this section, and with the \nNASA program and project life-cycle charts (see Fig-\nures 3.0-1 and 3.0-2). However, the timing of reviews, \naudits, and KDPs should accommodate the need of each \nspecific project.\n\nPurpose and Definition\n\nThe purpose of a review is to furnish the forum and pro-\ncess to provide NASA management and their contrac-\ntors assurance that the most satisfactory approach, plan, \nor design has been selected; that a configuration item has \nbeen produced to meet the specified requirements; or that \na configuration item is ready. Reviews help to develop a \nbetter understanding among task or project participants, \nopen communication channels, alert participants and \nmanagement to problems, and open avenues for solu-\ntions. Reviews are intended to add value to the project and \nenhance project quality and the likelihood of success. This \nis aided by inviting outside experts to confirm the viability \n\nof the presented approach, concept, or baseline or to rec-\nommend alternatives. Reviews may be program life-cycle \nreviews, project life-cycle reviews, or internal reviews.\n\nThe purpose of an audit is to provide NASA management \nand its contractors a thorough examination of adherence \nto program/project policies, plans, requirements, and \nspecifications. Audits are the systematic examination of \ntangible evidence to determine adequacy, validity, and \neffectiveness of the activity or documentation under re-\nview. An audit may examine documentation of policies \nand procedures, as well as verify adherence to them.\n\nThe purpose of a KDP is to provide a scheduled event at \nwhich the decision authority determines the readiness of \na program/project to progress to the next phase of the \nlife cycle (e.g., B to C, C to D, etc.) or to the next KDP. \nKDPs are part of NASA\u2019s oversight and approval pro-\ncess for programs/projects. For a detailed description of \nthe process and management oversight teams, see NPR \n7120.5. Essentially, KDPs serve as gates through which \nprograms and projects must pass. Within each phase, a \nKDP is preceded by one or more reviews, including the \ngoverning Program Management Council (PMC) re-\nview. Allowances are made within a phase for the differ-\nences between human and robotic space flight programs \nand projects, but phases always end with the KDP. The \npotential outcomes at a KDP include:\n\nApproval for continuation to the next KDP. ?\nApproval for continuation to the next KDP, pending  ?\nresolution of actions.\nDisapproval for continuation to the next KDP. In such  ?\ncases, follow-up actions may include a request for \nmore information and/or a delta independent review; \na request for a Termination Review (described below) \nfor the program or the project (Phases B, C, D, and E \nonly); direction to continue in the current phase; or \nredirection of the program/project.\n\nThe decision authority reviews materials submitted by the \ngoverning PMC, SRB, Program Manager (PM), project \nmanager, and Center Management Council (CMC) in \naddition to agreements and program/project documen-\ntation to support the decision process. The decision au-\nthority makes decisions by considering a number of fac-\ntors, including continued relevance to Agency strategic \nneeds, goals, and objectives; continued cost affordability \nwith respect to the Agency\u2019s resources; the viability \nand the readiness to proceed to the next phase; and re-\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 169\n\nmaining program or project risk (cost, schedule, tech-\nnical, safety). Appeals against the final decision of the de-\ncision authority go to the next higher decision authority.\n\nProject Termination\nIt should be noted that project termination, while usu-\nally disappointing to project personnel, may be a proper \nreaction to changes in external conditions or to an im-\nproved understanding of the system\u2019s projected cost-ef-\nfectiveness.\n\nGeneral Principles for Reviews\nSeveral factors can affect the implementation plan for \nany given review, such as design complexity, schedule, \ncost, visibility, NASA Center practices, the review itself, \netc. As such, there is no set standard for conducting a re-\nview across the Agency; however, there are key elements, \nor principles, that should be included in a review plan. \nThese include definition of review scope, objectives, suc-\ncess criteria (consistent with NPR 7123.1), and process. \nDefinition of the review process should include identi-\nfication of schedule, including duration of the face-to-\nface meeting (and draft agenda), definition of roles and \nresponsibilities of participants, identification of presenta-\ntion material and data package contents, and a copy of \nthe form to be used for Review Item Disposition (RID)/\nRequest For Action (RFA)/Comment. The review pro-\ncess for screening and processing discrepancies/requests/\ncomments should also be included in the plan. The re-\nview plan must be agreed to by the technical team lead, \nproject manager, and for SRB-type reviews, the SRB chair \nprior to the review.\n\nIt is recommended that all reviews consist of oral pre-\nsentations of the applicable project requirements and the \napproaches, plans, or designs that satisfy those require-\nments. These presentations are normally provided by the \ncognizant design engineers or their immediate supervisor. \nIt is also recommended that, in addition to the SRB, the \nreview audience include key stakeholders, such as the sci-\nence community, program executive, etc. This ensures \nthat the project obtains buy-in from the personnel who \nhave control over the project as well as those who benefit \nfrom a successful mission. It is also very beneficial to have \nproject personnel in attendance that are not directly asso-\nciated with the design being reviewed (e.g., EPS attending \na thermal discussion). This gives the project an additional \nopportunity to utilize cross-discipline expertise to iden-\ntify design shortfalls or recommend improvements. Of \ncourse, the audience should also include nonproject spe-\ncialists from safety, quality and mission assurance, reli-\nability, verification, and testing. \n\nProgram Technical Life?Cycle Reviews \nWithin NASA there are various types of programs:\n\nSingle-project programs (e.g., James Webb Space  ?\nTelescope Program) tend to have long development \nand/or operational lifetimes, represent a large invest-\nment of Agency resources in one program/project, \nand have contributions to that program/project from \nmultiple organizations or agencies.\nUncoupled programs (e.g., Discovery Program, Ex- ?\nplorer) are implemented under a broad scientific \ntheme and/or a common program implementation \nconcept, such as providing frequent flight opportu-\nnities for cost-capped projects selected through AOs \nor NASA research announcements. Each such project \nis independent of the other projects within the pro-\ngram. \nLoosely coupled programs (e.g., Mars Exploration  ?\nProgram or Lunar Precursor and Robotic Program) \naddress specific scientific or exploration objectives \nthrough multiple space flight projects of varied \nscope. While each individual project has an assigned \nset of mission objectives, architectural and techno-\nlogical synergies and strategies that benefit the pro-\ngram as a whole are explored during the Formula-\ntion process. For instance, all orbiters designed for \nmore than one year in Mars orbit are required to \ncarry a communication system to support present \nand future landers. \n\nTermination Review\n\nA termination review is initiated by the decision au-\nthority to secure a recommendation as to whether to \ncontinue or terminate a program or project. Failing to \nstay within the parameters or levels specified in con-\ntrolling documents will result in consideration of a ter-\nmination review.\n\nAt the termination review, the program and the \nproject teams present status, including any mate-\nrial requested by the decision authority. Appropriate \nsupport organizations are represented (e.g., procure-\nment, external affairs, legislative affairs, public affairs) \nas needed. The decision and basis of the decision are \nfully documented and reviewed with the NASA Asso-\nciate Administrator prior to final implementation.\n\n\n\n170 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nTightly coupled programs (e.g., Constellation Pro- ?\ngram) have multiple projects that execute portions of \na mission or missions. No single project is capable of \nimplementing a complete mission. Typically, multiple \nNASA Centers contribute to the program. Individual \nprojects may be managed at different Centers. The \nprogram may also include other Agency or interna-\ntional partner contributions. \n\nRegardless of the type, all programs are required to un-\ndergo the two technical reviews listed in Table 6.7-1. The \nmain difference lies between uncoupled/loosely coupled \nprograms that tend to conduct \u201cstatus-type\u201d reviews on \ntheir projects after KDP I and single-project/tightly cou-\npled programs that tend to follow the project technical \nlife-cycle review process post KDP I.\n\nAfter KDP I, single-project/tightly coupled programs \nare responsible for conducting the system-level reviews. \nThese reviews bring the projects together and help en-\nsure the flowdown of requirements and that the overall \nsystem/subsystem design solution satisfies the program \nrequirements. The program/program reviews also help \nresolve interface/integration issues between projects. For \nthe sake of this handbook, single-project programs and \ntightly coupled programs will follow the project life-cycle \nreview process defined after this table. Best practices and \nlessons learned drive programs to conduct their \u201cconcept \nand requirements-type\u201d reviews prior to project concept \nand requirements reviews and \u201cprogram design and ac-\nceptance-type\u201d reviews after project design and accep-\ntance reviews. \n\nProject Technical Life?Cycle Reviews\nThe phrase \u201cproject life cycle/project milestone reviews\u201d \nhas, over the years, come to mean different things to \nvarious Centers. Some equate it to mean the project\u2019s \ncontrolled formal review using RIDS and pre-boards/\nboards, while others use it to mean the activity tied to \nRFAs and SRB/KDP process. This document will use the \nlatter process to define the term. Project life-cycle re-\nviews are mandatory reviews convened by the decision \nauthority, which summarize the results of internal tech-\nnical processes (peer reviews) throughout the project \nlife cycle to NASA management and/or an independent \nreview team, such as an SRB (see NPR 7120.5). These \nreviews are used to assess the progress and health of a \nproject by providing NASA management assurance that \nthe most satisfactory approach, plan, or design has been \nselected, that a configuration item has been produced to \nmeet the specified requirements, or that a configuration \nitem is ready for launch/operation. Some examples of \nlife-cycle reviews include System Requirements Review, \nPreliminary Design Review, Critical Design Review, and \nAcceptance Review. \n\nSpecified life-cycle reviews are followed by a KDP in \nwhich the decision authority for the project determines, \nbased on results and recommendations from the life-\ncycle review teams, whether or not the project can pro-\nceed to the next life-cycle phase.\n\nStanding Review Boards\nThe SRB\u2019s role is advisory to the program/project and the \nconvening authorities, and does not have authority over \nany program/project content. Its review provides expert \nassessment of the technical and programmatic approach, \nrisk posture, and progress against the program/project \nbaseline. When appropriate, it may offer recommenda-\ntions to improve performance and/or reduce risk.\n\nInternal Reviews \nDuring the course of a project or task, it is necessary \nto conduct internal reviews that present technical ap-\nproaches, trade studies, analyses, and problem areas to \na peer group for evaluation and comment. The timing, \nparticipants, and content of these reviews is normally \ndefined by the project manager or the manager of the \nperforming organization with support from the tech-\nnical team. In preparation for the life-cycle reviews a \nproject will initiate an internal review process as defined \nin the project plan. These reviews are not just meetings \n\nTable 6.7?1 Program Technical Reviews\n\nReview Purpose\n\nProgram/ \nSystem \nRequirements  \nReview\n\nThe P/SRR examines the functional \nand performance requirements \ndefined for the program (and its \nconstituent projects) and ensures that \nthe requirements and the selected \nconcept will satisfy the program \nand higher level requirements. It is \nan internal review. Rough order of \nmagnitude budgets and schedules are \npresented.\n\nProgram/ \nSystem  \nDefinition \nReview\n\nThe P/SDR examines the proposed \nprogram architecture and the \nflowdown to the functional elements \nof the system.\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 171\n\nto share ideas and resolve issues, but are internal reviews \nthat allow the project to establish baseline requirements, \nplans, or design through the review of technical ap-\nproaches, trade studies, and analyses. \n\nInternal peer reviews provide an excellent means for \ncontrolling the technical progress of the project. They \nshould also be used to ensure that all interested parties \nare involved in the development early on and throughout \nthe process. Thus, representatives from areas such as \nmanufacturing and quality assurance should attend the \ninternal reviews as active participants. It is also a good \npractice to include representatives from other Centers \nand outside organizations providing support or devel-\noping systems or subsystems that may interface to your \nsystem/subsystem. They can then, for example, ensure \nthat the design is producible and integratable and that \nquality is managed through the project life cycle. \n\nSince internal peer reviews will be at a much greater level \nof detail than the life-cycle reviews, the team may uti-\nlize internal and external experts to help develop and as-\nsess approaches and concepts at the internal reviews. Some \norganizations form a red team to provide an internal, inde-\npendent, peer review to identify deficiencies and offer rec-\nommendations. Projects often refer to their internal reviews \nas \u201ctabletop\u201d reviews or \u201cinterim\u201d design reviews. Whatever \n\nthe name, the purpose is the same: to ensure the readiness \nof the baseline for successful project life-cycle review. \n\nIt should be noted that due to the importance of these \nreviews each review should have well-defined entrance \nand success criteria established prior to the review.\n\nRequired Technical Reviews\nThis subsection describes the purpose, timing, objec-\ntives, success criteria, and results of the NPR 7123.1 \nrequired technical reviews in the NASA program and \nproject life cycles. This information is intended to pro-\nvide guidance to program/project managers and systems \nengineers, and to illustrate the progressive maturation of \nreview activities and systems engineering products. For \nFlight Systems and Ground Support (FS&GS) projects, \nthe NASA life-cycle phases of Formulation and Imple-\nmentation divide into seven project phases. The check-\nlists provided below aid in the preparation of specific \nreview entry and success criteria, but do not take their \nplace. To minimize extra work, review material should \nbe keyed to program/project documentation.\n\nProgram/System Requirements Review\nThe P/SRR is used to ensure that the program require-\nments are properly formulated and correlated with the \nAgency and mission directorate strategic objectives.\n\nTable 6.7?2 P/SRR Entrance and Success Criteria\n\nProgram/System Requirements Review\n\nEntrance Criteria Success Criteria\n\nAn FAD has been approved. 1. \n\nProgram requirements have been defined that support mission 2. \ndirectorate requirements on the program.\n\nMajor program risks and corresponding mitigation strategies have 3. \nbeen identified.\n\nThe high-level program requirements have been documented to include:4. \nperformance,a. \nsafety, andb. \nprogrammatic requirements.c. \n\nAn approach for verifying compliance with program requirements has 5. \nbeen defined.\n\nProcedures for controlling changes to program requirements have 6. \nbeen defined and approved.\n\nTraceability of program requirements to individual projects is 7. \ndocumented in accordance with Agency needs, goals, and objectives, \nas described in the NASA Strategic Plan. \n\nTop program/project risks with significant technical, safety, cost, and 8. \nschedule impacts are identified. \n\nWith respect to mission and science 1. \nrequirements, defined high-level program \nrequirements are determined to be \ncomplete and are approved.\n\nDefined interfaces with other programs 2. \nare approved.\n\nThe program requirements are determined 3. \nto provide a cost-effective program.\n\nThe program requirements are adequately 4. \nlevied on either the single-program project \nor the multiple projects of the program.\n\nThe plans for controlling program require-5. \nment changes have been approved. \n\nThe approach for verifying compliance 6. \nwith program requirements has been \napproved.\n\nThe mitigation strategies for handling 7. \nidentified major risks have been approved. \n\n\n\n172 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nProgram/System Definition Review\nThe P/SDR applies to all NASA space flight programs to \nensure the readiness of these programs to enter an ap-\nproved Program Commitment Agreement (PCA). The \napproved PCA permits programs to transition from the \nprogram Formulation phase to the program Implemen-\ntation phase. A Program Approval Review (PAR) is con-\nducted as part of the P/SDR to provide Agency manage-\nment with an independent assessment of the readiness of \nthe program to proceed into implementation. \n\nThe P/SDR examines the proposed program architec-\nture and the flowdown to the functional elements of \nthe system. The proposed program\u2019s objectives and the \nconcept for meeting those objectives are evaluated. Key \ntechnologies and other risks are identified and assessed. \nThe baseline program plan, budgets, and schedules are \npresented. The technical team provides the technical \ncontent to support the P/SDR. The P/SDR examines the \nproposed program architecture and the flowdown to the \nfunctional elements of the system.\n\nTable 6.7?3 P/SDR Entrance and Success Criteria\n\nProgram/System Definition Review\n\nEntrance Criteria Success Criteria\n\nA P/SRR has been satisfactorily completed.1. \n\nA program plan has been prepared that includes the following:2. \nhow the program will be managed;a. \na list of specific projects;b. \nthe high-level program requirements (including risk criteria);c. \nperformance, safety, and programmatic requirements correlated to Agency and directorate strategic d. \nobjectives;\ndescription of the systems to be developed (hardware and software), legacy systems, system interfaces, e. \nand facilities; and\nidentification of major constraints affecting system development (e.g., cost, launch window, required f. \nlaunch vehicle, mission planetary environment, engine design, international partners, and technology \ndrivers).\n\nProgram-level SEMP that includes project technical approaches and management plans to implement the 3. \nallocated program requirements including constituent launch, flight, and ground systems; and operations \nand logistics concepts. \n\nIndependent cost analyses (ICAs) and independent cost estimates (ICEs).4. \n\nManagement plan for resources other than budget.5. \n\nDocumentation for obtaining the PCA that includes the following:6. \nthe feasibility of the program mission solution with a cost estimate within acceptable cost range,a. \nproject plans adequate for project formulation initiation,b. \nidentified and prioritized program concept evaluation criteria to be used in project evaluations,c. \nestimates of required annual funding levels,d. \ncredible program cost and schedule allocation estimates to projects,e. \nacceptable risk and mitigation strategies (supported by a technical risk assessment),f. \norganizational structures and defined work assignments,g. \ndefined program acquisition strategies,h. \ninterfaces to other programs and partners,i. \na draft plan for program implementation, andj. \na defined program management system.k. \n\nA draft program control plan that includes:7. \nhow the program plans to control program requirements, technical design, schedule, and cost to achieve a. \nits high-level requirements;\nhow the requirements, technical design, schedule, and cost of the program will be controlled;b. \nhow the program will utilize its technical, schedule, and cost reserves to control the baseline;c. \nhow the program plans to report technical, schedule, and cost status to the MDAA, including frequency d. \nand the level of detail; and\nhow the program will address technical waivers and how dissenting opinions will be handled.e. \n\nFor each project, a top-level description has been documented.8. \n\nAn approved 1. \nprogram plan \nand manage-\nment approach. \n\nApproved SEMP 2. \nand technical \napproach. \n\nEstimated costs 3. \nare adequate.\n\nDocumentation 4. \nfor obtaining \nthe PCA is \napproved.\n\nAn approved 5. \ndraft program \ncontrol plan.\n\nAgreement that 6. \nthe program \nis aligned with \nAgency needs, \ngoals, and \nobjectives.\n\nThe technical 7. \napproach is \nadequate.\n\nThe schedule is 8. \nadequate and \nconsistent with \ncost, risk, and \nmission goals. \n\nResources other 9. \nthan budget are \nadequate and \navailable.\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 173\n\nTable 6.7?4 MCR Entrance and Success Criteria\n\nMission Concept Review\n\nEntrance Criteria Success Criteria\n\nMission goals and objectives.1. \n\nAnalysis of alternative concepts to show 2. \nat least one is feasible.\n\nConOps.3. \n\nPreliminary mission descope options.4. \n\nPreliminary risk assessment including 5. \ntechnologies and associated risk \nmanagement/mitigation strategies and \noptions.\n\nConceptual test and evaluation strategy.6. \n\nPreliminary technical plans to achieve 7. \nnext phase.\n\nDefined MOEs and MOPs.8. \n\nConceptual life-cycle support strategies 9. \n(logistics, manufacturing, operation, etc.).\n\nMission objectives are clearly defined and stated and are unambiguous 1. \nand internally consistent.\n\nThe preliminary set of requirements satisfactorily provides a system 2. \nthat will meet the mission objectives.\n\nThe mission is feasible. A solution has been identified that is technically 3. \nfeasible. A rough cost estimate is within an acceptable cost range.\n\nThe concept evaluation criteria to be used in candidate systems evalu-4. \nation have been identified and prioritized.\n\nThe need for the mission has been clearly identified.5. \n\nThe cost and schedule estimates are credible.6. \n\nAn updated technical search was done to identify existing assets or 7. \nproducts that could satisfy the mission or parts of the mission.\n\nTechnical planning is sufficient to proceed to the next phase.8. \n\nRisk and mitigation strategies have been identified and are acceptable 9. \nbased on technical assessments.\n\nMission Concept Review\nThe MCR will affirm the mission need and examine \nthe proposed mission\u2019s objectives and the concept for \nmeeting those objectives. It is an internal review that \nusually occurs at the cognizant organization for system \ndevelopment. The MCR should be completed prior to \nentering the concept development phase (Phase A).\n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \n\nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \nEnsure issues raised during the review are appropriately  ?\ndocumented and a plan for resolution is prepared.\n\nResults of Review\nA successful MCR supports the determination that the \nproposed mission meets the customer need, and has \nsufficient quality and merit to support a field Center \nmanagement decision to propose further study to the \ncognizant NASA program associate administrator as a \ncandidate Phase A effort.\n\n\n\n174 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nTable 6.7?5 SRR Entrance and Success Criteria\n\nSystem Requirements Review\n\nEntrance Criteria Success Criteria\n\nSuccessful completion of the MCR and responses made to all MCR RFAs and 1. \nRIDs.\n\nA preliminary SRR agenda, success criteria, and charge to the board have been 2. \nagreed to by the technical team, project manager, and review chair prior to the SRR.\n\nThe following technical products for hardware and software system elements 3. \nare available to the cognizant participants prior to the review:\n\nsystem requirements document;a. \nsystem software functionality description;b. \nupdated ConOps;c. \nupdated mission requirements, if applicable;d. \nbaselined SEMP;e. \nrisk management plan;f. \npreliminary system requirements allocation to the next lower level system;g. \nupdated cost estimate;h. \ntechnology development maturity assessment plan;i. \nupdated risk assessment and mitigations (including PRA, as applicable);j. \nlogistics documentation (e.g., preliminary maintenance plan);k. \npreliminary human rating plan, if applicable;l. \nsoftware development plan;m. \nsystem SMA plan;n. \nCM plan;o. \ninitial document tree;p. \nverification and validation approach;q. \npreliminary system safety analysis; andr. \nother specialty disciplines, as required.s. \n\nThe project utilizes a sound 1. \nprocess for the allocation \nand control of requirements \nthroughout all levels, and a plan \nhas been defined to complete \nthe definition activity within \nschedule constraints.\n\nRequirements definition is 2. \ncomplete with respect to \ntop-level mission and science \nrequirements, and interfaces with \nexternal entities and between \nmajor internal elements have \nbeen defined.\n\nRequirements allocation and 3. \nflowdown of key driving require-\nments have been defined down \nto subsystems.\n\nPreliminary approaches have 4. \nbeen determined for how \nrequirements will be verified \nand validated down to the \nsubsystem level.\n\nMajor risks have been identified 5. \nand technically assessed, and \nviable mitigation strategies have \nbeen defined.\n\nSystem Requirements Review\nThe SRR examines the functional and performance re-\nquirements defined for the system and the preliminary \nprogram or project plan and ensures that the require-\nments and selected concept will satisfy the mission.The \nSRR is conducted during the concept development phase \n(Phase A) and before conducting the SDR or MDR.\n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \n\nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \nEnsure issues raised during the review are appropri- ?\nately documented and a plan for resolution is pre-\npared.\n\nResults of Review\nSuccessful completion of the SRR freezes program/\nproject requirements and leads to a formal decision by \nthe cognizant program associate administrator to pro-\nceed with proposal request preparations for project im-\nplementation.\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 175\n\nMission Definition Review (Robotic Missions \nOnly)\n\nThe MDR examines the proposed requirements, the \nmission architecture, and the flowdown to all functional \nelements of the mission to ensure that the overall con-\ncept is complete, feasible, and consistent with available \nresources.\n\nMDR is conducted during the concept development \nphase (Phase A) following completion of the concept \nstudies phase (Pre-Phase A) and before the preliminary \ndesign phase (Phase B). \n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \nEnsure issues raised during the review are appropri- ?\nately documented and a plan for resolution is pre-\npared.\n\nResults of Review\nA successful MDR supports the decision to further de-\nvelop the system architecture/design and any technology \nneeded to accomplish the mission. The results reinforce \nthe mission\u2019s merit and provide a basis for the system ac-\nquisition strategy.\n\nTable 6.7?6 MDR Entrance and Success Criteria\n\nMission Definition Review\n\nEntrance Criteria Success Criteria\n\nSuccessful completion of the SRR and responses made to all SRR RFAs 1. \nand RIDs.\n\nA preliminary MDR agenda, success criteria, and charge to the board 2. \nhave been agreed to by the technical team, project manager, and review \nchair prior to the MDR.\n\nThe following technical products for hardware and software system ele-3. \nments are available to the cognizant participants prior to the review:\n\nsystem architecture;a. \nupdated system requirements document, if applicable;b. \nsystem software functionality description;c. \nupdated ConOps, if applicable;d. \nupdated mission requirements, if applicable;e. \nupdated SEMP, if applicable;f. \nupdated risk management plan, if applicable;g. \ntechnology development maturity assessment plan;h. \npreferred system solution definition including major trades and i. \noptions;\nupdated risk assessment and mitigations (including PRA, as ap-j. \nplicable);\nupdated cost and schedule data;k. \nlogistics documentation (e.g., preliminary maintenance plan);l. \nsoftware development plan;m. \nsystem SMA plan;n. \nCM plan;o. \nupdated initial document tree, if applicable;p. \npreliminary system safety analysis; andq. \nother specialty disciplines as required.r. \n\nThe resulting overall concept is reason-1. \nable, feasible, complete, responsive \nto the mission requirements, and is \nconsistent with system requirements \nand available resources (cost, schedule, \nmass, and power).\n\nSystem and subsystem design ap-2. \nproaches and operational concepts \nexist and are consistent with the \nrequirements set.\n\nThe requirements, design approaches, 3. \nand conceptual design will fulfill the \nmission needs within the estimated \ncosts.\n\nMajor risks have been identified and 4. \ntechnically assessed, and viable mitiga-\ntion strategies have been defined.\n\n\n\n176 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nTable 6.7?7 SDR Entrance and Success Criteria\n\nSystem Definition Review\n\nEntrance Criteria Success Criteria\n\nSuccessful completion of the SRR and responses made to all 1. \nSRR RFAs and RIDs.\n\nA preliminary SDR agenda, success criteria, and charge to the 2. \nboard have been agreed to by the technical team, project \nmanager, and review chair prior to the SDR.\n\nSDR technical products listed below for both hardware and 3. \nsoftware system elements have been made available to the \ncognizant participants prior to the review:\n\nsystem architecture; a. \npreferred system solution definition including major b. \ntrades and options;\nupdated baselined documentation, as required;c. \npreliminary functional baseline (with supporting tradeoff d. \nanalyses and data);\npreliminary system software functional requirements;e. \nSEMP changes, if any;f. \nupdated risk management plan;g. \nupdated risk assessment and mitigations (including PRA, h. \nas applicable);\nupdated technology development maturity assessment i. \nplan;\nupdated cost and schedule data;j. \nupdated logistics documentation;k. \nbased on system complexity, updated human rating plan;l. \nsoftware test plan;m. \nsoftware requirements document(s);n. \ninterface requirements documents (including software);o. \ntechnical resource utilization estimates and margins;p. \nupdated SMA plan; andq. \nupdated preliminary safety analysis.r. \n\nSystems requirements, including mission success 1. \ncriteria and any sponsor-imposed constraints, \nare defined and form the basis for the proposed \nconceptual design.\n\nAll technical requirements are allocated, and 2. \nthe flowdown to subsystems is adequate. The \nrequirements, design approaches, and conceptual \ndesign will fulfill the mission needs consistent with \nthe available resources (cost, schedule, mass, and \npower). \n\nThe requirements process is sound and can 3. \nreasonably be expected to continue to identify \nand flow detailed requirements in a manner timely \nfor development.\n\nThe technical approach is credible and responsive 4. \nto the identified requirements.\n\nTechnical plans have been updated, as necessary.5. \n\nThe tradeoffs are completed, and those planned 6. \nfor Phase B adequately address the option space.\n\nSignificant development, mission, and safety risks 7. \nare identified and technically assessed, and a risk \nprocess and resources exist to manage the risks.\n\nAdequate planning exists for the development of 8. \nany enabling new technology.\n\nThe ConOps is consistent with proposed design 9. \nconcept(s) and is in alignment with the mission \nrequirements.\n\nSystem Definition Review (Human Space \nFlight Missions Only)\n\nThe SDR examines the proposed system architecture/de-\nsign and the flowdown to all functional elements of the \nsystem. SDR is conducted at the end of the concept de-\nvelopment phase (Phase A) and before the preliminary \ndesign phase (Phase B) begins.\n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \n\nEnsure issues raised during the review are appropri- ?\nately documented and a plan for resolution is pre-\npared.\n\nResults of Review\nAs a result of successful completion of the SDR, the \nsystem and its operation are well enough understood \nto warrant design and acquisition of the end items. Ap-\nproved specifications for the system, its segments, and \npreliminary specifications for the design of appropriate \nfunctional elements may be released. A configuration \nmanagement plan is established to control design and \nrequirement changes. Plans to control and integrate the \nexpanded technical process are in place.\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 177\n\nPreliminary Design Review\nThe PDR demonstrates that the preliminary design \nmeets all system requirements with acceptable risk and \nwithin the cost and schedule constraints and establishes \nthe basis for proceeding with detailed design. It will show \nthat the correct design options have been selected, inter-\nfaces have been identified, approximately 10 percent of \nengineering drawings have been created, and verifica-\ntion methods have been described. PDR occurs near the \ncompletion of the preliminary design phase (Phase B) as \nthe last review in the Formulation phase.\n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \nEnsure issues raised during the review are appropri- ?\nately documented and a plan for resolution is pre-\npared.\n\nResults of Review\nAs a result of successful completion of the PDR, the de-\nsign-to baseline is approved. A successful review result \nalso authorizes the project to proceed into implementa-\ntion and toward final design.\n\nTable 6.7?8 PDR Entrance and Success Criteria\n\nPreliminary Design Review\n\nEntrance Criteria Success Criteria\n\nSuccessful completion of the SDR or MDR and responses made to all 1. \nSDR or MDR RFAs and RIDs, or a timely closure plan exists for those \nremaining open.\n\nA preliminary PDR agenda, success criteria, and charge to the board 2. \nhave been agreed to by the technical team, project manager, and \nreview chair prior to the PDR.\n\nPDR technical products listed below for both hardware and software 3. \nsystem elements have been made available to the cognizant partici-\npants prior to the review:\n\nUpdated baselined documentation, as required.a. \nPreliminary subsystem design specifications for each configuration b. \nitem (hardware and software), with supporting tradeoff analyses \nand data, as required. The preliminary software design specification \nshould include a completed definition of the software architecture \nand a preliminary database design description as applicable.\nUpdated technology development maturity assessment plan.c. \nUpdated risk assessment and mitigation.d. \nUpdated cost and schedule data.e. \nUpdated logistics documentation, as required.f. \nApplicable technical plans (e.g., technical performance measure-g. \nment plan, contamination control plan, parts management plan, \nenvironments control plan, EMI/EMC control plan, payload-to-car-\nrier integration plan, producibility/manufacturability program plan, \nreliability program plan, quality assurance plan).\nApplicable standards.h. \nSafety analyses and plans.i. \nEngineering drawing tree.j. \nInterface control documents.k. \nVerification and validation plan.l. \nPlans to respond to regulatory (e.g., National Environmental Policy m. \nAct) requirements, as required.\nDisposal plan.n. \nTechnical resource utilization estimates and margins.o. \nSystem-level safety analysis.p. \nPreliminary LLIL.q. \n\nThe top-level requirements\u2014including mission 1. \nsuccess criteria, TPMs, and any sponsor-imposed con-\nstraints\u2014are agreed upon, finalized, stated clearly, \nand consistent with the preliminary design.\n\nThe flowdown of verifiable requirements is complete 2. \nand proper or, if not, an adequate plan exists for \ntimely resolution of open items. Requirements are \ntraceable to mission goals and objectives.\n\nThe preliminary design is expected to meet the 3. \nrequirements at an acceptable level of risk.\n\nDefinition of the technical interfaces is consistent 4. \nwith the overall technical maturity and provides an \nacceptable level of risk.\n\nAdequate technical interfaces are consistent with the 5. \noverall technical maturity and provide an acceptable \nlevel of risk.\n\nAdequate technical margins exist with respect to 6. \nTPMs.\n\nAny required new technology has been developed 7. \nto an adequate state of readiness, or backup options \nexist and are supported to make them a viable \nalternative.\n\nThe project risks are understood and have been 8. \ncredibly assessed, and plans, a process, and resources \nexist to effectively manage them.\n\nSMA (e.g., safety, reliability, maintainability, quality, 9. \nand EEE parts) has been adequately addressed in pre-\nliminary designs and any applicable SMA products \n(e.g., PRA, system safety analysis, and failure modes \nand effects analysis) have been approved.\n\nThe operational concept is technically sound, 10. \nincludes (where appropriate) human factors, and \nincludes the flowdown of requirements for its execu-\ntion.\n\n\n\n178 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nTable 6.7?9 CDR Entrance and Success Criteria\n\nCritical Design Review\n\nEntrance Criteria Success Criteria\n\nSuccessful completion of the PDR and responses made to all PDR RFAs and RIDs, 1. \nor a timely closure plan exists for those remaining open.\n\nA preliminary CDR agenda, success criteria, and charge to the board have been 2. \nagreed to by the technical team, project manager, and review chair prior to the CDR.\n\nCDR technical work products listed below for both hardware and software 3. \nsystem elements have been made available to the cognizant participants prior to \nthe review:\n\nupdated baselined documents, as required;a. \nproduct build-to specifications for each hardware and software configuration b. \nitem, along with supporting tradeoff analyses and data;\nfabrication, assembly, integration, and test plans and procedures;c. \ntechnical data package (e.g., integrated schematics, spares provisioning list, d. \ninterface control documents, engineering analyses, and specifications);\noperational limits and constraints;e. \ntechnical resource utilization estimates and margins;f. \nacceptance criteria;g. \ncommand and telemetry list;h. \nverification plan (including requirements and specifications);i. \nvalidation plan;j. \nlaunch site operations plan;k. \ncheckout and activation plan;l. \ndisposal plan (including decommissioning or termination);m. \nupdated technology development maturity assessment plan;n. \nupdated risk assessment and mitigation;o. \nupdate reliability analyses and assessments;p. \nupdated cost and schedule data;q. \nupdated logistics documentation;r. \nsoftware design document(s) (including interface design documents);s. \nupdated LLIL;t. \nsubsystem-level and preliminary operations safety analyses;u. \nsystem and subsystem certification plans and requirements (as needed); andv. \nsystem safety analysis with associated verifications.w. \n\nThe detailed design is expected to meet the 1. \nrequirements with adequate margins at an \nacceptable level of risk.\n\nInterface control documents are appropri-2. \nately matured to proceed with fabrication, \nassembly, integration, and test, and plans \nare in place to manage any open items.\n\nHigh confidence exists in the product 3. \nbaseline, and adequate documentation \nexists or will exist in a timely manner to al-\nlow proceeding with fabrication, assembly, \nintegration, and test.\n\nThe product verification and product valida-4. \ntion requirements and plans are complete.\n\nThe testing approach is comprehensive, 5. \nand the planning for system assembly, \nintegration, test, and launch site and mis-\nsion operations is sufficient to progress into \nthe next phase.\n\nAdequate technical and programmatic 6. \nmargins and resources exist to complete \nthe development within budget, schedule, \nand risk constraints.\n\nRisks to mission success are understood and 7. \ncredibly assessed, and plans and resources \nexist to effectively manage them.\n\nSMA (e.g., safety, reliability, maintain-8. \nability, quality, and EEE parts) have been \nadequately addressed in system and opera-\ntional designs, and any applicable SMA plan \nproducts (e.g., PRA, system safety analysis, \nand failure modes and effects analysis) have \nbeen approved.\n\nCritical Design Review\nThe purpose of the CDR is to demonstrate that the ma-\nturity of the design is appropriate to support proceeding \nwith full scale fabrication, assembly, integration, and \ntest, and that the technical effort is on track to complete \nthe flight and ground system development and mission \noperations to meet mission performance requirements \nwithin the identified cost and schedule constraints. Ap-\nproximately 90 percent of engineering drawings are ap-\nproved and released for fabrication. CDR occurs during \nthe final design phase (Phase C).\n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \n\nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \nEnsure issues raised during the review are appropri- ?\nately documented and a plan for resolution is pre-\npared.\n\nResults of Review\nAs a result of successful completion of the CDR, the \nbuild-to baseline, production, and verification plans \nare approved. A successful review result also authorizes \ncoding of deliverable software (according to the build-\nto baseline and coding standards presented in the re-\nview), and system qualification testing and integration. \nAll open issues should be resolved with closure actions \nand schedules.\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 179\n\nProduction Readiness Review\nA PRR is held for FS&GS projects developing or ac-\nquiring multiple or similar systems greater than three or \nas determined by the project. The PRR determines the \nreadiness of the system developers to efficiently produce \nthe required number of systems. It ensures that the pro-\nduction plans; fabrication, assembly, and integration-en-\nabling products; and personnel are in place and ready \nto begin production. PRR occurs during the final design \nphase (Phase C).\n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \n\nEnsure the products meet the entrance criteria and  ?\nsuccess criteria.\nEnsure issues raised during the review are appropriately  ?\ndocumented and a plan for resolution is prepared.\n\nResults of Review\nAs a result of successful completion of the PRR, the final \nproduction build-to baseline, production, and verifica-\ntion plans are approved. Approved drawings are released \nand authorized for production. A successful review re-\nsult also authorizes coding of deliverable software (ac-\ncording to the build-to baseline and coding standards \npresented in the review), and system qualification testing \nand integration. All open issues should be resolved with \nclosure actions and schedules.\n\nTable 6.7?10 PRR Entrance and Success Criteria\n\nProduction Readiness Review\n\nEntrance Criteria Success Criteria\n\nThe significant production engi-1. \nneering problems encountered \nduring development are resolved.\n\nThe design documentation is 2. \nadequate to support production.\n\nThe production plans and prepa-3. \nration are adequate to begin \nfabrication.\n\nThe production-enabling prod-4. \nucts and adequate resources are \navailable, have been allocated, \nand are ready to support end \nproduct production.\n\nThe design is appropriately certified.1. \n\nThe system requirements are fully met in the final production configuration.2. \n\nAdequate measures are in place to support production.3. \n\nDesign-for-manufacturing considerations ensure ease and efficiency of 4. \nproduction and assembly.\n\nRisks have been identified, credibly assessed, and characterized; and mitigation 5. \nefforts have been defined.\n\nThe bill of materials has been reviewed and critical parts identified.6. \n\nDelivery schedules have been verified.7. \n\nAlternative sources for resources have been identified, as appropriate.8. \n\nAdequate spares have been planned and budgeted.9. \n\nRequired facilities and tools are sufficient for end product production.10. \n\nSpecified special tools and test equipment are available in proper quantities.11. \n\nProduction and support staff are qualified.12. \n\nDrawings are certified.13. \n\nProduction engineering and planning are sufficiently mature for cost-effective 14. \nproduction.\n\nProduction processes and methods are consistent with quality requirements 15. \nand compliant with occupational safety, environmental, and energy conserva-\ntion regulations.\n\nQualified suppliers are available for materials that are to be procured.16. \n\n\n\n180 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nTable 6.7?11 SIR Entrance and Success Criteria\n\nSystem Integration Review\n\nEntrance Criteria Success Criteria\n\nIntegration plans and procedures have been completed and 1. \napproved.\n\nSegments and/or components are available for integration.2. \n\nMechanical and electrical interfaces have been verified against the 3. \ninterface control documentation.\n\nAll applicable functional, unit-level, subsystem, and qualification 4. \ntesting has been conducted successfully.\n\nIntegration facilities, including clean rooms, ground support 5. \nequipment, handling fixtures, overhead cranes, and electrical test \nequipment, are ready and available.\n\nSupport personnel have been adequately trained.6. \n\nHandling and safety requirements have been documented.7. \n\nAll known system discrepancies have been identified and disposed 8. \nin accordance with an agreed-upon plan. \n\nAll previous design review success criteria and key issues have 9. \nbeen satisfied in accordance with an agreed-upon plan.\n\nThe quality control organization is ready to support the integration 10. \neffort.\n\nAdequate integration plans and procedures 1. \nare completed and approved for the system \nto be integrated.\n\nPrevious component, subsystem, and system 2. \ntest results form a satisfactory basis for \nproceeding to integration.\n\nRisk level is identified and accepted by 3. \nprogram/project leadership, as required.\n\nThe integration procedures and workflow 4. \nhave been clearly defined and documented.\n\nThe review of the integration plans, as well 5. \nas the procedures, environment, and the \nconfiguration of the items to be integrated, \nprovides a reasonable expectation that the \nintegration will proceed successfully.\n\nIntegration personnel have received appro-6. \npriate training in the integration and safety \nprocedures.\n\nSystem Integration Review\nAn SIR ensures that the system is ready to be integrated. \nSegments, components, and subsystems are available \nand ready to be integrated into the system. Integration \nfacilities, support personnel, and integration plans and \nprocedures are ready for integration. SIR is conducted \nat the end of the final design phase (Phase C) and be-\nfore the systems assembly, integration, and test phase \n(Phase D) begins. \n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \n\nEnsure the products meet the entrance criteria and  ?\nsuccess criteria.\nEnsure issues raised during the review are appropriately  ?\ndocumented and a plan for resolution is prepared.\n\nResults of Review\nAs a result of successful completion of the SIR, the final \nas-built baseline and verification plans are approved. Ap-\nproved drawings are released and authorized to support \nintegration. All open issues should be resolved with clo-\nsure actions and schedules. The subsystems/systems in-\ntegration procedures, ground support equipment, facili-\nties, logistical needs, and support personnel are planned \nfor and are ready to support integration.\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 181\n\nTest Readiness Review\nA TRR ensures that the test article (hardware/software), \ntest facility, support personnel, and test procedures are \nready for testing and data acquisition, reduction, and \ncontrol. A TRR is held prior to commencement of verifi-\ncation or validation testing.\n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \n\nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \nEnsure issues raised during the review are appropri- ?\nately documented and a plan for resolution is pre-\npared.\n\nResults of Review\nA successful TRR signifies that test and safety engineers \nhave certified that preparations are complete, and that \nthe project manager has authorized formal test initia-\ntion.\n\nTable 6.7?12 TRR Entrance and Success Criteria\n\nTest Readiness Review\n\nEntrance Criteria Success Criteria\n\nThe objectives of the testing have been clearly defined and 1. \ndocumented and all of the test plans, procedures, environment, \nand the configuration of the test item(s) support those objec-\ntives.\n\nConfiguration of the system under test has been defined and 2. \nagreed to. All interfaces have been placed under configuration \nmanagement or have been defined in accordance with an \nagreed-to plan, and a version description document has been \nmade available to TRR participants prior to the review.\n\nAll applicable functional, unit-level, subsystem, system, and 3. \nqualification testing has been conducted successfully.\n\nAll TRR-specific materials such as test plans, test cases, and 4. \nprocedures have been made available to all participants prior to \nconducting the review.\n\nAll known system discrepancies have been identified and 5. \ndisposed in accordance with an agreed-upon plan.\n\nAll previous design review success criteria and key issues have 6. \nbeen satisfied in accordance with an agreed-upon plan.\n\nAll required test resources people (including a designated test 7. \ndirector), facilities, test articles, test instrumentation, and other \nenabling products have been identified and are available to \nsupport required tests.\n\nRoles and responsibilities of all test participants are defined and 8. \nagreed to.\n\nTest contingency planning has been accomplished, and all 9. \npersonnel have been trained.\n\nAdequate test plans are completed and ap-1. \nproved for the system under test.\n\nAdequate identification and coordination of 2. \nrequired test resources are completed.\n\nPrevious component, subsystem, and system 3. \ntest results form a satisfactory basis for pro-\nceeding into planned tests.\n\nRisk level is identified and accepted by pro-4. \ngram/competency leadership as required.\n\nPlans to capture any lessons learned from the 5. \ntest program are documented.\n\nThe objectives of the testing have been clearly 6. \ndefined and documented, and the review of \nall the test plans, as well as the procedures, \nenvironment, and the configuration of the test \nitem, provide a reasonable expectation that the \nobjectives will be met.\n\nThe test cases have been reviewed and ana-7. \nlyzed for expected results, and the results are \nconsistent with the test plans and objectives.\n\nTest personnel have received appropriate train-8. \ning in test operation and safety procedures.\n\n\n\n182 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nTable 6.7?13 SAR Entrance and Success Criteria\n\nSystem Acceptance Review\n\nEntrance Criteria Success Criteria\n\nA preliminary agenda has been coordinated (nominally) prior to the 1. \nSAR.\n\nThe following SAR technical products have been made available to the 2. \ncognizant participants prior to the review:\n\nresults of the SARs conducted at the major suppliers;a. \ntransition to production and/or manufacturing plan;b. \nproduct verification results;c. \nproduct validation results;d. \ndocumentation that the delivered system complies with the e. \nestablished acceptance criteria;\ndocumentation that the system will perform properly in the f. \nexpected operational environment;\ntechnical data package updated to include all test results;g. \ncertification package;h. \nupdated risk assessment and mitigation;i. \nsuccessfully completed previous milestone reviews; andj. \nremaining liens or unclosed actions and plans for closure.k. \n\nRequired tests and analyses are 1. \ncomplete and indicate that the system \nwill perform properly in the expected \noperational environment.\n\nRisks are known and manageable.2. \n\nSystem meets the established accep-3. \ntance criteria.\n\nRequired safe shipping, handling, 4. \ncheckout, and operational plans and \nprocedures are complete and ready for \nuse.\n\nTechnical data package is complete and 5. \nreflects the delivered system.\n\nAll applicable lessons learned for 6. \norganizational improvement and system \noperations are captured.\n\nSystem Acceptance Review\nThe SAR verifies the completeness of the specific end \nproducts in relation to their expected maturity level and \nassesses compliance to stakeholder expectations. The \nSAR examines the system, its end products and docu-\nmentation, and test data and analyses that support ver-\nification. It also ensures that the system has sufficient \ntechnical maturity to authorize its shipment to the desig-\nnated operational facility or launch site.\n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \n\nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \n\nEnsure issues raised during the review are appropri- ?\nately documented and a plan for resolution is pre-\npared.\n\nResults of Review\nAs a result of successful completion of the SAR, the \nsystem is accepted by the buyer, and authorization is \ngiven to ship the hardware to the launch site or opera-\ntional facility, and to install software and hardware for \noperational use.\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 183\n\nOperational Readiness Review\nThe ORR examines the actual system characteristics and \nthe procedures used in the system or end product\u2019s op-\neration and ensures that all system and support (flight \nand ground) hardware, software, personnel, procedures, \nand user documentation accurately reflect the deployed \nstate of the system. \n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \nEnsure issues raised during the review are appropriately  ?\ndocumented and a plan for resolution is prepared.\n\nResults of Review\nAs a result of successful ORR completion, the system is \nready to assume normal operations.\n\nTable 6.7?14 ORR Entrance and Success Criteria \n\nOperational Readiness Review\n\nEntrance Criteria Success Criteria\n\nAll validation testing has been completed.1. \n\nTest failures and anomalies from validation testing have been 2. \nresolved and the results incorporated into all supporting and \nenabling operational products.\n\nAll operational supporting and enabling products (e.g., 3. \nfacilities, equipment, documents, updated databases) that are \nnecessary for the nominal and contingency operations have \nbeen tested and delivered/installed at the site(s) necessary to \nsupport operations.\n\nOperations handbook has been approved.4. \n\nTraining has been provided to the users and operators on the 5. \ncorrect operational procedures for the system.\n\nOperational contingency planning has been accomplished, 6. \nand all personnel have been trained.\n\nThe system, including any enabling products, 1. \nis determined to be ready to be placed in an \noperational status.\n\nAll applicable lessons learned for organizational 2. \nimprovement and systems operations have been \ncaptured.\n\nAll waivers and anomalies have been closed.3. \n\nSystems hardware, software, personnel, and 4. \nprocedures are in place to support operations.\n\n\n\n184 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nFlight Readiness Review\nThe FRR examines tests, demonstrations, analyses, and \naudits that determine the system\u2019s readiness for a safe \nand successful flight or launch and for subsequent flight \noperations. It also ensures that all flight and ground \nhardware, software, personnel, and procedures are op-\nerationally ready. \n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \n\nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \nEnsure issues raised during the review are appropri- ?\nately documented and a plan for resolution is pre-\npared.\n\nResults of Review\nAs a result of successful FRR completion, technical and \nprocedural maturity exists for system launch and flight \nauthorization and in some cases initiation of system op-\nerations.\n\nTable 6.7?15 FRR Entrance and Success Criteria \n\nFlight Readiness Review\n\nEntrance Criteria Success Criteria\n\nReceive certification that flight operations can 1. \nsafely proceed with acceptable risk. \n\nThe system and support elements have been con-2. \nfirmed as properly configured and ready for flight. \n\nInterfaces are compatible and function as expected.3. \n\nThe system state supports a launch Go decision 4. \nbased on Go or No-Go criteria.\n\nFlight failures and anomalies from previously 5. \ncompleted flights and reviews have been resolved \nand the results incorporated into all supporting and \nenabling operational products.\n\nThe system has been configured for flight.6. \n\nThe flight vehicle is ready for flight.1. \n\nThe hardware is deemed acceptably safe for flight (i.e., meet-2. \ning the established acceptable risk criteria or documented as \nbeing accepted by the PM and DGA).\n\nFlight and ground software elements are ready to support 3. \nflight and flight operations.\n\nInterfaces are checked out and found to be functional.4. \n\nOpen items and waivers have been examined and found to 5. \nbe acceptable.\n\nThe flight and recovery environmental factors are within 6. \nconstraints.\n\nAll open safety and mission risk items have been addressed.7. \n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 185\n\nPost?Launch Assessment Review\nA PLAR is a post-deployment evaluation of the readi-\nness of the spacecraft systems to proceed with full, rou-\ntine operations. The review evaluates the status, perfor-\nmance, and capabilities of the project evident from the \nflight operations experience since launch. This can also \nmean assessing readiness to transfer responsibility from \nthe development organization to the operations organi-\nzation. The review also evaluates the status of the project \nplans and the capability to conduct the mission with \nemphasis on near-term operations and mission-critical \n\nevents. This review is typically held after the early flight \noperations and initial checkout.\n\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \nEnsure issues raised during the review are appropriately  ?\ndocumented and a plan for resolution is prepared.\n\nTable 6.7?16 PLAR Entrance and Success Criteria \n\nPost?Launch Assessment Review\n\nEntrance Criteria Success Criteria\n\nThe launch and early operations performance, including (when ap-1. \npropriate) the early propulsive maneuver results, are available.\n\nThe observed spacecraft and science instrument performance, includ-2. \ning instrument calibration plans and status, are available. \n\nThe launch vehicle performance assessment and mission implications, 3. \nincluding launch sequence assessment and launch operations experi-\nence with lessons learned, are completed.\n\nThe mission operations and ground data system experience, including 4. \ntracking and data acquisition support and spacecraft telemetry data \nanalysis, are available.\n\nThe mission operations organization, including status of staffing, 5. \nfacilities, tools, and mission software (e.g., spacecraft analysis, and \nsequencing), is available.\n\nIn-flight anomalies and the responsive actions taken, including any 6. \nautonomous fault protection actions taken by the spacecraft, or any \nunexplained spacecraft telemetry, including alarms, are documented.\n\nThe need for significant changes to procedures, interface agreements, 7. \nsoftware, and staffing has been documented.\n\nDocumentation is updated, including any updates originating from the 8. \nearly operations experience.\n\nFuture development/test plans are developed.9. \n\nThe observed spacecraft and science 1. \npayload performance agrees with predic-\ntion, or if not, it is adequately understood \nso that future behavior can be predicted \nwith confidence.\n\nAll anomalies have been adequately 2. \ndocumented, and their impact on \noperations assessed. Further, anomalies \nimpacting spacecraft health and safety \nor critical flight operations have been \nproperly disposed.\n\nThe mission operations capabilities, 3. \nincluding staffing and plans, are ad-\nequate to accommodate the actual flight \nperformance.\n\nLiens, if any, on operations, identified as 4. \npart of the ORR, have been satisfactorily \ndisposed.\n\n\n\n186 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nCritical Event Readiness Review\nA CERR confirms the project\u2019s readiness to execute the \nmission\u2019s critical activities during flight operation. \n\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \nEnsure issues raised during the review are appropriately  ?\ndocumented and a plan for resolution is prepared.\n\nPost?Flight Assessment Review\nThe PFAR evaluates the activities from the flight after re-\ncovery. The review identifies all anomalies that occurred \nduring the flight and mission and determines the actions \nnecessary to mitigate or resolve the anomalies for future \nflights.\n\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \nEnsure issues raised during the review are appropriately  ?\ndocumented and a plan for resolution is prepared.\n\nTable 6.7?18 PFAR Entrance and Success Criteria\n\nPost?Flight Assessment Review\n\nEntrance Criteria Success Criteria\n\nAll anomalies that occurred 1. \nduring the mission, as well \nas during preflight testing, \ncountdown, and ascent, \nidentified.\n\nReport on overall post-recov-2. \nery condition.\n\nReport any evidence of ascent 3. \ndebris.\n\nAll photo and video docu-4. \nmentation available.\n\nRetention plans for scrapped 5. \nhardware completed.\n\nPost-flight assessment team 6. \noperating plan completed.\n\nDisassembly activities 7. \nplanned and scheduled.\n\nProcesses and controls to 8. \ncoordinate in-flight anomaly \ntroubleshooting and post-\nflight data preservation \ndeveloped.\n\nProblem reports, corrective 9. \naction requests, post-flight \nanomaly records, and final \npost-flight documentation \ncompleted.\n\nAll post-flight hardware and 10. \nflight data evaluation reports \ncompleted.\n\nFormal final 1. \nreport docu-\nmenting flight \nperformance and \nrecommenda-\ntions for future \nmissions. \n\nAll anomalies 2. \nhave been \nadequately \ndocumented and \ndisposed. \n\nThe impact of 3. \nanomalies on \nfuture flight \noperations has \nbeen assessed. \n\nPlans for retain-4. \ning assessment \ndocumentation \nand imaging \nhave been made.\n\nReports and 5. \nother docu-\nmentation have \nbeen added to \na database for \nperformance \ncomparison and \ntrending.\n\nTable 6.7?17 CERR Entrance and Success Criteria\n\nCritical Event Readiness Review\n\nEntrance Criteria Success Criteria\n\nMission overview and 1. \ncontext for the critical \nevent(s).\n\nActivity requirements and 2. \nconstraints.\n\nCritical activity sequence 3. \ndesign description \nincluding key tradeoffs \nand rationale for selected \napproach.\n\nFault protection strategy.4. \n\nCritical activity operations 5. \nplan including planned \nuplinks and criticality.\n\nSequence verification 6. \n(testing, walk-throughs, \npeer review) and critical \nactivity validation.\n\nOperations team training 7. \nplan and readiness report.\n\nRisk areas and mitigations.8. \n\nSpacecraft readiness 9. \nreport.\n\nOpen items and plans.10. \n\nThe critical activity 1. \ndesign complies \nwith requirements.\n\nThe preparation 2. \nfor the critical \nactivity, including \nthe verification \nand validation, is \nthorough.\n\nThe project (includ-3. \ning all the systems, \nsupporting services, \nand documentation) \nis ready to support \nthe activity.\n\nThe requirements 4. \nfor the successful \nexecution of the \ncritical event(s) \nare complete and \nunderstood and \nhave been flowed \ndown to the ap-\npropriate levels for \nimplementation.\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 187\n\nDecommissioning Review\nThe DR confirms the decision to terminate or decom-\nmission the system and assesses the readiness of the \nsystem for the safe decommissioning and disposal of \nsystem assets. The DR is normally held near the end of \nroutine mission operations upon accomplishment of \nplanned mission objectives. It may be advanced if some \nunplanned event gives rise to a need to prematurely ter-\nminate the mission, or delayed if operational life is ex-\ntended to permit additional investigations. \n\nObjectives\nThe objectives of the review are to:\n\nEnsure a thorough review of the products supporting  ?\nthe review. \nEnsure the products meet the entrance criteria and  ?\nsuccess criteria. \nEnsure issues raised during the review are appropri- ?\nately documented and a plan for resolution is pre-\npared.\n\nResults of Review\nA successful DR completion ensures that the decommis-\nsioning and disposal of system items and processes are \nappropriate and effective.\n\nTable 6.7?19 DR Entrance and Success Criteria\n\nDecommissioning Review\n\nEntrance Criteria Success Criteria\n\nRequirements associated with decom-1. \nmissioning and disposal are defined.\n\nPlans are in place for decommissioning, 2. \ndisposal, and any other removal from \nservice activities. \n\nResources are in place to support de-3. \ncommissioning and disposal activities, \nplans for disposition of project assets, \nand archival of essential mission and \nproject data. \n\nSafety, environmental, and any other 4. \nconstraints are described.\n\nCurrent system capabilities are 5. \ndescribed. \n\nFor off-nominal operations, all contrib-6. \nuting events, conditions, and changes \nto the originally expected baseline are \ndescribed.\n\nThe reasons for decommissioning disposal are documented.1. \n\nThe decommissioning and disposal plan is complete, approved by ap-2. \npropriate management, and compliant with applicable Agency safety, \nenvironmental, and health regulations. Operations plans for all potential \nscenarios, including contingencies, are complete and approved. All \nrequired support systems are available. \n\nAll personnel have been properly trained for the nominal and contin-3. \ngency procedures.\n\nSafety, health, and environmental hazards have been identified. Controls 4. \nhave been verified. \n\nRisks associated with the disposal have been identified and adequately 5. \nmitigated. Residual risks have been accepted by the required manage-\nment.\n\nIf hardware is to be recovered from orbit: 6. \n\nReturn site a. activity plans have been defined and approved.\nRequired facilities are available and meet requirements, including b. \nthose for contamination control, if needed.\n\nc. Transportation plans are defined and approved. Shipping containers \nand handling equipment, as well as contamination and environmen-\ntal control and monitoring devices, are available.\n\nPlans for disposition of mission-owned assets (i.e., hardware, software, 7. \nfacilities) have been defined and approved.\n\nPlans for archival and subsequent analysis of mission data have been 8. \ndefined and approved. Arrangements have been finalized for the \nexecution of such plans. Plans for the capture and dissemination of \nappropriate lessons learned during the project life cycle have been \ndefined and approved. Adequate resources (schedule, budget, and \nstaffing) have been identified and are available to successfully complete \nall decommissioning, disposal, and disposition activities.\n\n\n\n188 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nOther Technical Reviews\n\nThese typical technical reviews are some that have been \nconducted on previous programs and projects but are \nnot required as part of the NPR 7123.1 systems engi-\nneering process.\n\nDesign Certification Review\n\nPurpose\nThe Design Certification Review (DCR) ensures that the \nqualification verifications demonstrate design compli-\nance with functional and performance requirements.\n\nTiming\nThe DCR follows the system CDR, and after qualifica-\ntion tests and all modifications needed to implement \nqualification-caused corrective actions have been com-\npleted.\n\nObjectives\nThe objectives of the review are to:\n\nConfirm that the verification results met functional  ?\nand performance requirements, and that test plans \nand procedures were executed correctly in the speci-\nfied environments.\nCertify that traceability between test article and pro- ?\nduction article is correct, including name, identifica-\ntion number, and current listing of all waivers.\n\nIdentify any incremental tests required or conducted  ?\ndue to design or requirements changes made since test \ninitiation, and resolve issues regarding their results.\n\nCriteria for Successful Completion\nThe following items comprise a checklist to aid in deter-\nmining the readiness of DCR product preparation:\n\nAre the pedigrees of the test articles directly traceable  ?\nto the production units?\nIs the verification plan used for this article current  ?\nand approved?\nDo the test procedures and environments used comply  ?\nwith those specified in the plan?\nAre there any changes in the test article configuration  ?\nor design resulting from the as-run tests? Do they re-\nquire design or specification changes and/or retests?\nHave design and specification documents been au- ?\ndited?\nDo the verification results satisfy functional and per- ?\nformance requirements?\nDo the verification, design, and specification docu- ?\nmentation correlate?\n\nResults of Review\nAs a result of a successful DCR, the end item design is \napproved for production. All open issues should be re-\nsolved with closure actions and schedules.\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 189\n\nFunctional and Physical Configuration Audits\nConfiguration audits confirm that the configured \nproduct is accurate and complete. The two types of con-\nfiguration audits are the Functional Configuration Audit \n(FCA) and the Physical Configuration Audit (PCA). The \nFCA examines the functional characteristics of the con-\nfigured product and verifies that the product has met, via \ntest results, the requirements specified in its functional \nbaseline documentation approved at the PDR and CDR. \nFCAs will be conducted on both hardware or software \nconfigured products and will precede the PCA of the \nconfigured product. The PCA (also known as a configu-\nration inspection) examines the physical configuration \nof the configured product and verifies that the product \ncorresponds to the build-to (or code-to) product base-\nline documentation previously approved at the CDR. \nPCAs will be conducted on both hardware and software \nconfigured products.\n\nTechnical Peer Reviews\nPeer reviews provide the technical insight essential to \nensure product and process quality. Peer reviews are \nfocused, in-depth technical reviews that support the \nevolving design and development of a product, including \n\ncritical documentation or data packages. They are often, \nbut not always, held as supporting reviews for technical \nreviews such as PDR and CDR. A purpose of the peer \nreview is to add value and reduce risk through expert \nknowledge infusion, confirmation of approach, identi-\nfication of defects, and specific suggestions for product \nimprovements. \n\nThe results of the engineering peer reviews comprise a \nkey element of the review process. The results and is-\nsues that surface during these reviews are documented \nand reported out at the appropriate next higher element \nlevel.\n\nThe peer reviewers should be selected from outside the \nproject, but they should have a similar technical back-\nground, and they should be selected for their skill and \nexperience. Peer reviewers should be concerned with \nonly the technical integrity and quality of the product. \nPeer reviews should be kept simple and informal. They \nshould concentrate on a review of the documentation \nand minimize the viewgraph presentations. A round-\ntable format rather than a stand-up presentation is pre-\nferred. The peer reviews should give the full technical \npicture of items being reviewed.\n\nTable 6.7?20 Functional and Physical Configuration Audits \n\nRepresentative Audit Data List\n\nFCA PCA\n\nDesign specifications ?\n\nDesign drawings and parts list ?\n\nEngineering change proposals/engineering change requests ?\n\nDeviation/waiver approval requests incorporated and pending ?\n\nSpecification and drawing tree ?\n\nFracture control plan ?\n\nStructural dynamics, analyses, loads, and models documentation ?\n\nMaterials usage agreements/materials identification usage list ?\n\nVerification and validation requirements, plans, procedures, and  ?\nreports\n\nSoftware requirements and development documents ?\n\nListing of accomplished tests and test results ?\n\nCDR completion documentation including RIDs/RFAs and disposi- ?\ntion reports\n\nAnalysis reports ?\n\nALERT (Acute Launch Emergency Restraint Tip) tracking log ?\n\nHazard analysis/risk assessment ?\n\nFinal version of all specifications ?\n\nProduct drawings and parts list  ?\n\nConfiguration accounting and status reports  ?\n\nFinal version of all software and software docu- ?\nments\n\nCopy of all FCA findings for each product ?\n\nList of approved and outstanding engineering  ?\nchange proposals, engineering change requests, \nand deviation/waiver approval requests\n\nIndentured parts list ?\n\nAs-run test procedures ?\n\nDrawing and specification tree ?\n\nManufacturing and inspection \u201cbuild\u201d records ?\n\nInspection records ?\n\nAs-built discrepancy reports ?\n\nProduct log books ?\n\nAs-built configuration list ?\n\n\n\n190 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nTechnical depth should be established at a level that al-\nlows the review team to gain insight into the technical \nrisks. Rules need to be established to ensure consistency \nin the peer review process. At the conclusion of the re-\nview, a report on the findings, recommendations, and \nactions must be distributed to the technical team.\n\nFor those projects where systems engineering is done \nout-of-house, peer reviews must be part of the contract.\n\nAdditional guidance on establishing and conducting \npeer reviews can be found in Appendix N.\n\n6.7.2.2 Status Reporting and Assessment\n\nThis subsection provides additional information on \nstatus reporting and assessment techniques for costs and \nschedules (including EVM), technical performance, and \nsystems engineering process metrics. \n\nCost and Schedule Control Measures\n\nStatus reporting and assessment on costs and schedules \nprovides the project manager and systems engineer vis-\nibility into how well the project is tracking against its \nplanned cost and schedule targets. From a management \npoint of view, achieving these targets is on a par with \nmeeting the technical performance requirements of the \nsystem. It is useful to think of cost and schedule status \nreporting and assessment as measuring the performance \nof the \u201csystem that produces the system.\u201d\n\nNPR 7120.5 provides specific requirements for the ap-\nplication of EVM to support cost and schedule man-\nagement. EVM is applicable to both in-house and con-\ntracted efforts. The level of EVM system implementation \nwill depend on the dollar value and risk of a project or \ncontract. The standard for EVM systems is ANSI-EIA-\n748. The project manager/systems engineer will use the \nguidelines to establish the program and project EVM \nimplementation plan. \n\nAssessment Methods\nPerformance measurement data are used to assess project \ncost, schedule, and technical performance and their im-\npacts on the completion cost and schedule of the project. \nIn program control terminology, a difference between \nactual performance and planned costs or schedule status \nis called a \u201cvariance.\u201d Variances must be controlled at the \ncontrol account level, which is typically at the subsystem \nWBS level. The person responsible for this activity is fre-\n\nquently called the Control Account Manager (CAM). \nThe CAM develops work and product plans, schedules, \nand time-phased resource plans. The technical sub-\nsystem manager/leads often takes on this role as part of \ntheir subsystem management responsibilities.\n\nFigure 6.7-3 illustrates two types of variances, cost and \nschedule, and some related concepts. A product-ori-\nented WBS divides the project work into discrete tasks \nand products. Associated with each task and product (at \nany level in the WBS) is a schedule and a budgeted (i.e., \nplanned) cost. The Budgeted Cost for Work Scheduled \n(BCWSt) for any set of WBS elements is the sum of the \nbudgeted cost of all work on tasks and products in those \nelements scheduled to be completed by time t. The Bud-\ngeted Cost for Work Performed (BCWPt), also called \nEarned Value (EVt), is the sum of the budgeted cost for \ntasks and products that have actually been produced at \ntime t in the schedule for those WBS elements. The dif-\nference, BCWPt and BCWSt, is called the schedule vari-\nance at time t. A negative value indicates that the work is \nbehind schedule.\n\nThe Actual Cost of Work Performed (ACWPt) repre-\nsents the funds that have been expended up to time t on \nthose WBS elements. The difference between the bud-\ngeted and actual costs, BCWPt  ? ACWPt, is called the \ncost variance at time t. A negative value here indicates a \ncost overrun.\n\nCu\nm\n\nul\nat\n\niv\ne \n\n$\n\n0\nTime t\n\nCost Variance\nto Date\n(BCWP - ACWP)\n\nSchedule\nVariance\nto Date\n(BCWP - BCWS)\n\nForecast\nof Cost\nVariance at\nCompletion\n\nCurrent\nDate\n\nBudget\n\nEAC\n\nBCW\nS\n\nAC\nW\n\nP\n\nBCW\nP\n\n(or E\nV)\n\nFigure 6.7?3 Cost and schedule variances\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 191\n\nWhen either schedule variance or cost variance exceeds \npreestablished control-account-level thresholds that rep-\nresent significant departures from the baseline plan, the \nconditions must be analyzed to identify why the variance \nexists. Once the cause is understood, the CAM can make \nan informed forecast of the time and resources needed \nto complete the control account. When corrective ac-\ntions are feasible (can stay within the BCWS), the plan \nfor implementing them must be included in the analysis. \nSometimes no corrective action is feasible; overruns or \nschedule slips may be unavoidable. One must keep in \nmind that the earlier a technical problem is identified as \na result of schedule or cost variances, the more likely the \nproject team can minimize the impact on completion.\n\nVariances may indicate that the cost Estimate at Com-\npletion (EACt) of the project is likely to be different \nfrom the Budget at Completion (BAC). The difference \nbetween the BAC and the EAC is the Variance at Com-\npletion (VAC). A negative VAC is generally unfavorable, \nwhile a positive is usually favorable. These variances may \nalso point toward a change in the scheduled comple-\ntion date of the project. These types of variances enable \na program analyst to estimate the EAC at any point in \nthe project life cycle. (See box on analyzing EAC.) These \nanalytically derived estimates should be used only as a \n\u201csanity check\u201d against the estimates prepared in the vari-\nance analysis process.\n\nIf the cost and schedule baselines and the technical scope \nof the work are not adequately defined and fully inte-\ngrated, then it is very difficult (or impossible) to estimate \nthe current cost EAC of the project.\n\nOther efficiency factors can be calculated using the per-\nformance measurement data. The Schedule Performance \nIndex (SPI) is a measure of work accomplishment in dol-\nlars. The SPI is calculated by dividing work accomplished \nin dollars or BCWP by the dollar value of the work sched-\nuled or BCWS. Just like any other ratio, a value less than \none is a sign of a behind-schedule condition, equal to one \nindicates an on-schedule status, and greater than one de-\nnotes that work is ahead of schedule. The Cost Perfor-\nmance Index (CPI) is a measure of cost efficiency and is \ncalculated as the ratio of the earned value or BCWP for \na segment of work compared to the cost to complete that \nsame segment of work or ACWP. A CPI will show how \nmuch work is being accomplished for every dollar spent \non the project. A CPI of less than one reveals negative \ncost efficiency, equal to one is right on cost, and greater \n\nthan one is positive. Note that traditional measures com-\npare planned cost to actual cost; however, this compar-\nison is never made using earned value data. Comparing \nplanned to actual costs is an indicator only of spending \nand not of overall project performance.\n\nTechnical Measures\u2014MOEs, MOPs, and TPMs\n\nMeasures of Effectiveness\nMOEs are the \u201coperational\u201d measures of success that are \nclosely related to the achievement of mission or opera-\ntional objectives in the intended operational environ-\nment. MOEs are intended to focus on how well mission \nor operational objectives are achieved, not on how they \nare achieved, i.e., MOEs should be independent of any \nparticular solution. As such, MOEs are the standards \nagainst which the \u201cgoodness\u201d of each proposed solution \nmay be assessed in trade studies and decision analyses. \nMeasuring or calculating MOEs not only makes it pos-\nsible to compare alternative solutions quantitatively, but \nsensitivities to key assumptions regarding operational \nenvironments and to any underlying MOPs can also be \ninvestigated. (See MOP discussion below.)\n\nIn the systems engineering process, MOEs are used to:\n\nDefine high-level operational requirements from the  ?\ncustomer/stakeholder viewpoint.\nCompare and rank alternative solutions in trade  ?\nstudies.\n\nAnalyzing the Estimate at Completion\n\nAn EAC can be estimated at any point in the project \nand should be reviewed at least on a monthly basis. \nThe EAC requires a detailed review by the CAM. A sta-\ntistical estimate can be used as a cross-check of the \nCAM\u2019s estimate and to develop a range to bound the \nestimate. The appropriate formula used to calculate \nthe statistical EAC depends upon the reasons associ-\nated with any variances that may exist. If a variance \nexists due to a one-time event, such as an accident, \nthen EAC = ACWP + (BAC \u2013 BCWP). The CPI and SPI \nshould also be considered in developing the EAC.\n\nIf there is a growing number of liens, action items, or \nsignificant problems that will increase the difficulty \nof future work, the EAC might grow at a greater rate \nthan estimated by the above equation. Such factors \ncould be addressed using risk management methods \ndescribed in the Section 6.4. \n\n\n\n192 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nInvestigate the relative sensitivity of the projected  ?\nmission or operational success to key operational as-\nsumptions and performance parameters.\nDetermine that the mission or operational success  ?\nquantitative objectives remain achievable as system \ndevelopment proceeds. (See TPM discussion below.)\n\nMeasures of Performance\nMOPs are the measures that characterize physical or \nfunctional attributes relating to the system, e.g., engine \nIsp, max thrust, mass, and payload-to-orbit. These attri-\nbutes are generally measured under specified test condi-\ntions or operational environments. MOPs are attributes \ndeemed important in achieving mission or operational \nsuccess, but do not measure it directly. Usually mul-\ntiple MOPs contribute to an MOE. MOPs often become \nsystem performance requirements that, when met by a \ndesign solution, result in achieving a critical threshold \nfor the system MOEs.\n\nThe distinction between MOEs and MOPs is that they \nare formulated from different viewpoints. An MOE re-\nfers to the effectiveness of a solution from the mission \nor operational success criteria expressed by the user/cus-\ntomer/stakeholder. An MOE represents a stakeholder \nexpectation that is critical to the success of the system, \nand failure to attain a critical value for it will cause the \nstakeholder to judge the system a failure. An MOP is a \nmeasure of actual performance of a (supplier\u2019s) partic-\nular design solution, which taken alone may only be in-\ndirectly related to the customer/stakeholder\u2019s concerns.\n\nTechnical Performance Measures\nTPMs are critical or key mission success or performance \nparameters that are monitored during implementation \nby comparing the current actual achievement of the pa-\nrameters with the values that were anticipated for the \ncurrent time and projected for future dates. They are \nused to confirm progress and identify deficiencies that \nmight jeopardize meeting a system requirement or put \nthe project at cost or schedule risk. When a TPM value \nfalls outside the expected range around the anticipated \nvalue, it signals a need for evaluation and corrective ac-\ntion.\n\nIn the systems engineering process, TPMs are used to:\nForecast values to be achieved by critical parameters  ?\nat major milestones or key events during implemen-\ntation.\n\nIdentify differences between the actual and planned  ?\nvalues for those parameters. \nProvide projected values for those parameters in order  ?\nto assess the implications for system effectiveness.\nProvide early warning for emerging risks requiring  ?\nmanagement attention (when negative margins exist).\nProvide early identification of potential opportuni- ?\nties to make design trades that reduce risk or cost, or \nincrease system effectiveness (when positive margins \nexist).\nSupport assessment of proposed design changes. ?\n\nSelecting TPMs\nTPMs are typically selected from the defined set of \nMOEs and MOPs. Understanding that TPM tracking re-\nquires allocation of resources, care should be exercised \nin selecting a small set of succinct TPMs that accurately \nreflect key parameters or risk factors, are readily mea-\nsurable, and that can be affected by altering design deci-\nsions. In general, TPMs can be generic (attributes that are \nmeaningful to each PBS element, like mass or reliability) \nor unique (attributes that are meaningful only to specific \nPBS elements). The relationship of MOEs, MOPs, and \nTPMs can be found in Figure 6.7-4. The systems engi-\nneer needs to decide which generic and unique TPMs \nare worth tracking at each level of the PBS. (See box for \nexamples of TPMs.) At lower levels of the PBS, TPMs \n\nMOE #1 MOE #2\n\nMOEs Derived from stakeholder expectation statements;\n deemed critical to mission or operational success of \n the system\n\nMOPs Broad physical and performance parameters; means of\n ensuring meeting the associated MOEs\n\nTPMs Critical mission success or performance attributes;\n measurable; progress pro?le established, controlled,\n and monitored\n\nMOP #1 MOP #2 MOP #3 MOP#n\n\nTPM #1 TPM #2 TPM #3 TPM #4 TPM #5 TPM #k\n\nFigure 6.7?4 Relationships of MOEs, MOPs, \nand TPMs\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 193\n\nworth tracking can be identified through the functional \nand performance requirements levied on each individual \nsystem, subsystem, etc. \n\nAs TPMs are intended to provide an early warning of \nthe adequacy of a design in satisfying selected critical \ntechnical parameter requirements, the systems engineer \nshould select TPMs that fall within well-defined (quan-\n\ntitative) limits for reasons of system effectiveness or mis-\nsion feasibility. Usually these limits represent either a \nfirm upper or lower bound constraint. A typical example \nof such a TPM for a spacecraft is its injected mass, which \nmust not exceed the capability of the selected launch \nvehicle. Tracking injected mass as a high-level TPM is \nmeant to ensure that this does not happen. A high-level \nTPM like injected mass must often be \u201cbudgeted\u201d and \nallocated to multiple system elements. Tracking and re-\nporting should be required at these lower levels to gain \nvisibility into the sources of any variances. \n\nIn summary, for a TPM to be a valuable status and as-\nsessment tool, certain criteria must be met:\n\nBe a significant descriptor of the system (e.g., weight,  ?\nrange, capacity, response time, safety parameter) that \nwill be monitored at key events (e.g., reviews, audits, \nplanned tests). \nCan be measured (either by test, inspection, demon- ?\nstration, or analysis). \nIs such that reasonable projected progress profiles can  ?\nbe established (e.g., from historical data or based on \ntest planning).\n\nTPM Assessment and Reporting Methods\nStatus reporting and assessment of the system\u2019s TPMs \ncomplement cost and schedule control. There are a \nnumber of assessment and reporting methods that have \nbeen used on NASA projects, including the planned pro-\nfile method and the margin management method. \n\nA detailed example of the planned profile method \nfor the Chandra Project weight TPM is illustrated in \nFigure 6.7-5. This figure depicts the subsystem contri-\nbutions, various constraints, project limits, and manage-\nment reserves from project SRR to launch.\n\nA detailed example of the margin management method \nfor the Sojourner mass TPM is illustrated in Figure 6.7-6 \nThis figure depicts the margin requirements (horizontal \nstraight lines) and actual mass margins from project SRR \nto launch. \n\nRelationship of TPM Assessment Program to the \nSEMP\n\nThe SEMP is the usual document for describing the proj-\nect\u2019s TPM assessment program. This description should \ninclude a master list of those TPMs to be tracked, and the \nmeasurement and assessment methods to be employed. \nIf analytical methods and models are used to measure \n\nExamples of Technical Performance \nMeasures \n\nTPMs from MOEs\n\nMission performance (e.g., total science data vol- ?\nume returned)\n\nSafety (e.g., probability of loss of crew, probability  ?\nof loss of mission)\n\nAchieved availability (e.g., (system uptime)/(system  ?\nuptime + system downtime))\n\nTPMs from MOPs\n\nThrust versus predicted/specified ?\n\nI ?\nsp\n\n versus predicted/specified\n\nEnd of Mission (EOM) dry mass ?\n\nInjected mass (includes EOM dry mass, baseline  ?\nmission plus reserve propellant, other consum-\nables and upper stage adaptor mass)\n\nPropellant margins at EOM ?\n\nOther consumables margins at EOM ?\n\nElectrical power margins over mission life  ?\n\nControl system stability margins ?\n\nEMI/EMC susceptibility margins  ?\n\nOnboard data processing memory demand ?\n\nOnboard data processing throughput time ?\n\nOnboard data bus capacity ?\n\nTotal pointing error ?\n\nTotal vehicle mass at launch ?\n\nPayload mass (at nominal altitude or orbit) ?\n\nReliability  ?\n\nMean time before refurbishment required ?\n\nTotal crew maintenance time required ?\n\nSystem turnaround time ?\n\nFault detection capability ?\n\nPercentage of system designed for on-orbit crew  ?\naccess\n\n\n\n194 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\ncertain high-level TPMs, then these need to be identi-\nfied. The reporting frequency and timing of assessments \nshould be specified as well. In determining these, the sys-\ntems engineer must balance the project\u2019s needs for accu-\nrate, timely, and effective TPM tracking against the cost \nof the TPM tracking program. \n\nThe TPM assessment program plan, which may be a part \nof the SEMP or a stand-alone document for large pro-\ngrams/projects, should specify each TPM\u2019s allocation, \ntime-phased planned profile or margin requirement, \nand alert zones, as appropriate to the selected assessment \nmethod.\n\nA formal TPM assessment program should be fully \nplanned and baselined with the SEMP. Tracking TPMs \nshould begin as soon as practical in Phase B. Data to \nsupport the full set of selected TPMs may, however, \nnot be available until later in the project life cycle. As \nthe project life cycle proceeds through Phases C and D, \nthe measurement of TPMs should become increasingly \nmore accurate with the availability of more actual data \nabout the system. \n\nFor the WBS model in the system structure, typically the \nfollowing activities are performed:\n\nAnalyze stakeholder expectation statements to estab- ?\nlish a set of MOEs by which overall system or product \neffectiveness will be judged and customer satisfaction \nwill be determined. \n\nFigure 6.7?5 Use of the planned profile method for the weight TPM with rebaseline in Chandra Project\n\nChandra Project: Weight Changes\n\nNASA Reserve\n(Includes FPSI\nReserves)\n\nPropellant \n(Less MUPS)/\nPressurant/\nIUS Adapter/\nSystem Reserve\n\nScience\nInstruments\n\nContractor Margin\n\nObservatory\nContingency +\nS/C, T/S, and SIM\nReserves\n\nContractor Reserve\n\nWeight (lb)\n\nProjected Basic Weight\n\nPDR CDR\n\n13,000\nIICD Weight\n\nControl Weight\n\nObservatory +\nScience Instruments\n\nObservatory Contractor\nSpec Weight\nObservatory\n\nProjected Weight\nObservatory\n\nCurrent Weight +\nSubsystem Reserves\n\nObservatory Basic\nWeight (Planned)\n\nTolerance Band\n\nObservatory Basic\nWeight (Estimated)\n\n8,000\n\n10,000\n\nSRR Launch\n\n9,587\n\n9,907\n\n10,533\n\n10,560\n\n12,930\n\n9,994\n\nFigure 6.7?6 Use of the margin management \nmethod for the mass TPM in Sojourner\n\nNote: Current Margin Description: Microrover System (Rover + \nLander-Mounted Rover Equipment (LMRE)) Allocation = 16.0 kg; \nMicrorover System (Rover + LMRE) Current Best Estimate = \n15.2 kg; Microrover System (Rover + LMRE) Margin = 0.8 kg (5.0%).\n\n1% Margin Level\n\nCDR Value\n\n15% Margin Level\n\nMonth\n\nM\nas\n\ns \nM\n\nar\ngi\n\nn \n(k\n\ng)\n\n3.0\n\n2.5\n\n2.0\n\n1.5\n\n1.0\n\n0.5\n\n0.0\n\n-0.5\n\n-1.0\n\nJu\nly\n\n \u20189\n3\n\nO\nct\n\n. \u20189\n3\n\nJa\nn.\n\n \u20189\n4\n\nA\npr\n\n. \u2019\n94\n\nJu\nly\n\n  \u20189\n4\n\nO\nct\n\n. \u20189\n4\n\nJa\nn.\n\n \u20189\n5\n\nA\npr\n\n. \u2019\n95\n\nJu\nly\n\n  \u20189\n5\n\nO\nct\n\n. \u20189\n5\n\nJa\nn.\n\n \u20189\n6\n\nA\npr\n\n. \u2019\n96\n\nJu\nly\n\n  \u20189\n6\n\nO\nct\n\n. \u20189\n6\n\nJa\nn.\n\n \u20189\n7\n\n5% Margin Level\n\n10% Margin Level\n\n\n\n6.7 Technical Assessment\n\nNASA Systems Engineering Handbook ? 195\n\nDefine MOPs for each identified MOE. ?\nDefine appropriate TPMs and document the TPM as- ?\nsessment program in the SEMP.\n\nSystems Engineering Process Metrics\n\nStatus reporting and assessment of systems engineering \nprocess metrics provide additional visibility into the per-\nformance of the \u201csystem that produces the system.\u201d As \nsuch, these metrics supplement the cost and schedule \ncontrol measures discussed in this subsection.\n\nSystems engineering process metrics try to quantify \nthe effectiveness and productivity of the systems en-\ngineering process and organization. Within a single \nproject, tracking these metrics allows the systems engi-\nneer to better understand the health and progress of that \nproject. Across projects (and over time), the tracking of \nsystems engineering process metrics allows for better \nestimation of the cost and time of performing systems \nengineering functions. It also allows the systems engi-\nneering organization to demonstrate its commitment to \ncontinuous improvement.\n\nSelecting Systems Engineering Process \nMetrics\n\nGenerally, systems engineering process metrics fall into \nthree categories\u2014those that measure the progress of \nthe systems engineering effort, those that measure the \nquality of that process, and those that measure its pro-\nductivity. Different levels of systems engineering man-\nagement are generally interested in different metrics. \nFor example, a project manager or lead systems engi-\nneer may focus on metrics dealing with systems engi-\nneering staffing, project risk management progress, and \nmajor trade study progress. A subsystem systems engi-\nneer may focus on subsystem requirements and interface \ndefinition progress and verification procedures progress. \nIt is useful for each systems engineer to focus on just a \nfew process metrics. Which metrics should be tracked \ndepends on the systems engineer\u2019s role in the total sys-\ntems engineering effort. The systems engineering pro-\ncess metrics worth tracking also change as the project \nmoves through its life cycle.\n\nCollecting and maintaining data on the systems engi-\nneering process is not without cost. Status reporting and \nassessment of systems engineering process metrics divert \ntime and effort from the activity itself. The systems engi-\nneer must balance the value of each systems engineering \n\nprocess metric against its collection cost. The value \nof these metrics arises from the insights they provide \ninto the activities that cannot be obtained from cost and \nschedule control measures alone. Over time, these metrics \ncan also be a source of hard productivity data, which are \ninvaluable in demonstrating the potential returns from in-\nvestment in systems engineering tools and training. \n\nExamples and Assessment Methods\nTable 6.7-21 lists some systems engineering process met-\nrics to be considered. This list is not intended to be ex-\nhaustive. Because some of these metrics allow for dif-\nferent interpretations, each NASA Center needs to define \nthem in a common-sense way that fits its own processes. \nFor example, each field Center needs to determine what \nis meant by a \u201ccompleted\u201d versus an \u201capproved\u201d require-\nment, or whether these terms are even relevant. As part \nof this definition, it is important to recognize that not all \nrequirements, for example, need be lumped together. It \nmay be more useful to track the same metric separately \nfor each of several different types of requirements.\n\nQuality-related metrics should serve to indicate when \na part of the systems engineering process is overloaded \nand/or breaking down. These metrics can be defined and \ntracked in several different ways. For example, require-\nments volatility can be quantified as the number of newly \nidentified requirements, or as the number of changes to \nalready approved requirements. As another example, \nEngineering Change Request (ECR) processing could be \ntracked by comparing cumulative ECRs opened versus \ncumulative ECRs closed, or by plotting the age profile \nof open ECRs, or by examining the number of ECRs \nopened last month versus the total number open. The \nsystems engineer should apply his or her own judgment \nin picking the status reporting and assessment method.\n\nProductivity-related metrics provide an indication of \nsystems engineering output per unit of input. Although \nmore sophisticated measures of input exist, the most \ncommon is the number of systems engineering hours ded-\nicated to a particular function or activity. Because not all \nsystems engineering hours cost the same, an appropriate \nweighing scheme should be developed to ensure compa-\nrability of hours across systems engineering personnel.\n\nSchedule-related metrics can be depicted in a table or \ngraph of planned quantities versus actuals, for example, \ncomparing planned number of verification closure no-\ntices against actual. This metric should not be confused \n\n\n\n196 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nTable 6.7?21 Systems Engineering Process Metrics\n\nFunction Metric Category\n\nRequirements \ndevelopment and \nmanagement\n\nRequirements identified versus completed versus approved S\n\nRequirements volatility Q\n\nTrade studies planned versus completed S\n\nRequirements approved per systems engineering hour P\n\nTracking of TBAs, TBDs, and TBRs (to be announced, determined, or resolved) resolved \nversus remaining \n\nS\n\nDesign and devel-\nopment\n\nSpecifications planned versus completed S\n\nProcessing of engineering change proposals (ECPs)/engineering change requests (ECRs) Q\n\nEngineering drawings planned versus released S\n\nVerification and \nvalidation\n\nVerification and validation plans identified versus approved S\n\nVerification and validation procedures planned versus completed S\n\nFunctional requirements approved versus verified S\n\nVerification and validation plans approved per systems engineering hour P\n\nProcessing of problem/failure reports Q\n\nReviews Processing of RIDs Q\n\nProcessing of action items Q\n\nS = progress or schedule related; Q = quality related; P = productivity related\n\nwith EVM described in this subsection. EVM is focused \non integrated cost and schedule at the desired level, \nwhereas this metric focuses on an individual process or \nproduct within a subsystem, system, or project itself.\n\nThe combination of quality, productivity, and schedule \nmetrics can provide trends that are generally more im-\n\nportant than isolated snapshots. The most useful kind of \nassessment method allows comparisons of the trend on \na current project with that for a successfully completed \nproject of the same type. The latter provides a bench-\nmark against which the systems engineer can judge his \nor her own efforts.\n\n\n\nNASA Systems Engineering Handbook ? 197\n\nThe purpose of this section is to provide a description \nof the Decision Analysis Process, including alternative \ntools and methodologies. Decision analysis offers indi-\nviduals and organizations a methodology for making \ndecisions; it also offers techniques for modeling deci-\nsion problems mathematically and finding optimal de-\ncisions numerically. Decision models have the capacity \nfor accepting and quantifying human subjective inputs: \njudgments of experts and preferences of decisionmakers. \nImplementation of models can take the form of simple \npaper-and-pencil procedures or sophisticated computer \nprograms known as decision aids or decision systems. \nThe methodology is broad and must always be adapted \nto the issue under consideration. The problem is struc-\ntured by identifying alternatives, one of which must \nbe decided upon; possible events, one of which occurs \nthereafter; and outcomes, each of which results from a \ncombination of decision and event. Decisions are made \nthroughout a program/project life cycle and often are \nmade through a hierarchy of panels, boards, and teams \nwith increasing complementary authority, wherein each \nprogressively more detailed decision is affected by the \nassumptions made at the lower level. Not all decisions \nneed a formal process, but it is important to establish a \nprocess for those decisions that do require a formal pro-\ncess. Important decisions as well as supporting informa-\ntion (e.g., assumptions made), tools, and models must \nbe completely documented so that new information can \nbe incorporated and assessed and past decisions can be \nresearched in context. The Decision Analysis Process \naccommodates this iterative environment and occurs \nthroughout the project life cycle. \n\nAn important aspect of the Decision Analysis Process is \nto consider and understand at what time it is appropriate \nor required for a decision to be made or not made. When \nconsidering a decision, it is important to ask questions \nsuch as: Why is a decision required at this time? For how \nlong can a decision be delayed? What is the impact of \ndelaying a decision? Is all of the necessary information \navailable to make a decision? Are there other key drivers \nor dependent factors and criteria that must be in place \nbefore a decision can be made?\n\nThe outputs from this process support the decisionmak-\ner\u2019s difficult task of deciding among competing alterna-\ntives without complete knowledge; therefore, it is critical \n\nto understand and document the assumptions and lim-\nitation of any tool or methodology and integrate them \nwith other factors when deciding among viable options.\n\nEarly in the project life cycle, high-level decisions are \nmade regarding which technology could be used, such \nas solid or liquid rockets for propulsion. Operational \nscenarios, probabilities, and consequences are deter-\nmined and the design decision made without specifying \nthe component-level detail of each design alternative. \nOnce high-level design decisions are made, nested sys-\ntems engineering processes occur at progressively more \ndetailed design levels flowed down through the entire \nsystem. Each progressively more detailed decision is af-\nfected by the assumptions made at the previous levels. \nFor example, the solid rocket design is constrained by \nthe operational assumptions made during the decision \nprocess that selected that design. This is an iterative pro-\ncess among elements of the system. Also early in the life \ncycle, the technical team should determine the types of \ndata and information products required to support the \nDecision Analysis Process during the later stages of the \nproject. The technical team should then design, develop, \nor acquire the models, simulations, and other tools that \nwill supply the required information to decisionmakers. \nIn this section, application of different levels and kinds \nof analysis are discussed at different stages of the project \nlife cycle.\n\n6.8.1 Process Description\nThe Decision Analysis Process is used to help evaluate \ntechnical issues, alternatives, and their uncertainties to \nsupport decisionmaking. A typical process flow diagram \nis provided in Figure 6.8-1, including inputs, activities, \nand outputs. \n\nTypical processes that use decision analysis are:\nDetermining how to allocate limited resources (e.g.,  ?\nbudget, mass, power) among competing subsystem \ninterests to favor the overall outcome of the project; \nSelect and test evaluation methods and tools against  ?\nsample data;\nConfiguration management processes for major  ?\nchange requests or problem reports;\nDesign processes for making major design decisions  ?\nand selecting design approaches;\n\n6.8 Decision Analysis\n\n\n\n198 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nNote: Studies often deal in new territory, so it is impor-\ntant to test whether there are sufficient data, needed \nquality, resonance with decision authority, etc., be-\nfore diving in, especially for large or very complex de-\ncision trade spaces.\n\nKey decision point reviews or technical review deci- ?\nsions (e.g., PDR, CDR) as defined in NPR 7120.5 and \nNPR 7123.1;\nGo or No-Go decisions (e.g., FRR): ?\n\nGo\u2014authorization to proceed or ?\nNo-Go\u2014repeat some specific aspects of develop- ?\nment or conduct further research.\n\nProject management of major issues, schedule delays,  ?\nor budget increases;\nProcurement of major items; ?\nTechnology decisions; ?\nRisk management of major risks (e.g., red or yellow); ?\nSMA decisions; and ?\nMiscellaneous decisions (e.g., whether to intervene in  ?\nthe project to address an emergent performance issue).\n\nDecision analysis can also be used in emergency situa-\ntions. Under such conditions, process steps, procedures, \n\nand meetings may be combined, and the decision anal-\nysis documentation may be completed at the end of the \nprocess (i.e., after the decision is made). However, a deci-\nsion matrix should be completed and used during the de-\ncision. Decision analysis documentation must be archived \nas soon as possible following the emergency situation.\n\n6.8.1.1 Inputs\nFormal decision analysis has the potential to consume \nsignificant resources and time. Typically, its application \nto a specific decision is warranted only when some of the \nfollowing conditions are met:\n\nFrom all technical\nprocesses \n\nWork Products From\nDecision Analysis\n\nTo Technical Data\nManagement Process \n\nDecision Need,\nAlternatives, Issues, or \n\nProblems and\nSupporting Data To Technical\n\nAssessment Process\n\nDecision Support \nRecommendations and\n\nImpacts\n\nTo all technical processes\n\nAlternative Selection\nRecommendations and\n\nImpacts\n\nFrom Technical\nAssessment Process \n\nAnalysis Support\nRequests\n\nEstablish guidelines to determine\nwhich technical issues are subject to a\n\nformal analysis/evaluation process\n\nIdentify alternative solutions to \naddress decision issues\n\nSelect evaluation methods and tools\n\nDe?ne the criteria for evaluating\nalternative solutions\n\nEvaluate alternative solutions with \nthe established criteria and\n\nselected methods\n\nSelect recommended solutions from \nthe alternatives based on the\n\nevaluation criteria\n\nReport analysis results with \nrecommendations, impacts, and\n\ncorrective actions  \n\nCapture work products from \ndecision analysis activities\n\nFigure 6.8?1 Decision Analysis Process\n\n\n\n6.8 Decision Analysis\n\nNASA Systems Engineering Handbook ? 199\n\nHigh Stakes: ?  High stakes are involved in the decision, \nsuch as significant cost, safety, or mission success cri-\nteria. \nComplexity: ?  The actual ramifications of alternatives \nare difficult to understand without detailed analysis.\nUncertainty: ?  Uncertainty in key inputs creates sub-\nstantial uncertainty in the ranking of alternatives and \npoints to risks that may need to be managed. \nMultiple Attributes: ?  Greater numbers of attributes \ncause a greater need for formal analysis. \nDiversity of Stakeholders: ?  Extra attention is war-\nranted to clarify objectives and formulate TPMs when \nthe set of stakeholders reflects a diversity of values, \npreferences, and perspectives.\n\nSatisfaction of all of these conditions is not a requirement \nfor initiating decision analysis. The point is, rather, that \nthe need for decision analysis increases as a function of \nthe above conditions. When the Decision Analysis Pro-\ncess is triggered, the following are inputs:\n\nDecision need, identified alternatives, issues, or prob- ?\nlems and supporting data (from all technical manage-\nment processes).\nAnalysis support requests (from Technical Assess- ?\nment Process).\nHigh-level objectives and constraints (from the pro- ?\ngram/project).\n\n6.8.1.2 Process Activities\nFor the Decision Analysis Process, the following activi-\nties typically are performed.\n\nEstablish Guidelines to Determine Which \nTechnical Issues Are Subject to a Formal Analysis/\nEvaluation Process\n\nThis step includes determining: \nWhen to use a formal decisionmaking procedure,  ?\nWhat needs to be documented, ?\nWho will be the decisionmakers and their responsi- ?\nbilities and decision authorities, and \nHow decisions will be handled that do not require a  ?\nformal evaluation procedure. \n\nDecisions are based on facts, qualitative and quantitative \ndata, engineering judgment, and open communications \nto facilitate the flow of information throughout the hi-\nerarchy of forums where technical analyses and evalua-\n\ntions are presented and assessed and where decisions are \nmade. The extent of technical analysis and evaluation re-\nquired should be commensurate with the consequences \nof the issue requiring a decision. The work required to \nconduct a formal evaluation is not insignificant and ap-\nplicability must be based on the nature of the problem to \nbe resolved. Guidelines for use can be determined by the \nmagnitude of the possible consequences of the decision \nto be made.\n\nFor example, the consequence table from a risk score-\ncard can be used to assign numerical values for appli-\ncability according to impacts to mission success, flight \nsafety, cost, and schedule. Actual numerical thresholds \nfor use would then be set by a decision authority. Sample \nvalues could be as shown in Table 6.8-1. \n\nTable 6.8?1 Consequence Table\n\nNumerical Value Consequence Applicability\n\nConsequence = 5, 4 High Mandatory\n\nConsequence = 3 Moderate Optional\n\nConsequence = 1, 2 Low Not required\n\nDefine the Criteria for Evaluating Alternative \nSolutions\nThis step includes identifying:\n\nThe types of criteria to consider, such as customer ex- ?\npectations and requirements, technology limitations, \nenvironmental impact, safety, risks, total ownership \nand life-cycle costs, and schedule impact; \nThe acceptable range and scale of the criteria; and  ?\nThe rank of each criterion by its importance.  ?\n\nDecision criteria are requirements for individually as-\nsessing options and alternatives being considered. Typ-\nical decision criteria include cost, schedule, risk, safety, \nmission success, and supportability. However, consid-\nerations should include technical criteria specific to the \ndecision being made. Criteria should be objective and \nmeasurable. Criteria should also permit distinguishing \namong options or alternatives. Some criteria may not be \nmeaningful to a decision; however, they should be docu-\nmented as having been considered. Identify criteria that \nare mandatory (i.e., \u201cmust have\u201d) versus the other cri-\nteria (i.e., \u201cnice to have\u201d). If mandatory criteria are not \nmet, that option should be disregarded. For complex \ndecisions, criteria can be grouped into categories or ob-\n\n\n\n200 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\njectives. (See the analytical hierarchy process in Subsec-\ntion 6.8.2.6.)\n\nRanking or prioritizing the criteria is probably the \nhardest part of completing a decision matrix. Not all cri-\nteria have the same importance, and ranking is typically \naccomplished by assigning weights to each. To avoid \n\u201cgaming\u201d the decision matrix (i.e., changing decision \noutcomes by playing with criteria weights), it is best to \nagree upon weights before the decision matrix is com-\npleted. Weights should only be changed with consensus \nfrom all decision stakeholders.\n\nFor example, ranking can be done using a simple ap-\nproach like percentages. Have all the weights for each \ncriterion add up to 100. Assign percents based on how \nimportant the criterion is. (The higher the percentage, \nthe more important, such as a single criterion worth 50 \npercent.) The weights need to be divided by 100 to cal-\nculate percents. Using this approach, the option with the \nhighest percentage is typically the recommended option. \nRanking can also be done using sophisticated decision \ntools. For example, pair-wise comparison is a decision \ntechnique that calculates the weights using paired com-\nparisons among criteria and options. Other methods in-\nclude:\n\nFormulation of objectives hierarchy and TPMs; ?\nAnalytical hierarchy process, which addresses criteria  ?\nand paired comparisons; and\nRisk-informed decision analysis process with  ?\nweighting of TPMs.\n\nIdentify Alternative Solutions to Address \nDecision Issues\nThis step includes considering alternatives in addition to \nthose that may be provided with the issue.\n\nAlmost every decision will have options to choose from. \nBrainstorm decision options, and document option \nsummary names for the available options. For complex \ndecisions, it is also a best practice to perform a litera-\nture search to identify options. Reduce the decision op-\ntions to a reasonable set (e.g., seven plus or minus two). \nSome options will obviously be bad options. Document \nthe fact that these options were considered. The use of \nmandatory criteria also can help reduce the number of \noptions. A few decisions might only have one option. It \nis a best practice to document a decision matrix even for \none option if it is a major decision. (Sometimes doing \nnothing or not making a decision is an option.) \n\nSelect Evaluation Methods and Tools\n\nSelect evaluation methods and tools/techniques based \non the purpose for analyzing a decision and on the avail-\nability of the information used to support the method \nand/or tool. \n\nTypical evaluation methods include: simulations; weighted \ntradeoff matrices; engineering, manufacturing, cost, and \ntechnical opportunity of trade studies; surveys; extrapo-\nlations based on field experience and prototypes; user re-\nview and comment; and testing.\n\nTools and techniques to be used should be selected based \non the purpose for analyzing a decision and on the avail-\nability of the information used to support the method \nand/or tool. \n\nAdditional evaluation methods include:\nDecision matrix (see Figure 6.8-2); ?\nDecision analysis process support, evaluation methods,  ?\nand tools; \nRisk-informed decision analysis process; and  ?\nTrade studies and decision alternatives. ?\n\nEvaluate Alternative Solutions with the \nEstablished Criteria and Selected Methods\n\nRegardless of the methods or tools used, results must in-\nclude:\n\nEvaluation of assumptions related to evaluation cri- ?\nteria and of the evidence that supports the assump-\ntions, and \nEvaluation of whether uncertainty in the values for al- ?\nternative solutions affects the evaluation. \n\nAlternatives can be compared to evaluation criteria via \nthe use of a decision matrix as shown in Figure 6.8-2. \nEvaluation criteria typically are in the rows on the left \nside of the matrix. Alternatives are typically the column \nheadings on the top of the matrix (and to the right top). \nCriteria weights are typically assigned to each criterion. \nIn the example shown, there are also mandatory criteria. \nIf mandatory criteria are not met, the option is scored at \n0 percent. \n\nWhen decision criteria have different measurement \nbases (e.g., numbers, money, weight, dates), normaliza-\ntion can be used to establish a common base for math-\nematical operations. The process of \u201cnormalization\u201d is \nmaking a scale so that all different kinds of criteria can \n\n\n\n6.8 Decision Analysis\n\nNASA Systems Engineering Handbook ? 201\n\nbe compared or added together. This can be done infor-\nmally (e.g., low, medium, high), on a scale (e.g., 1-3-9), \nor more formally with a tool. No matter how normaliza-\ntion is done, the most important thing to remember is \nto have operational definitions of the scale. An opera-\ntional definition is a repeatable, measurable number. For \nexample, \u201chigh\u201d could mean \u201ca probability of 67 percent \nand above.\u201d \u201cLow\u201d could mean \u201ca probability of 33 per-\ncent and below.\u201d For complex decisions, decision tools \nusually provide an automated way for normalization. Be \nsure to question and understand the operational defini-\ntions for the weights and scales of the tool.\n\nSelect Recommended Solutions from the \nAlternatives Based on the Evaluation Criteria\nThis step includes documenting the information, including \nassumptions and limitations of the evaluation methods \n\nused, that justifies the recommendations made and gives \nthe impacts of taking the recommended course of action.\n\nThe highest score (e.g., percentage, total score) is typi-\ncally the option that is recommended to management. \nIf a different option is recommended, an explanation \nmust be provided as to why the lower score is preferred. \nUsually, if a lower score is recommended, the \u201crisks\u201d \nor \u201cdisadvantages\u201d were too great for the highest score. \nSometimes the benefits and advantages of a lower or \nclose score outweigh the highest score. Ideally, all risks/\nbenefits and advantages/disadvantages would show up \nin the decision matrix as criteria, but this is not always \npossible. Sometimes if there is a lower score being rec-\nommended, the weighting or scores given may not be \naccurate.\n\nReport the Analysis and Evaluation Results and \nFindings with Recommendations, Impacts, and \nCorrective Actions\n\nTypically a technical team of subject matter experts \nmakes a recommendation to a NASA decisionmaker \n(e.g., a NASA board, forum, or panel). It is highly recom-\nmended that the team produce a white paper to docu-\nment all major recommendations to serve as a backup to \n\nFigure 6.8?2 Example of a decision matrix \n\nDecision Matrix \n\nExample for Battery ENTER SCORES\n\nEx\nte\n\nnd\n O\n\nld\n B\n\nat\nte\n\nry\n \n\nLi\nfe\n\nBu\ny \n\nN\new\n\n B\nat\n\nte\nrie\n\ns\n\nCo\nlle\n\nct\n E\n\nxp\ner\n\nie\nnt\n\n \nD\n\nat\na \n\nW\nith\n\n \nA\n\nlte\nrn\n\nat\niv\n\ne \nEx\n\npe\nrim\n\nen\nt\n\nCa\nnc\n\nel\nle\n\nd \nEx\n\npe\nrim\n\nen\nt\n\nCRITERIA Mandatory (Y=1/N=0)? Weight SCALE\nMission Success (Get\nExperiment Data) \n\n1 30 3 = Most Supportive1 = Least Supportive 2 3 3 0\n\nCost per Option 0 10 3 = Least Expensive1 = Most Expensive 1 2 3 1\n\nRisk (Overall Option Risk) 0 15 3 = Least Risk1 = Most Risk 2 1 2 3\n\nSchedule 0 10 3 = Shortest Schedule1 = Longest Schedule 3 2 1 3\n\nSafety 1 15 3 = Most Safe1 = Least Safe 2 1 2 3\n\nUninterrupted Data Collection 0 20 3 = Most Supportive1 = Least Supportive 3 1 2 1\n\nWEIGHTED TOTALS in % 100% 3 73% 60% 77% 0%\n\nSCALE 1-3\n\nNote: Completing the decision matrix can be thought \nof as a default evaluation method. Completing the \ndecision matrix is iterative. Each cell for each criterion \nand each option needs to be completed by the team. \nUse evaluation methods as needed to complete the \nentire decision matrix.\n\n\n\n202 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nany presentation materials used. A presentation can also \nbe used, but a paper in conjunction with a decision ma-\ntrix is preferred (especially for complex decisions). Deci-\nsions are typically captured in meeting minutes, but can \nbe captured in the white paper.\n\nCapture Work Products from Decision Analysis \nActivities\nThis step includes capturing: \n\nDecision analysis guidelines generated and strategy  ?\nand procedures used; \nAnalysis/evaluation approach, criteria, and methods  ?\nand tools used; \nAnalysis/evaluation results, assumptions made in ar- ?\nriving at recommendations, uncertainties, and sensi-\n\ntivities of the recommended actions or corrective ac-\ntions; and \nLessons learned and recommendations for improving  ?\nfuture decision analyses.\n\nTypical information captured in a decision report is \nshown in Table 6.8-2.\n\n6.8.1.3 Outputs\nDecision analysis continues throughout the life cycle. \nThe products from decision analysis include:\n\nAlternative selection recommendations and impacts  ?\n(to all technical management processes);\nDecision support recommendations and impacts (to  ?\nTechnical Assessment Process);\n\nTable 6.8?2 Typical Information to Capture in a Decision Report \n\n# Section Section Description\n\n1 Executive Summary Provide a short half-page executive summary of the report:\n\nRecommendation (short summary\u20141 sentence) ?\n\nProblem/issue requiring a decision (short summary\u20141 sentence) ?\n\n2 Problem/Issue \nDescription\n\nDescribe the problem/issue that requires a decision. Provide background, history, the \ndecisionmaker(s) (e.g., board, panel, forum, council), and decision recommendation team, etc.\n\n3 Decision Matrix  \nSetup Rationale\n\nProvide the rationale for setting up the decision matrix:\n\nCriteria selected ?\n\nOptions selected ?\n\nWeights selected ?\n\nEvaluation methods selected ?\n\nProvide a copy of the setup decision matrix.\n\n4 Decision Matrix \nScoring Rationale\n\nProvide the rationale for the scoring of the decision matrix. Provide the results of populating the \nscores of the matrix using the evaluation methods selected.\n\n5 Final Decision \nMatrix\n\nCut and paste the final spreadsheet into the document. Also include any important snapshots of \nthe decision matrix.\n\n6 Risk/Benefits For the final options being considered, document the risks and benefits of each option.\n\n7 Recommendation \nand/or Final  \nDecision\n\nDescribe the recommendation that is being made to the decisionmaker(s) and the rationale for \nwhy the option was selected. Can also document the final decision in this section.\n\n8 Dissent If applicable, document any dissent with the recommendation. Document how dissent was \naddressed (e.g., decision matrix, risk, etc.).\n\n9 References Provide any references.\n\nA Appendices Provide the results of the literature search, including lessons learned, previous related decisions, \nand previous related dissent. Also document any detailed data analysis and risk analysis used for \nthe decision. Can also document any decision metrics.\n\n\n\n6.8 Decision Analysis\n\nNASA Systems Engineering Handbook ? 203\n\nWork products of decision analysis activities (to Tech- ?\nnical Data Management Process); and\nTechnical risk status measurements (to Technical Risk  ?\nManagement Process).\nTPMs, Performance Indexes (PIs) for alternatives, the  ?\nprogram- or project-specific objectives hierarchy, and \nthe decisionmakers\u2019 preferences (to all technical man-\nagement processes).\n\n6.8.2 Decision Analysis Guidance\nThe purpose of this subsection is to provide guidance, \nmethods, and tools to support the Decision Analysis \nProcess at NASA. \n\n6.8.2.1 Systems Analysis, Simulation, and \nPerformance\n\nSystems analysis can be better understood in the context \nof the system\u2019s overall life cycle. Systems analysis within \nthe context of the life cycle is responsive to the needs of \nthe stakeholder at every phase of the life cycle, from pre-\nPhase A through Phase B to realizing the final product \nand beyond (See Figure 6.8-3.) \n\nSystems analysis of a product must support the transfor-\nmation from a need into a realized, definitive product; \nbe able to support compatibility with all physical and \nfunctional requirements; and support the operational \n\nFigure 6.8?3 Systems analysis across the life cycle\n\nConcept\nCollaboration,\n\nAssessment, and\nFeedback to Design \n\nVirtual Prototyping Detailed, Focused Development\n\nRequirements Understanding/Filtering Architectures Selection/Analysis Execution\n\n60\u201390% of life-cycle\ncost locked in (but not\nnecessarily known)Needs Identi?cation\n\nRequirements Analysis\n System Feasibility Analysis\n Cost/Bene?t Analysis\n Operational Requirements Analysis\n Establish MOEs, TPMs\n Functional Systems Analysis and Allocation\n Analysis, Synthesis, and Evaluation\nFeasible Technology Applications Evaluation\nTechnical Approach Selection \nFunctional De?nition of System\nSystem Planning\n\nAlternative Architectures\nTop-Level Architecture\n Functional Analysis\n Requirements Allocation\n Trade Studies and Decision Analysis\n Input Data Required\nDe?nitions\n\nFunctional Baseline\n Selection Evaluation Techniques\n Select Model(s)\n Identi?cation of Design-Dependent Parameters\n  Reevaluation of Established MOEs/TPMs\n Acquire Input Data\n Evaluate Each Candidate\n Tradeo? and Decision Analysis\n Sensitivity Analysis\n Design Re?nement\n Recommendations\n  Con?dence Levels\n  Tradeo?s\n  Breakeven Points\n  Sensitivities\u2014Risks and Uncertainties\n  MOEs Associated With Decision Alternatives\nDesign to Baseline or Allocated Baseline\n Re?nement of Synthesis/Evaluation for System\n  and Subsystem Levels\n Evaluation in Terms of Requirements\n Continuation of Iterations\n Sensitivity and Contingency Analysis\nUpdate Analysis and Models With New Data\n\nBuild-to Product Baseline\n Evaluation and Selection of Di?erent Technologies\n Evaluation and Selection of Di?erent Materials\n Alternative System Packaging and Wiring\n Alternative Diagnostics\n Alternative Instrumentation Concepts\n Evaluation and Selection of COTS Items\n Alternative Maintenance and Support Concepts\n Evaluation of Autonomous vs. Human Facilitation\n Acquisition Plans\n Contracting\n Program Implementation\n Major Suppliers and Supplier Activities\n Subsystem Component Design\n Trade Studies and Evaluation of Decision Alternatives\n Development of Engineering and Prototype Models\n Veri?cation of Manufacturing and Production Processes\n Developmental Test and Evaluation\n Supplier Activities\n  Alternative Manufacture/Producibility Concepts\n  Alternative Logistics Support Concepts\n  Alternative System/Material Disposal Concepts\nUpdate Analysis and Models With New Data\n\nAs-Built Product Baseline\n Production and/or Construction of System Components\n Supplier Production Activities\n Acceptance Testing\n System Distribution and Operation\n Developmental/Operational Test and Evaluation\n Interim Contractor Support\nUpdate Analysis With New Data \n\nAs-Deployed Baseline\n System Operation in User Environment\n Sustaining Maintenance & Logistic Support\n Operational Testing\n System Modi?cations for Improvement\n Contractor Support\n System Assessment/Field Data Collection \n  and Analysis\nUpdate Analysis and Models \n\nPhase E:\nOperations and\n\nSustainment \n\nPhase D:\nSystem Assembly,\n\nIntegration & Test, Launch\n\nPre-Phase A:\nConcept Studies\n\nPhase B:\nPreliminary Design and\nTechnology Completion\n\nPhase A:\nConcept & Technology\n\nDevelopment \n\nPhase C:\nFinal Design and\n\nFabrication\n\nProcess/Product\nAssessment, and\n\nRe?nement\nOperations\n\n\n\n204 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nscenarios in terms of reliability, maintainability, sup-\nportability, serviceability, and disposability, while main-\ntaining performance and affordability. \n\nSystems analysis support is provided from cradle to \ngrave of the system. This covers the product design, ver-\nification, manufacturing, operations and support, and \ndisposal. Viewed in this manner, life-cycle engineering \nis the basis for concurrent engineering. \n\nSystems analysis should support concurrent engineering. \nAppropriate systems analysis can be conducted early in \nthe life cycle to support planning and development. The \nintent here is to support seamless systems analysis opti-\nmally planned across the entire life cycle. For example, \nsystems engineering early in the life cycle can support \noptimal performance of the deployment, operations, and \ndisposal facets of the system. \n\nHistorically, this has not been the case. Systems analysis \nwould focus only on the life cycle that the project oc-\ncupied at that time. The systems analyses for the later \n\nphases were treated serially, in chronological order. This \nresulted in major design modifications that were very \ncostly in the later life-cycle phases. Resources can be \nused more efficiently if the requirements across the life \ncycle are considered concurrently, providing results for \ndecisionmaking about the system. \n\nFigure 6.8-3 shows a life-cycle chart that indicates how \nthe various general types of systems analyses fit across \nthe phases of the life cycle. The requirements for analysis \nbegin with a broader scope and more types of analysis \nrequired in the early phases of the life cycle and funnel \nor narrow in scope and analysis requirements as deci-\nsions are made and project requirements become clearer \nas the project proceeds through its life cycle. Figure 6.8-4 \npresents a specific spaceport example and shows how \nspecific operational analysis inputs can provide anal-\nysis result outputs pertinent to the operations portion of \nthe life cycle. Note that these simulations are conducted \nacross the life cycle and updated periodically with the \nnew data that is obtained as the project evolves.\n\nFigure 6.8?4 Simulation model analysis techniques\nFrom: Lockheed Martin presentation to KSC, November 2003, Kevin Brughelli, Lockheed Martin Space Systems Company; Debbie Carstens, \nFlorida Institute of Technology; and Tim Barth, KSC.\n\nSEE IMPACTS HERE\n\nProgram Schedules\n\nDate\n\nLanding ops\nHangar ops\nPPF ops\nVIF ops\nPad ops\nFlight ops\n\nPersonnel Utilization\n\nM\nan\n\nho\nur\n\ns\n\nWorked Paid\n\nPreventive Maint.\nUnplanned Work\nPlanned Work\n\nStraight Time\n\nOvertime\n\nFacility Utilization\n\nU\ntil\n\niz\nat\n\nio\nn\n\nHangar VIF PPF Pad\n\nSimulation Model Outputs\n\nMAKE CHANGES HERE\n\nMission Model\n? Annual Launches for Each Vehicle \n\nConfiguration\nProcess Flow\n? Task Duration, Sequence\n? Resource Requirements\nConOps\n? Work Shifts and Priorities\n? Weather, Range, Safety Constraints\n? Extended Maintenance\nProbabilistic Events\n? Weather and Range Events\n? Unplanned Work\n? Equipment Downtime (MTBF \n\nand MTTR)\n? Process Time Variability and \n\nLearning Effects\n? Loss of Vehicle\nResources\n? Launch Vehicle Quantity\n? Facilities and Equipment\n? Personnel Quality and Skills\n\nSimulation Model Inputs\n\nSensitivity Analyses Sensitivity Analyses Customized Analyses\n\nUnlimited Customized\nAnalyses per Specific\nProgram Needs \n\nFlight H/W Reliability\n\nOptimum\n\nTu\nrn\n\nar\nou\n\nnd\n\nPersonnel Quantity\n\nOptimum\n\nLa\nun\n\nch\n R\n\nat\ne\n\nSchedule Dependability\n\nFr\neq\n\nue\nnc\n\ny\n\nLaunch Delay (days)\n0 1 2 3 4 5 6 7\n\nmean = 1.0\n\nPersonnel Utilization\n\nDate\n\nAvailable Pool\n\nU\ntil\n\niz\nat\n\nio\nn\n\nvehicle\n2\n\nvehicle\n1\n\nvehicle\n1\n\nStationized Cost\n\nM\nan\n\nho\nur\n\ns\n\nSys1 Sys2\n\nLanding ops\nHangar ops\nPPF ops\nVIF ops\nPad ops\nFlight ops\n\nTurnaround\n\nFr\neq\n\nue\nnc\n\ny\n\nTurnaround (days)\n16 17 18 19 20 21 22 23\n\nmean = 18.2\n\nLaunch Rate\n\nLa\nun\n\nch\n R\n\nat\ne\n\nSys1 Sys2 Sys3\n\nOptimization\n=n    =y\n\nOptimum\nN\n\non\nre\n\ncu\nrr\n\nin\ng \n\n$\n\nSimulation Model Iterations\n\nMeets all req\u2019mts \n\n\n\n6.8 Decision Analysis\n\nNASA Systems Engineering Handbook ? 205\n\nDuring the early life-cycle phases, inputs should in-\nclude a plan for collecting the quantitative and qualita-\ntive data necessary to manage contracts and improve \nprocesses and products as the project evolves. This plan \nshould indicate the type of data necessary to determine \nthe cause of problems, nonconformances, and anoma-\nlies, and propose corrective action to prevent recurrence. \nThis closed-loop plan involving identification, resolu-\ntion, and recurrence control systems is critical to pro-\nducing actual reliability that approaches predicted reli-\nability. It should indicate the information technology \ninfrastructure and database capabilities to provide data \nsorting, data mining, data analysis, and precursor man-\nagement. Management of problems, nonconformances, \nand anomalies should begin with data collection, should \nbe a major part of technical assessment, and should pro-\nvide critical information for decision analysis.\n\n6.8.2.2 Trade Studies\nThe trade study process is a critical part of systems engi-\nneering. Trade studies help to define the emerging system \nat each level of resolution. One key message of this sub-\n\nsection is that to be effective, trade studies require the \nparticipation of people with many skills and a unity of \neffort to move toward an optimum system design.\n\nFigure 6.8-5 shows the trade study process in simplest \nterms, beginning with the step of defining the system\u2019s \ngoals and objectives, and identifying the constraints it \nmust meet. In the early phases of the project life cycle, \nthe goals, objectives, and constraints are usually stated in \ngeneral operational terms. In later phases of the project \nlife cycle, when the architecture and, perhaps, some as-\npects of the design have already been decided, the goals \nand objectives may be stated as performance require-\nments that a segment or subsystem must meet. \n\nAt each level of system resolution, the systems engineer \nneeds to understand the full implications of the goals, \nobjectives, and constraints to formulate an appropriate \nsystem solution. This step is accomplished by performing \na functional analysis. \u201cFunctional analysis\u201d is the process \nof identifying, describing, and relating the functions a \nsystem must perform to fulfill its goals and objectives \nand is described in detail in Section 4.4.\n\nFigure 6.8?5 Trade study process\n\nDefine/Identify\nGoals/Objectives\nand Constraints \n\nPerform Functional\nAnalysis \n\nDefine\nSelection\n\nRule\n\nMake a\ntentative\nselection\n(decision)\n\nIs tentative\nselection\n\nacceptable?*\n\nYes\n\nNo\n\nProceed to further\nresolution of\n\nsystem design, or to\nimplementation \n\n *The following questions\nshould be considered:\n\n? Have the goals/objectives and\nconstraints been met?\n\n? Is the tentative selection robust?\n\n? Is more analytical refinement\nneeded to distinguish among\nalternatives?\n\n? Have the subjective aspects of\nthe problem been addressed?\n\nDefine measures and\nmeasurement methods for:\n\n? System effectiveness\n? System performance or\n\ntechnical attributes\n? System cost\n\nCollect data on\neach alternative\n\nto support\nevaluation by\n\nselected\nmeasurement\n\nmethods \n\n? Compute an estimate of system effectiveness,\nperformance or technical attributes, and cost\nfor each alternative\n? Compute or estimate uncertainty ranges\n? Perform sensitivity analyses\n\nAnalytical Portion of Trade Studies\n\nDefine\nPlausible\n\nAlternatives\n\n\n\n206 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nClosely related to defining the goals and objectives and \nperforming a functional analysis is the step of defining \nthe measures and measurement methods for system ef-\nfectiveness (when this is practical), system performance \nor technical attributes, and system cost. (These variables \nare collectively called outcome variables, in keeping with \nthe discussion in Section 2.3. Some systems engineering \nbooks refer to these variables as decision criteria, but this \nterm should not be confused with \u201cselection rule,\u201d de-\nscribed below. Sections 2.5 and 6.1 discuss the concepts of \nsystem cost and effectiveness in greater detail.) Defining \nmeasures and measurement methods begins the analyt-\nical portion of the trade study process, since it suggests the \ninvolvement of those familiar with quantitative methods.\n\nFor each measure, it is important to address how that \nquantitative measure will be computed\u2014that is, which \nmeasurement method is to be used. One reason for \ndoing this is that this step then explicitly identifies those \nvariables that are important in meeting the system\u2019s goals \nand objectives. \n\nEvaluating the likely outcomes of various alternatives \nin terms of system effectiveness, the underlying perfor-\nmance or technical attributes, and cost before actual fab-\nrication and/or programming usually requires the use of \na mathematical model or series of models of the system. \nSo a second reason for specifying the measurement \nmethods is to identify necessary models. \n\nSometimes these models are already available from pre-\nvious projects of a similar nature; other times, they need \nto be developed. In the latter case, defining the measure-\nment methods should trigger the necessary system mod-\neling activities. Since the development of new models \ncan take a considerable amount of time and effort, early \nidentification is needed to ensure they will be ready for \nformal use in trade studies. Defining the selection rule \nis the step of explicitly determining how the outcome \nvariables will be used to make a (tentative) selection \nof the preferred alternative. As an example, a selection \nrule may be to choose the alternative with the highest \nestimated system effectiveness that costs less than x dol-\nlars (with some given probability), meets safety require-\nments, and possibly meets other political or schedule \nconstraints. Defining the selection rule is essentially de-\nciding how the selection is to be made. This step is in-\ndependent from the actual measurement of system ef-\nfectiveness, system performance or technical attributes, \nand system cost.\n\nMany different selection rules are possible. The selection \nrule in a particular trade study may depend on the con-\ntext in which the trade study is being conducted\u2014in par-\nticular, what level of system design resolution is being ad-\ndressed. At each level of the system design, the selection \nrule generally should be chosen only after some guid-\nance from the next higher level. The selection rule for \ntrade studies at lower levels of the system design should \nbe in consonance with the higher level selection rule. \n\nDefining plausible alternatives is the step of creating \nsome alternatives that can potentially achieve the goals \nand objectives of the system. This step depends on under-\nstanding (to an appropriately detailed level) the system\u2019s \nfunctional requirements and operational concept. Run-\nning an alternative through an operational timeline or \nreference mission is a useful way of determining whether \nit can plausibly fulfill these requirements. (Sometimes it \nis necessary to create separate behavioral models to de-\ntermine how the system reacts when a certain stimulus \nor control is applied, or a certain environment is encoun-\ntered. This provides insights into whether it can plausibly \nfulfill time-critical and safety requirements.) Defining \nplausible alternatives also requires an understanding \nof the technologies available, or potentially available, \nat the time the system is needed. Each plausible alter-\nnative should be documented qualitatively in a descrip-\ntion sheet. The format of the description sheet should, \nat a minimum, clarify the allocation of required system \nfunctions to that alternative\u2019s lower level architectural or \ndesign components (e.g., subsystems).\n\nOne way to represent the trade study alternatives under \nconsideration is by a trade tree.\n\nDuring Phase A trade studies, the trade tree should contain \na number of alternative high-level system architectures to \navoid a premature focus on a single one. As the systems \nengineering process proceeds, branches of the trade tree \ncontaining unattractive alternatives will be \u201cpruned,\u201d and \ngreater detail in terms of system design will be added to \nthose branches that merit further attention. The process \nof pruning unattractive early alternatives is sometimes \nknown as doing \u201ckiller trades.\u201d (See trade tree box.)\n\nGiven a set of plausible alternatives, the next step is to \ncollect data on each to support the evaluation of the mea-\nsures by the selected measurement methods. If models \nare to be used to calculate some of these measures, then \nobtaining the model inputs provides some impetus and \n\n\n\n6.8 Decision Analysis\n\nNASA Systems Engineering Handbook ? 207\n\ndirection to the data collection activity. By providing \ndata, engineers in such disciplines as reliability, main-\ntainability, producibility, integrated logistics, software, \ntesting, operations, and costing have an important sup-\nporting role in trade studies. The data collection activity, \nhowever, should be orchestrated by the systems engineer. \nThe results of this step should be a quantitative descrip-\ntion of each alternative to accompany the qualitative.\n\nTest results on each alternative can be especially useful. \nEarly in the systems engineering process, performance \nand technical attributes are generally uncertain and must \nbe estimated. Data from breadboard and brassboard tes-\ntbeds can provide additional confidence that the range of \nvalues used as model inputs is correct. Such confidence \nis also enhanced by drawing on data collected on related, \npreviously developed systems.\n\nThe next step in the trade study process is to quantify the \noutcome variables by computing estimates of system ef-\nfectiveness, its underlying system performance or tech-\nnical attributes, and system cost. If the needed data have \nbeen collected and the measurement methods (for ex-\nample, models) are in place, then this step is, in theory, \nmechanical. In practice, considerable skill is often needed \nto get meaningful results.\n\nIn an ideal world, all input values would be precisely \nknown and models would perfectly predict outcome \nvariables. This not being the case, the systems engineer \nshould supplement point estimates of the outcome vari-\nables for each alternative with computed or estimated \nuncertainty ranges. For each uncertain key input, a \nrange of values should be estimated. Using this range \nof input values, the sensitivity of the outcome variables \n\nAn Example of a Trade Tree for a Mars Rover\n\nThe figure below shows part of a trade tree for a robotic Mars rover system, whose goal is to find a suitable manned land-\ning site. Each layer represents some aspect of the system that needs to be treated in a trade study to determine the best \nalternative. Some alternatives have been eliminated a priori because of technical feasibility, launch vehicle constraints, \netc. The total number of alternatives is given by the number of end points of the tree. Even with just a few layers, the \nnumber of alternatives can increase quickly. (This tree has already been pruned to eliminate low-autonomy, large rov-\ners.) As the systems engineering process proceeds, branches of the tree with unfavorable trade study outcomes are dis-\ncarded. The remaining branches are further developed by identifying more detailed trade studies that need to be made. \nA whole family of (implicit) alternatives can be represented in a trade tree by the continuous variable. In this example, \nrover speed or range might be so represented. By treating a variable this way, mathematical optimization techniques \ncan be applied. Note that a trade tree is, in essence, a decision tree without chance nodes.\n\nMars Rover \n\nSmall\n(\u201310 kg) \n\nLarge\n(\u20131,000 kg)\n\nSize\n\nNumber\n\nMobility\n\nMedium\n(\u2013100 kg)\n\n50 20 10 10 5 2 1\n\nLow High Low High Low High HighSemiLow High Semi\n\nW\nhe\n\nel\ns\n\nLe\ngs\n\nW\nhe\n\nel\ns\n\nW\nhe\n\nel\ns\n\nLe\ngs\n\nW\nhe\n\nel\ns\n\nW\nhe\n\nel\ns\n\nLe\ngs\n\nW\nhe\n\nel\ns\n\nW\nhe\n\nel\ns\n\nLe\ngs\n\nW\nhe\n\nel\ns\n\nW\nhe\n\nel\ns\n\nLe\ngs\n\nW\nhe\n\nel\ns\n\nLe\ngs\n\nW\nhe\n\nel\ns\n\nLe\ngs\n\nSemiLow High\n\nW\nhe\n\nel\ns\n\nLe\ngs\n\nW\nhe\n\nel\ns\n\nW\nhe\n\nel\ns\n\nLe\ngs\n\nSemiLow High\n\nW\nhe\n\nel\ns\n\nLe\ngs\n\nW\nhe\n\nel\ns\n\nW\nhe\n\nel\ns\n\nLe\ngs\n\nAutonomy\n\n\n\n208 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\ncan be gauged and their uncertainty ranges calculated. \nThe systems engineer may be able to obtain meaningful \nprobability distributions for the outcome variables using \nMonte Carlo simulation, but when this is not feasible, \nthe systems engineer must be content with only ranges \nand sensitivities. See the risk-informed decision analysis \nprocess in Subsection 6.8.2.8 for more information on \nuncertainty.\n\nThis essentially completes the analytical portion of the \ntrade study process. The next steps can be described as \nthe judgmental portion. Combining the selection rule \nwith the results of the analytical activity should enable \nthe systems engineer to array the alternatives from most \npreferred to least, in essence making a tentative selec-\ntion.\n\nThis tentative selection should not be accepted blindly. In \nmost trade studies, there is a need to subject the results \nto a \u201creality check\u201d by considering a number of ques-\ntions. Have the goals, objectives, and constraints truly \nbeen met? Is the tentative selection heavily dependent \non a particular set of input values to the measurement \nmethods, or does it hold up under a range of reasonable \ninput values? (In the latter case, the tentative selection \nis said to be robust.) Are there sufficient data to back up \nthe tentative selection? Are the measurement methods \nsufficiently discriminating to be sure that the tentative \nselection is really better than other alternatives? Have the \nsubjective aspects of the problem been fully addressed?\n\nIf the answers support the tentative selection, then the \nsystems engineer can have greater confidence in a rec-\nommendation to proceed to a further resolution of the \nsystem design, or to the implementation of that design. \nThe estimates of system effectiveness, its underlying per-\nformance or technical attributes, and system cost gen-\nerated during the trade study process serve as inputs \nto that further resolution. The analytical portion of the \ntrade study process often provides the means to quantify \nthe performance or technical (and cost) attributes that \nthe system\u2019s lower levels must meet. These can be formal-\nized as performance requirements.\n\nIf the reality check is not met, the trade study process re-\nturns to one or more earlier steps. This iteration may re-\nsult in a change in the goals, objectives, and constraints; \na new alternative; or a change in the selection rule, based \non the new information generated during the trade study. \nThe reality check may lead instead to a decision to first \n\nimprove the measures and measurement methods (e.g., \nmodels) used in evaluating the alternatives, and then to \nrepeat the analytical portion of the trade study process.\n\nControlling the Trade Study Process\nThere are a number of mechanisms for controlling the \ntrade study process. The most important one is the SEMP. \nThe SEMP specifies the major trade studies that are to be \nperformed during each phase of the project life cycle. It \nshould also spell out the general contents of trade study \nreports, which form part of the decision support pack-\nages (i.e., documentation submitted in conjunction with \nformal reviews and change requests).\n\nA second mechanism for controlling the trade study pro-\ncess is the selection of the study team leaders and mem-\nbers. Because doing trade studies is part art and part sci-\nence, the composition and experience of the team is an \nimportant determinant of a study\u2019s ultimate usefulness. \nA useful technique to avoid premature focus on a spe-\ncific technical design is to include in the study team indi-\nviduals with differing technology backgrounds.\n\nTrade Study Reports\n\nTrade study reports should be prepared for each \ntrade study. At a minimum, each trade study report \nshould identify:\n\nThe system under analysis ?\n\nSystem goals and objectives (or requirements, as  ?\nappropriate to the level of resolution), and con-\nstraints\n\nThe measures and measurement methods (mod- ?\nels) used\n\nAll data sources used ?\n\nThe alternatives chosen for analysis ?\n\nThe computational results, including uncertainty  ?\nranges and sensitivity analyses performed\n\nThe selection rule used ?\n\nThe recommended alternative. ?\n\nTrade study reports should be maintained as part of \nthe system archives so as to ensure traceability of de-\ncisions made through the systems engineering pro-\ncess. Using a generally consistent format for these \nreports also makes it easier to review and assimilate \nthem into the formal change control process.\n\n\n\n6.8 Decision Analysis\n\nNASA Systems Engineering Handbook ? 209\n\nAnother mechanism is limiting the number of alter-\nnatives that are to be carried through the study. This \nnumber is usually determined by the time and resources \navailable to do the study because the work required in \ndefining additional alternatives and obtaining the nec-\nessary data on them can be considerable. However, fo-\ncusing on too few or too similar alternatives defeats the \npurpose of the trade study process.\n\nA fourth mechanism for controlling the trade study pro-\ncess can be exercised through the use (and misuse) of \nmodels. Lastly, the choice of the selection rule exerts a \nconsiderable influence on the results of the trade study \nprocess. See Appendix O for different examples of how \ntrade studies are used throughout the life cycle. \n\n6.8.2.3 Cost-Benefit Analysis\nA cost-benefit analysis is performed to determine the \nadvantage of one alternative over another in terms of \nequivalent cost or benefits. The analysis relies on the ad-\ndition of positive factors and the subtraction of negative \nfactors to determine a net result. Cost-benefit analysis \nmaximizes net benefits (benefits minus costs). A cost-\nbenefit analysis finds, quantifies, and adds all the positive \nfactors. These are the benefits. Then it identifies, quanti-\nfies, and subtracts all the negatives, the costs. The dif-\nference between the two indicates whether the planned \naction is a preferred alternative. The real trick to doing \na cost-benefit analysis well is making sure to include all \nthe costs and all the benefits and properly quantify them. \nA similar approach, used when a cost cap is imposed \nexternally, is to maximize effectiveness for a given level \nof cost. Cost-effectiveness is a systematic quantitative \nmethod for comparing the costs of alternative means of \nachieving the same equivalent benefit for a specific ob-\njective. A project is cost-effective if, on the basis of life-\ncycle cost analysis of competing alternatives, it is deter-\nmined to have the lowest costs expressed in present value \nterms for a given amount of benefits.\n\nCost-effectiveness analysis is appropriate whenever it is \nimpractical to consider the dollar value of the benefits \nprovided by the alternatives. This is the scenario when-\never each alternative has the same life-cycle benefits ex-\npressed in monetary terms, or each alternative has the \nsame life-cycle effects, but dollar values cannot be as-\nsigned to their benefits. After determining the scope of \nthe project on the basis of mission and other require-\nments, and having identified, quantified, and valued \n\nthe costs and benefits of the alternatives, the next step is \nto identify the least-cost or most cost-effective alterna-\ntive to achieve the purpose of the project. A compara-\ntive analysis of the alternative options or designs is often \nrequired. This is illustrated in Figure 4.4-3. In cases in \nwhich alternatives can be defined that deliver the same \nbenefits, it is possible to estimate the equivalent rate be-\ntween each alternative for comparison. Least-cost anal-\nysis aims at identifying the least-cost project option for \nmeeting the technical requirements. Least-cost anal-\nysis involves comparing the costs of the various tech-\nnically feasible options and selecting the one with the \nlowest costs. Project options must be alternative ways of \nachieving the mission objectives. If differences in results \nor quality exist, a normalization procedure must be ap-\nplied that takes the benefits of one option relative to an-\nother as a cost to the option that does not meet all of \nthe mission objectives to ensure an equitable compar-\nison. Procedures for the calculation and interpretation \nof the discounting factors should be made explicit, with \nthe least-cost project being identified by comparing the \ntotal life-cycle costs of the project alternatives and calcu-\nlating the equalizing factors for the difference in costs. \nThe project with the highest equalizing factors for all \ncomparisons is the least-cost alternative.\n\nCost-effectiveness analysis also deals with alternative \nmeans of achieving mission requirements. However, \nthe results may be estimated only indirectly. For ex-\nample, different types of systems may be under con-\nsideration to obtain science data. The effectiveness of \neach alternative may be measured through obtaining \nscience data through different methods. An example \nof a cost-effectiveness analysis requires the increase in \nscience data to be divided by the costs for each alter-\nnative. The most cost-effective method is the one that \nraises science data by a given amount for the least cost. \nIf this method is chosen and applied to all similar al-\nternatives, the same increase in science data can be ob-\ntained for the lowest cost. Note, however, that the most \ncost-effective method is not necessarily the most effec-\ntive method of meeting mission objectives. Another \nmethod may be the most effective, but also cost a lot \nmore, so it is not the most cost-effective. The cost-ef-\nfectiveness ratios\u2014the cost per unit increase in science \ndata for each method\u2014can be compared to see how \nmuch more it would cost to implement the most effec-\ntive method. Which method is chosen for implemen-\ntation then depends jointly on the desired mission ob-\n\n\n\n210 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\njectives and the extra cost involved in implementing the \nmost effective method.\n\nThere will be circumstances where project alternatives \nhave more than one outcome. To assess the cost-effec-\ntiveness of the different alternatives, it is necessary to de-\nvise a testing system where the results for the different \nfactors can be added together. It also is necessary to de-\ncide on weights for adding the different elements to-\ngether, reflecting their importance in relation to the ob-\njectives of the project. Such a cost-effectiveness analysis is \ncalled weighted cost-effectiveness analysis. It introduces \na subjective element, the weights, into the comparison of \nproject alternatives, both to find the most cost-effective \nalternative and to identify the extra cost of implementing \nthe most effective alternative.\n\n6.8.2.4 Influence Diagrams\nAn influence diagram (also called a decision network) is \na compact graphical and mathematical representation of \na decision state. (See Figure 6.8-6.) Influence diagrams \nwere first developed in the mid-1970s within the deci-\nsion analysis community as an intuitive approach that is \neasy to understand. They are now adopted widely and \nare becoming an alternative to decision trees, which \ntypically suffer from exponential growth in number of \nbranches with each variable modeled. An influence di-\nagram is directly applicable in team decision analysis \n\nsince it allows incomplete sharing of information among \nteam members to be modeled and solved explicitly. Its \nelements are:\n\nDecision nodes, indicating the decision inputs, and  ?\nthe items directly influenced by the decision out-\ncome;\nChance nodes, indicating factors that impact the  ?\nchance outcome, and items influenced by the chance \noutcome;\nValue nodes, indicating factors that affect the value,  ?\nand items influenced by the value; and \nArrows, indicating the relationships among the ele- ?\nments.\n\nAn influence diagram does not depict a strictly sequen-\ntial process. Rather, it illustrates the decision process at \na particular point, showing all of the elements impor-\ntant to the decision. The influence diagram for a partic-\nular model is not unique. The strength of influence dia-\ngrams is their ability to display the structure of a decision \nproblem in a clear, compact form, useful both for com-\nmunication and to help the analyst think clearly during \nproblem formulation. An influence diagram can be \ntransformed into a decision tree for quantification. \n\n6.8.2.5 Decision Trees\nLike the influence diagram, a decision tree portrays a de-\ncision model, but a decision tree is drawn from a point \nof view different from that of the influence diagram. The \ndecision tree exhaustively works out the expected con-\nsequences of all decision alternatives by discretizing all \n\u201cchance\u201d nodes, and, based on this discretization, calcu-\nlating and appropriately weighting all possible conse-\nquences of all alternatives. The preferred alternative is then \nidentified by summing the appropriate outcome variables \n(MOE or expected utility) from the path end states. \n\nA decision tree grows horizontally from left to right, \nwith the trunk at the left. Typically, the possible alterna-\ntives initially available to the decisionmaker stem from \nthe trunk at the left. Moving across the tree, the deci-\nsionmaker encounters branch points corresponding to \nprobabilistic outcomes and perhaps additional decision \nnodes. Thus, the tree branches as it is read from left to \nright. At the far right side of the decision tree, a vector \nof TPM scores is listed for each terminal branch, rep-\nresenting each combination of decision outcome and \nchance outcome. From the TPM scores, and the chosen \nselection rule, a preferred alternative is determined. \n\nConsequences\n\nForecast Hurricane Path\nHits\nMisses\n\nDecision Table:\nDecision\\\nHurricane Path\n\nWill Hit\nWill Miss\n\nEvacuate\nStay Decision \n\nElements of Influence Diagrams\n\nChance \n\nDecision \n\nValue\n\nValue\n\nDecision node: represents\nalternatives\n\nChance node: represents\nevents (states of nature)\n\nValue node: represents\nconsequences, objectives,\nor calculations}\n\nFigure 6.8?6 Influence diagrams\n\n\n\n6.8 Decision Analysis\n\nNASA Systems Engineering Handbook ? 211\n\nIn even moderately complicated problems, decision trees \ncan quickly become difficult to understand. Figure 6.8-7 \nshows a sample of a decision tree. This figure only shows \na simplified illustration. A complete decision tree with \nadditional branches would be expanded to the appro-\npriate level of detail as required by the analysis. A com-\nmonly employed strategy is to start with an equivalent \ninfluence diagram. This often aids in helping to under-\nstand the principal issues involved. Some software pack-\nages make it easy to develop an influence diagram and \nthen, based on the influence diagram, automatically fur-\nnish a decision tree. The decision tree can be edited if \nthis is desired. Calculations are typically based on the de-\ncision tree itself.\n\n6.8.2.6 Multi-Criteria Decision Analysis\nMulti-Criteria Decision Analysis (MCDA) is a method \naimed at supporting decisionmakers who are faced with \nmaking numerous and conflicting evaluations. These \ntechniques aim at highlighting the conflicts in alterna-\ntives and deriving a way to come to a compromise in \na transparent process. For example, NASA may apply \nMCDA to help assess whether selection of one set of \n\nsoftware tools for every NASA application is cost effec-\ntive. MCDA involves a certain element of subjectiveness; \nthe bias and position of the team implementing MCDA \nplay a significant part in the accuracy and fairness of de-\ncisions. One of the MCDA methods is the Analytic Hi-\nerarchy Process (AHP).\n\nThe Analytic Hierarchy Process\nAHP was first developed and applied by Thomas Saaty. \nAHP is a multi-attribute methodology that provides a \nproven, effective means to deal with complex decision-\nmaking and can assist with identifying and weighting \nselection criteria, analyzing the data collected for the \ncriteria, and expediting the decisionmaking process. \nMany different problems can be investigated with the \nmathematical techniques of this approach. AHP helps \ncapture both subjective and objective evaluation mea-\nsures, providing a useful mechanism for checking the \nconsistency of the evaluation measures and alternatives \nsuggested by the team, and thus reducing bias in deci-\nsionmaking. AHP is supported by pair-wise compar-\nison techniques, and it can support the entire decision \nprocess. AHP is normally done in six steps:\n\nFigure 6.8?7 Decision tree\n\nAlternative\nB\n\nAlternative\nA\n\n100%\n\n100 Hours \n\nNone\n\nLocalized\n\nWidespread\n\nCost\n\n0.0\n\nNone\n\nLocalized\n\nWidespread\n\nContam-\nination\n\nCrew\nHours\n\nCost\n\n0.00.0\n\n0.00.0\n\n0.1\n\n0.1\n\n0.1\n\n0.1 0.1\n\n0.9\n\n0.90.9\n\n0.8\n\n0.30.3\n\n0.70.7\n\n100%\n\n120%\n\n110%\n\n110%\n\n120%\n\n100 Hours \n\n150 Hours \n\n150 Hours \n\n200 Hours \n\n200 Hours \n\nCrew\nHours\n\nContam-\nination\n\nAlternative\nSelection \n\nAlternative\nC\n\n\n\n212 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nDescribe in summary form the alternatives under 1. \nconsideration.\nDevelop a set of high-level objectives.2. \nDecompose the high-level objective from general to 3. \nspecific to produce an objectives hierarchy.\nDetermine the relative importance of the evalua-4. \ntion objectives and attributes by assigning weights \narrived at by engaging experts through a structured \nprocess such as interviews or questionnaires.\nHave each expert make pair-wise comparisons of 5. \nthe performance of each decision alternative with \nrespect to a TPM. Repeat this for each TPM. Com-\nbine the results of these subjective evaluations math-\nematically using a process or, commonly, an avail-\nable software tool that ranks the alternatives.\nIterate the interviews/questionnaires and AHP eval-6. \nuation process until a consensus ranking of the al-\nternatives is achieved.\n\nIf AHP is used only to produce the TPM weights to be \nused in a PI or MOE calculation, then only the first four \nsteps listed above are applicable.\n\nWith AHP, consensus may be achieved quickly or several \nfeedback rounds may be required. The feedback consists \nof reporting the computed ranking, for each evaluator \nand for the group, for each option, along with the reasons \nfor differences in rankings, and identified areas of diver-\ngence. Experts may choose to change their judgments on \nTPM weights. At this point, divergent preferences can be \ntargeted for more detailed study. AHP assumes the ex-\nistence of an underlying preference vector with magni-\ntudes and directions that are revealed through the pair-\nwise comparisons. This is a powerful assumption, which \nmay at best hold only for the participating experts. The \nranking of the alternatives is the result of the experts\u2019 \njudgments and is not necessarily a reproducible result. \nFor further information on AHP, see references by Saaty, \nThe Analytic Hierarchy Process.\n\nFlexibility and Extensibility Attributes\n\nIn some decision situations, the selection of a particular \ndecision alternative will have implications for the long \nterm that are very difficult to model in the present. In \nsuch cases, it is useful to structure the problem as a series \nof linked decisions, with some decisions to be made in \nthe near future and others to be made later, perhaps on \nthe basis of information to be obtained in the meantime. \n\nThere is value in delaying some decisions to the future, \nwhen additional information will be available. Some \ntechnology choices might foreclose certain opportuni-\nties that would be preserved by other choices. \n\nIn these cases, it is desirable to consider attributes such \nas \u201cflexibility\u201d and \u201cextensibility.\u201d Flexibility refers to the \nability to support more than one current application. Ex-\ntensibility refers to the ability to be extended to other ap-\nplications. For example, in choosing an architecture to \nsupport lunar exploration, one might consider exten-\nsibility to Mars missions. A technology choice that im-\nposes a hard limit on mass that can be boosted into a \nparticular orbit has less flexibility than a choice that is \nmore easily adaptable to boost more. Explicitly adding \nextensibility and flexibility as attributes to be weighted \nand evaluated allows these issues to be addressed sys-\ntematically. In such applications, extensibility and flex-\nibility are being used as surrogates for certain future per-\nformance attributes. \n\n6.8.2.7 Utility Analysis\n\u201cUtility\u201d is a measure of the relative value gained from \nan alternative. Given this measure, the team looks at in-\ncreasing or decreasing utility, and thereby explain alterna-\ntive decisions in terms of attempts to increase their utility. \nThe theoretical unit of measurement for utility is the util.\n\nThe utility function maps the range of the TPM into the \nrange of associated utilities, capturing the decisionmak-\ner\u2019s preferences and risk attitude. It is possible to imagine \nsimply mapping the indicated range of values linearly \nonto the interval [0,1] on the utility axis, but in general, \nthis would not capture the decisionmaker\u2019s preferences. \nThe decisionmaker\u2019s attitude toward risk causes the \ncurve to be convex (risk prone), concave (risk averse), or \neven some of each. \n\nThe utility function directly reflects the decisionmaker\u2019s \nattitude toward risk. When ranking alternatives on the \nbasis of utility, a risk-averse decisionmaker will rank an \nalternative with highly uncertain performance below an \nalternative having the same expected performance but \nless uncertainty. The opposite outcome would result for \na risk-prone decisionmaker. When the individual TPM \nutility functions have been assessed, it is important to \ncheck the result for consistency with the decisionmaker\u2019s \nactual preferences (e.g., is it true that intermediate values \nof TPM1 and TPM2 are preferred to a high value of TPM1 \nand a low value of TPM2). \n\n\n\n6.8 Decision Analysis\n\nNASA Systems Engineering Handbook ? 213\n\nAn example of a utility function for the TPM \u201cvolume\u201d \nis shown in Figure 6.8-8. This measure was developed \nin the context of design of sensors for a space mission. \nVolume was a precious commodity. The implication of \nthe graph is that low volume is good, large volume is bad, \nand the decisionmaker would prefer a design alternative \nwith a very well-determined volume of a few thousand \ncc\u2019s to an alternative with the same expected volume but \nlarge uncertainty. \n\nSometimes the expected utility is referred to as a PI. An \nimportant benefit of applying this method is that it is the \nbest way to deal with significant uncertainties when the \ndecisionmaker is not risk neutral. Probabilistic methods \nare used to treat uncertainties. A downside of applying \nthis method is the need to quantify the decisionmaker\u2019s \nrisk attitudes. Top-level system architecture decisions are \nnatural examples of appropriate applications of MAUT. \n\n6.8.2.8 Risk-Informed Decision Analysis \nProcess Example\n\nIntroduction\n\nA decision matrix works for many decisions, but the de-\ncision matrix may not scale up to very complex decisions \nor risky decisions. For some decisions, a tool is needed \nto handle the complexity. The following subsection de-\nscribes a detailed Decision Analysis Process that can be \nused to support a risk-informed decision.\n\nIn practice, decisions are made in many different ways. \nSimple approaches may be useful, but it is important to \nrecognize their limitations and upgrade to better anal-\nysis when this is warranted. Some decisionmakers, when \nfaced with uncertainty in an important quantity, deter-\nmine a best estimate for that quantity, and then reason as \nif the best estimate were correct. This might be called the \n\u201ctake-your-best-shot\u201d approach. Unfortunately, when \nthe stakes are high, and uncertainty is significant, this \nbest-shot approach may lead to poor decisions. \n\nThe following steps are a risk-informed decision analysis \nprocess:\n\nFormulation of the objectives hierarchy, TPMs.1. \nProposing and identifying decision alternatives. 2. \nAlternatives from this process are combined with \nthe alternatives identified in the other systems en-\ngineering processes including the Design Solution \nDefinition Process, but also including verification \nand validation as well as production.\nRisk analysis of decision alternatives and ranking of 3. \nalternatives. \nDeliberation and recommendation of decision alter-4. \nnatives.\nFollowup tracking of the implementation of the de-5. \ncision.\n\nThese steps support good decisions by focusing first on \nobjectives, next on developing decision alternatives with \nthose objectives clearly in mind and/or using decision al-\n\nFigure 6.8?8 Utility function for a \u201cvolume\u201d \nperformance measure\n\n2,000             4,000          6,000          8,000        10,000        12,000\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\nUtility\n\nVolume (ccm) \n\nValue functions can take the place of utility functions \nwhen a formal treatment of risk attitude is unnecessary. \nThey appear very similar to utility functions, but have \none important difference. Value functions do not con-\nsider the risk attitude of the decisionmaker. They do not \nreflect how the decisionmaker compares certain out-\ncomes to uncertain outcomes.\n\nThe assessment of a TPM\u2019s value function is relatively \nstraightforward. The \u201cbest\u201d end of the TPM\u2019s range is as-\nsigned a value of 1. The \u201cworst\u201d is assigned a value of \n0. The decisionmaker makes direct assessments of the \nvalue of intermediate points to establish the preference \nstructure in the space of possible TPM values. The utility \nfunction can be treated as a value function, but the value \nfunction is not necessarily a utility function. \n\nOne way to rank alternatives is to use a Multi-Attribute, \nUtility Theory (MAUT) approach. With this approach, \nthe \u201cexpected utility\u201d of each alternative is quantified, and \nalternatives are ranked based on their expected utilities. \n\n\n\n214 ? NASA Systems Engineering Handbook\n\n6.0 Crosscutting Technical Management\n\nternatives that have been developed under other systems \nengineering processes. The later steps of the Decision \nAnalysis Process interrelate heavily with the Technical \nRisk Management Process, as indicated in Figure 6.8-9. \nThese steps include risk analysis of the decision alterna-\ntives, deliberation informed by risk analysis results, and \nrecommendation of a decision alternative to the deci-\nsionmaker. Implementation of the decision is also im-\nportant.\n\nObjectives Hierarchy/TPMs\nAs shown in Figure 6.8-9, risk-informed decision anal-\nysis starts with formulation of the objectives hierarchy. \nUsing this hierarchy, TPMs are formulated to quantify \nperformance of a decision with respect to the program \nobjectives. The TPMs should have the following charac-\nteristics:\n\nThey can support ranking of major decision alternatives.  ?\nThey are sufficiently detailed to be used directly in the  ?\nrisk management process. \nThey are preferentially independent. This means that  ?\nthey contribute in distinct ways to the program goal. \nThis property helps to ensure that alternatives are \nranked appropriately. \n\nAn example of an objectives hierarchy is shown in \nFigure 6.8-10. Details will vary from program to pro-\ngram, but a construct like Figure 6.8-10 is behind the \nprogram-specific objectives hierarchy.\n\nThe TPMs in this figure are meant to be generically im-\nportant for many missions. Depending on the mission, \nthese TPMs are further subdivided to the point where \nthey can be objectively measured. Not all TPMs can be \nmeasured directly. For example, safety-related TPMs are \n\nFigure 6.8?9 Risk?informed Decision Analysis Process\n\nTechnical Risk Management\n\nFormulation of Objectives\nHierarchy and TPMs\n\nDecision Analysis\n\nDecisionmaking\nand\n\nImplementation\nof Decision\nAlternative\n\nProposing and/or\nIdentifying Decision\n\nAlternatives\n\nRisk Analysis of Decision\nAlternatives, Performing\n\nTrade Studies and Ranking\n\nDeliberating and\nRecommending a\n\nDecision Alternative\n\nRisk Analysis of Decision\nAlternatives, Performing\n\nTrade Studies and Ranking\n\nStakeholder\nExpectation,\n\nRequirements\nDefinition/\n\nManagement\n\nDesign Solution,\nTechnical Planning\n\nDesign Solution,\nTechnical Planning,\n\nDecision Analysis\n\nTechnical Planning,\nDecision Analysis\n\nDecision Analysis,\nLessons Learned,\n\nKnowledge\nManagement \n\nTracking and Controlling\nPerformance Deviations\n\nRecognition\nof Issues\n\nor\nOpportunities\n\n\n\n6.8 Decision Analysis\n\nNASA Systems Engineering Handbook ? 215\n\nFigure 6.8?10 Example of an objectives hierarchy\n\nMission Success\n\nAffordability\n\nMeet\nBudget\n\nConstraints \n\nDecision\nAlternative \n\nTechnical Performance Measures \n\nDesign/\nDevelopment \n\nCost\nOverrun\n\nO\nBJ\n\nEC\nTI\n\nV\nES\n\n H\nIE\n\nRA\nRC\n\nH\nY \n\nPE\nRF\n\nO\nRM\n\nA\nN\n\nCE\n M\n\nEA\nSU\n\nRE\nS\n\n(re\npr\n\nes\nen\n\nta\ntiv\n\ne \nm\n\nea\nsu\n\nre\ns a\n\nre\n sh\n\now\nn)\n\nEconomics and\nSchedule Models\n\nModels to Assess Life-\nCycle Cost and\n\nSchedule Performance\n\nSchedule\nSlippage\n\nOperation\nCost\n\nOverrun\n\u2026\n\nLoss of\nMission\n\nFunction x\n\nMass/\nCargo\n\nCapacity\n\nReliability/\nAvailability/\n\nQA\nEffectiveness\n\nCommercial\nExtensibility\n\nLoss of \nSupport\n\nCapability\nx\n\nLoss of\nFlight\n\nSystems\n\nLoss of\nPublic\n\nProperty\n\nPlanetary\nContamination\n\nEarth\nContamination\n\nAstronaut\nDeath or\n\nInjury\n\nPublic\n Death\n\nor Injury\n\nScience\nCommunity\n\nSupport\n\nPublic\nSupport\n\n\u2026\n\nMeet\nSchedules \n\nSafety\nOther Stakeholders\u2019\n\nSupport \nTechnical Objectives\n\nand Performance \n\n? Models Quantifying Capability Metrics Relating to Mission Requirements (Mass,\nThrust, Cargo Capacity)\n? Models Quantifying Metrics Relating to Probability of Loss of Mission-Critical \n\nSystem/Function (e.g., PRA)\n? Models Quantifying  Metrics Relating to Frequency or Probability of Failure to Meet\n\nHigh-Level Safety Objectives (e.g., PRA)\n\nModels Quantifying Technical Performance Measures\n\nStakeholder\nModels \n\nAchieve\nMission-\nCritical\n\nFunctions \n\nEnhance\nEffectiveness/\n Performance\n\nProvide\nExtensibility/\nSupportability\n\nProtect\nWorkforce\n\n& Public\nHealth\n\nProtect\nEnvironment\n\nProtect\nMission\n& Public\nAssets\n\nRealize\nOther\n\nStakeholders\u2019\nSupport\n\nM\nO\n\nD\nEL\n\n-B\nA\n\nSE\nD\n\nQ\nU\n\nA\nN\n\nTI\nFI\n\nCA\nTI\n\nO\nN\n\nO\nF \n\nPE\nRF\n\nO\nRM\n\nA\nN\n\nCE\nM\n\nEA\nSU\n\nRE\nS \n\ndefined in terms of the probability of a consequence type \nof a specific magnitude (e.g., probability of any general \npublic deaths or injuries) or the expected magnitude of \na consequence type (e.g., the number of public deaths \nor injuries). Probability of Loss of Mission and Prob-\n\nability of Loss of Crew (P(LOM) and P(LOC)) are two \nparticularly important safety-related TPMs for manned \nspace missions. Because an actuarial basis does not suf-\nfice for prediction of these probabilities, modeling will \nbe needed to quantify them. \n\n\n\n\n\nNASA Systems Engineering Handbook ? 217\n\nThe topics below are of special interest for enhancing \nthe performance of the systems engineering process or \nconstitute special considerations in the performance \nof systems engineering. The first section elucidates the \nprocess of how the systems engineering principles need \nto be applied to contracting and contractors that im-\nplement NASA processes and create NASA products. \n\n7.0 Special Topics\n\nApplying lessons learned enhances the efficiency of the \npresent with the wisdom of the past. Protecting the en-\nvironment and the Nation\u2019s space assets are important \nconsiderations in the design and development of re-\nquirements and designs. Integrated design can en-\nhance the efficiency and effectiveness of the design \nprocess.\n\n7.1.1 Introduction, Purpose, and Scope\nHistorically, most successful NASA projects have de-\npended on effectively blending project management, sys-\ntems engineering, and technical expertise among NASA, \ncontractors, and third parties. Underlying these suc-\ncesses are a variety of agreements (e.g., contract, mem-\norandum of understanding, grant, cooperative agree-\nment) between NASA organizations or between NASA \nand other Government agencies, Government organiza-\ntions, companies, universities, research laboratories, and \nso on. To simplify the discussions, the term \u201ccontract\u201d is \nused to encompass these agreements. \n\nThis section focuses on the engineering activities perti-\nnent to awarding a contract, managing contract perfor-\nmance, and completing a contract. However, interfaces \nto the procurement process will be covered, since the en-\ngineering technical team plays a key role in development \nand evaluation of contract documentation.\n\nContractors and third parties perform activities that sup-\nplement (or substitute for) the NASA project technical \nteam accomplishment of the common technical process \nactivities and requirements. Since contractors might be \ninvolved in any part of the systems engineering life cycle, \nthe NASA project technical team needs to know how to \nprepare for, perform, and complete surveillance of tech-\nnical activities that are allocated to contractors.\n\n7.1.2 Acquisition Strategy\nCreating an acquisition strategy for a project is a col-\nlaborative effort among several NASA HQ offices that \n\n7.1 Engineering with Contracts\n\nleads to approval for project execution. The program \nand project offices characterize the acquisition strategy \nin sufficient detail to identify the contracts needed to ex-\necute the strategy. Awarding contracts at the project level \noccurs in the context of the overall program acquisition \nstrategy. \n\nWhile this section pertains to projects where the decision \nhas been made to have a contractor implement a por-\ntion of the project, it is important to remember that the \nchoice between \u201cmaking\u201d a product in-house by NASA \nor \u201cbuying\u201d it from a contractor is one of the most cru-\ncial decisions in systems development. (See Section 5.1.) \nQuestions that should be considered in the \u201cmake/buy\u201d \ndecision include the following:\n\nIs the desired system a development item or more off  ?\nthe shelf?\nWhat is the relevant experience of NASA versus po- ?\ntential contractors?\nWhat are the relative importance of risk, cost, schedule,  ?\nand performance?\nIs there a desire to maintain an \u201cin-house\u201d capability? ?\n\nAs soon as it is clear that a contract will be needed to \nobtain a system or service, the responsible project man-\nager should contact the local procurement office. The \ncontracting officer will assign a contract specialist to \nnavigate the numerous regulatory requirements that af-\nfect NASA procurements and guide the development \nof contract documentation needed to award a contract. \nThe contract specialist engages the local legal office as \nneeded.\n\n\n\n218 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\ntegration, and details of product support. Similarly, the \ntechnical team provides corporate knowledge to identify \nand evaluate risks of acquiring the desired product, es-\npecially regarding the proposed contract type and par-\nticular contract elements. \n\n7.1.2.2 Acquisition Life Cycle\nContract activities are part of the broader acquisi-\ntion life cycle, which comprises the phases solicitation, \nsource selection, contract monitoring, and acceptance. \n(See Figure 7.1-1.) The acquisition life cycle overlaps \nand interfaces with the systems engineering processes \nin the project life cycle. Acquisition planning focuses on \ntechnical planning when a particular contract (or pur-\nchase) is required. (See Section 6.1.) In the figure below, \nrequirements development corresponds to the Tech-\nnical Requirements Definition Process in the systems \nengineering engine. (See Figure 2.1-1.) The next four \nphases\u2014solicitation, source selection, contract moni-\ntoring, and acceptance\u2014are the phases of the contract \nactivities. Transition to operations and maintenance rep-\nresents activities performed to transition acquired prod-\nucts to the organization(s) responsible for operating \nand maintaining them (which could be contractor(s)). \nAcquisition management refers to project management \nactivities that are performed throughout the acquisition \nlife cycle by the acquiring organization.\n\n7.1.2.3 NASA Responsibility for Systems \nEngineering\n\nThe technical team is responsible for systems engineering \nthroughout the acquisition life cycle. The technical team \ncontributes heavily to systems engineering decisions and \nresults, whatever the acquisition strategy, for any combi-\nnation of suppliers, contractors, and subcontractors. The \ntechnical team is responsible for systems engineering \nwhether the acquisition strategy calls for the technical \nteam, a prime contractor, or some combination of the \ntwo to perform system integration and testing of prod-\nucts from multiple sources. \n\nThis subsection provides specific guidance on how to as-\nsign responsibility when translating the technical pro-\ncesses onto a contract. Generally, the Technical Planning, \n\n7.1.2.1 Develop an Acquisition Strategy\nThe project manager, assisted by the assigned procure-\nment and legal offices, first develops a project acquisition \nstrategy or verifies the one provided. The acquisition \nstrategy provides a business and technical management \noutline for planning, directing, and managing a project \nand obtaining products and services via contract.\n\nIn some cases, it may be appropriate to probe outside \nsources in order to gather sufficient information to for-\nmulate an acquisition strategy. This can be done by is-\nsuing a Request for Information (RFI) to industry and \nother parties that may have interest in potential future \ncontracts. An RFI is a way to obtain information about \ntechnology maturity, technical challenges, capabilities, \nprice and delivery considerations, and other market in-\nformation that can influence strategy decisions.\n\nThe acquisition strategy includes:\nObjectives of the acquisition\u2014capabilities to be pro- ?\nvided, major milestones;\nAcquisition approach\u2014single step or evolutionary  ?\n(incremental), single or multiple suppliers/contracts, \ncompetition or sole source, funding source(s), phases, \nsystem integration, Commercial-Off-the-Shelf \n(COTS) products;\nBusiness considerations\u2014constraints (e.g., funding,  ?\nschedule), availability of assets and technologies, ap-\nplicability of commercial items versus internal tech-\nnical product development;\nRisk management of acquired products or services\u2014 ?\nmajor risks and risk sharing with the supplier;\nContract types\u2014performance-based or level of effort,  ?\nfixed-price or cost reimbursable; \nContract elements\u2014incentives, performance param- ?\neters, rationale for decisions on contract type; and\nProduct support strategy\u2014oversight of delivered  ?\nsystem, maintenance, and improvements.\n\nThe technical team gathers data to facilitate the decision-\nmaking process regarding the above items. The technical \nteam knows about issues with the acquisition approach, \ndetermining availability of assets and technologies, ap-\nplicability of commercial items, issues with system in-\n\nFigure 7.1?1 Acquisition life cycle \n\nAcceptance\nContract \n\nMonitoring\nSource \n\nSelection\nSolicitation\n\nRequirements \nDevelopment\n\nAcquisition \nPlanning\n\nTransition to\nOperations &\nMaintenance \n\n\n\n7.1 Engineering with Contracts\n\nNASA Systems Engineering Handbook ? 219\n\nInterface Management, Technical Risk Management, \nConfiguration Management, Technical Data Manage-\nment, Technical Assessment, and Decision Analysis pro-\ncesses should be implemented throughout the project \nby both the NASA team and the contractor. Stakeholder \nExpectations Definition, Technical Requirements Defi-\nnition, Logical Decomposition, Design Solution Defini-\ntion, Product Implementation and Integration, Product \nVerification and Validation, Product Transition, and Re-\nquirements Management Processes are implemented by \nNASA or the contractor depending upon the level of the \nproduct decomposition. \n\nTable 7.1-1 provides guidance on how to implement the \n17 technical processes from NPR 7123.1. The first two \ncolumns have the number of the technical process and \nthe requirement statement of responsibility. The next \ncolumn provides general guidance on how to distin-\nguish who has responsibility for implementing the pro-\ncess. The last column provides a specific example of the \napplication of how to implement the process for a par-\nticular project. The particular scenario is a science mis-\nsion where a contractor is building the spacecraft, NASA \nassigns Government-Furnished Property (GFP) instru-\nments to the contractor, and NASA operates the mission.\n\n7.1.3 Prior to Contract Award\n\n7.1.3.1 Acquisition Planning\nBased on the acquisition strategy, the technical team \nneeds to plan acquisitions and document the plan \nin developing the SEMP. The SEMP covers the tech-\nnical team\u2019s involvement in the periods before con-\ntract award, during contract performance, and upon \ncontract completion. Included in acquisition planning \nare solicitation preparation, source selection activi-\nties, contract phase-in, monitoring contractor perfor-\nmance, acceptance of deliverables, completing the con-\ntract, and transition beyond the contract. The SEMP \nfocuses on interface activities with the contractor, in-\ncluding NASA technical team involvement with and \nmonitoring of contracted work.\n\nOften overlooked in project staffing estimates is the \namount of time that technical team members are in-\nvolved in contracting-related activities. Depending on \nthe type of procurement, a technical team member in-\nvolved in source selection could be consumed nearly full \ntime for 6 to 12 months. After contract award, technical \nmonitoring consumes 30 to 50 percent, peaking at full \n\ntime when critical milestones or key deliverables arrive. \nKeep in mind that for most contractor activities, NASA \nstaff performs supplementary activities.\n\nThe technical team is intimately involved in developing \ntechnical documentation for the acquisition package. The \nacquisition package consists of the solicitation (e.g., Re-\nquest for Proposals (RFPs) and supporting documents. \nThe solicitation contains all the documentation that is \nadvertised to prospective contractors (or offerors). The \nkey technical sections of the solicitation are the SOW (or \nperformance work statement), technical specifications, \nand contract data requirements list. Other sections of \nthe solicitation include proposal instructions and eval-\nuation criteria. Documents that support the solicitation \ninclude a procurement schedule, source evaluation plan, \nGovernment cost estimate, and purchase request. Input \nfrom the technical team will be needed for some of the \nsupporting documents.\n\nIt is the responsibility of the contract specialist, with \ninput from the technical team, to ensure that the appro-\npriate clauses are included in the solicitation. The con-\ntract specialist is familiar with requirements in the Fed-\neral Acquisition Regulation (FAR) and the NASA FAR \nSupplement (NFS) that will be included in the solicita-\n\nSolicitations\n\nThe release of a solicitation to interested parties is \nthe formal indication of a future contract. A solicita-\ntion conveys sufficient details of a Government need \n(along with terms, conditions, and instructions) to al-\nlow prospective contractors (or offerors) to respond \nwith a proposal. Depending on the magnitude and \ncomplexity of the work, a draft solicitation may be \nissued. After proposals are received, a source evalu-\nation board (or committee) evaluates technical and \nbusiness proposals per its source evaluation plan \nand recommends a contractor selection to the con-\ntracting officer. The source evaluation board, led by a \ntechnical expert, includes other technical experts and \na contracting specialist. The source selection process \nis completed when the contracting officer signs the \ncontract. \n\nThe most common NASA solicitation types are RFP \nand Announcement of Opportunity (AO). Visit the on-\nline NASA Procurement Library for a full range of de-\ntails regarding procurements and source selection.\n\n\n\n220 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nTable 7.1?1 Applying the Technical Processes on Contract\n\n# NPR 7123.1 Process\nGeneral Guidance on Who \n\nImplements the Process Application to a Science Mission\n\n1 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for the definition of stake-\nholder expectations for the applicable \nWBS model.\n\nIf stakeholders are at the contrac-\ntor, then the contractor should \nhave responsibility and vice versa.\n\nStakeholders for the mission/proj-\nect are within NASA; stakeholders \nfor the spacecraft power subsystem \nare mostly at the contractor.\n\n2 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for definition of the \ntechnical requirements from the set of \nagreed-upon stakeholder expectations for \nthe applicable WBS model.\n\nAssignment of responsibility \nfollows the stakeholders, e.g., if \nstakeholders are at the contractor, \nthen requirements are developed \nby the contractor and vice versa.\n\nNASA develops the high-level \nrequirements, and the contractor \ndevelops the requirements for the \npower subsystem.\n\n3 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for logical decomposition \nof the validated technical requirements of \nthe applicable WBS.\n\nFollows the requirements, e.g., if \nrequirements are developed at \nthe contractor, then the decom-\nposition of those requirements is \nimplemented by the contractor \nand vice versa.\n\nNASA performs the decomposition \nof the high-level requirements, and \nthe contractor performs the decom-\nposition of the power subsystem \nrequirements.\n\n4 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for designing product \nsolution definitions within the applicable \nWBS model that satisfy the derived techni-\ncal requirements.\n\nFollows the requirements, e.g., if \nrequirements are developed at the \ncontractor, then the design of the \nproduct solution is implemented \nby the contractor and vice versa.\n\nNASA designs the mission/project, \nand the contractor designs the \npower subsystem.\n\n5 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for implementation of \na design solution definition by making, \nbuying, or reusing an end product of the \napplicable WBS model.\n\nFollows the design, e.g., if the \ndesign is developed at the con-\ntractor, then the implementation \nof the design is performed by the \ncontractor and vice versa.\n\nNASA implements (and retains \nresponsibility for) the design for the \nmission/project, and the contractor \ndoes the same for the power \nsubsystem.\n\n6 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for the integration of \nlower level products into an end product \nof the applicable WBS model in accor-\ndance with its design solution definition.\n\nFollows the design, e.g., if the \ndesign is developed at the \ncontractor, then the integration of \nthe design elements is performed \nby the contractor and vice versa.\n\nNASA integrates the design for the \nmission/project, and the contractor \ndoes the same for the power \nsubsystem.\n\n7 The Center Directors or designees establish \nand maintain a process to include activities, \nrequirements, guidelines, and documenta-\ntion for verification of end products \ngenerated by the Product Implementation \nProcess or Product Integration Process \nagainst their design solution definitions.\n\nFollows the product integration, \ne.g., if the product integration is \nimplemented at the contractor, \nthen the verification of the prod-\nuct is performed by the contractor \nand vice versa.\n\nNASA verifies the mission/project, \nand the contractor does the same \nfor the power subsystem.\n\n (continued)\n\n\n\n7.1 Engineering with Contracts\n\nNASA Systems Engineering Handbook ? 221\n\n# NPR 7123.1 Process\nGeneral Guidance on Who \n\nImplements the Process Application to a Science Mission\n\n8 The Center Directors or designees establish \nand maintain a process to include activities, \nrequirements, guidelines, and documenta-\ntion for validation of end products \ngenerated by the Product Implementation \nProcess or Product Integration Process \nagainst their stakeholder expectations.\n\nFollows the product integration, \ne.g., if the product integration is \nimplemented at the contractor, \nthen the validation of the product \nis performed by the contractor \nand vice versa.\n\nNASA validates the mission/project, \nand the contractor does the same \nfor the power subsystem.\n\n9 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for transitioning end \nproducts to the next-higher-level WBS \nmodel customer or user.\n\nFollows the product verification and \nvalidation, e.g., if the product verifica-\ntion and validation is implemented \nat the contractor, then the transition \nof the product is performed by the \ncontractor and vice versa.\n\nNASA transitions the mission/proj-\nect to operations, and the contrac-\ntor transitions the power subsystem \nto the spacecraft level.\n\n10 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for planning the technical \neffort.\n\nAssuming both NASA and the \ncontractor have technical work \nto perform, then both NASA and \nthe contractor need to plan their \nrespective technical efforts.\n\nNASA would plan the technical \neffort associated with the GFP \ninstruments and the launch and op-\nerations of the spacecraft, and the \ncontractor would plan the technical \neffort associated with the design, \nbuild, verification and validation, \nand delivery and operations of the \npower subsystem.\n\n11 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for management of \nrequirements defined and baselined \nduring the application of the system \ndesign processes.\n\nFollows process #2.\n\n12 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for management of \nthe interfaces defined and generated \nduring the application of the system \ndesign processes.\n\nInterfaces should be managed \none level above the elements \nbeing interfaced.\n\nThe interface from the spacecraft \nto the project ground system \nwould be managed by NASA, while \nthe power subsystem to attitude \ncontrol subsystem interface would \nbe managed by the contractor.\n\n13 The Center Directors or designees establish \nand maintain a process to include activities, \nrequirements, guidelines, and documenta-\ntion for management of the technical risk \nidentified during the technical effort. \nNPR 8000.4, Risk Management Procedural \nRequirements is to be used as a source \ndocument for defining this process; and \nNPR 8705.5, Probabilistic Risk Assessment \n(PRA) Procedures for NASA Programs and \nProjects provides one means of identifying \nand assessing technical risk.\n\nTechnical risk management \nis a process that needs to be \nimplemented by both NASA and \nthe contractor. All elements of the \nproject need to identify their risks \nand participate in the project risk \nmanagement process. Deciding \nwhich risks to mitigate, when, at \nwhat cost is generally a function of \nNASA project management.\n\nNASA project management should \ncreate a project approach to risk \nmanagement that includes partici-\npation from the contractor. Risks \nidentified throughout the project \ndown to the power subsystem level \nand below should be identified \nand reported to NASA for possible \nmitigation.\n\n (continued)\n\nTable 7.1?1 Applying the Technical Processes on Contract (continued)\n\n\n\n222 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\n# NPR 7123.1 Process\nGeneral Guidance on Who \n\nImplements the Process Application to a Science Mission\n\n14 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for CM.\n\nLike risk management, CM is a \nprocess that should be imple-\nmented throughout the project \nby both the NASA and contractor \nteams.\n\nNASA project management should \ncreate a project approach to CM \nthat includes participation from \nthe contractor. The contractor\u2019s \ninternal CM process will have to be \nintegrated with the NASA approach. \nCM needs to be implemented \nthroughout the project down to the \npower subsystem level and below.\n\n15 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for management of the \ntechnical data generated and used in \nthe technical effort.\n\nLike risk management and CM, \ntechnical data management is \na process that should be imple-\nmented throughout the project \nby both the NASA and contractor \nteams.\n\nNASA project management should \ncreate a project approach to \ntechnical data management that \nincludes participation from the \ncontractor. The contractor\u2019s internal \ntechnical data process will have to \nbe integrated with the NASA ap-\nproach. Management of technical \ndata needs to be implemented \nthroughout the project down to the \npower subsystem level and below.\n\n16 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for making assessments \nof the progress of planned technical ef?\nfort and progress toward requirements \nsatisfaction.\n\nAssessing progress is a process \nthat should be implemented \nthroughout the project by both \nthe NASA and contractor teams.\n\nNASA project management should \ncreate a project approach to \nassessing progress that includes \nparticipation from the contractor. \nTypically this would be the project \nreview plan. The contractor\u2019s inter-\nnal review process will have to be \nintegrated with the NASA approach. \nTechnical reviews need to be imple-\nmented throughout the project \ndown to the power subsystem level \nand below.\n\n17 The Center Directors or designees estab-\nlish and maintain a process to include \nactivities, requirements, guidelines, and \ndocumentation for making technical \ndecisions.\n\nClearly technical decisions are \nmade throughout the project \nboth by NASA and contractor per-\nsonnel. Certain types of decisions \nor decisions on certain topics may \nbest be made by either NASA or \nthe contractor depending upon \nthe Center\u2019s processes and the \ntype of project.\n\nFor this example, decisions affecting \nhigh-level requirements or mission \nsuccess would be made by NASA \nand those at the lower level, e.g., \nthe power subsystem that did not \naffect mission success would be \nmade by the contractor.\n\nTable 7.1?1 Applying the Technical Processes on Contract (continued)\n\ntion as clauses in full text form or as clauses incorpo-\nrated by reference. Many of these clauses relate to public \nlaws, contract administration, and financial manage-\nment. Newer clauses address information technology se-\ncurity, data rights, intellectual property, new technology \n\nreporting, and similar items. The contract specialist stays \nabreast of updates to the FAR and NFS. As the SOW and \nother parts of the solicitation mature, it is important for \nthe contract specialist and technical team to work closely \nto avoid duplication of similar requirements.\n\n\n\n7.1 Engineering with Contracts\n\nNASA Systems Engineering Handbook ? 223\n\n7.1.3.2 Develop the Statement of Work\nEffective surveillance of a contractor begins with the de-\nvelopment of the SOW. The technical team establishes \nthe SOW requirements for the product to be developed. \nThe SOW contains process, performance, and manage-\nment requirements the contractor must fulfill during \nproduct development.\n\nAs depicted in Figure 7.1-2, developing the SOW requires \nthe technical team to analyze the work, performance, \nand data needs to be accomplished by the contractor. \nThe process is iterative and supports the development of \nother documentation needed for the contracting effort. \nThe principal steps in the figure are discussed further in \nTable 7.1-2.\n\nAfter a few iterations, baseline the SOW requirements \nand place them under configuration management. (See \nSection 6.5.)\n\nUse the SOW checklist, which is in Appendix P, to help \nensure that the SOW is complete, consistent, correct, un-\nambiguous, and verifiable. Below are some key items to \nrequire in the SOW:\n\nTechnical and management deliverables having the  ?\nhighest risk potential (e.g., the SEMP, development \nand transition plans); requirements and architecture \nspecifications; test plans, procedures and reports; \nmetrics reports; delivery, installation, and mainte-\nnance documentation.\nContractual or scheduling incentives in a contract  ?\nshould not be tied to the technical milestone reviews. \nThese milestone reviews (for example, SRR, PDR, CDR, \netc.) enable a critical and valuable technical assessment \nto be performed. These reviews have specific entrance \ncriteria that should not be waived. The reviews should \nbe conducted when these criteria are met, rather than \nbeing driven by a particular schedule.\nTimely electronic access to data, work products, and  ?\ninterim deliverables to assess contractor progress on \nfinal deliverables.\nProvision(s) to flow down requirements to subcon- ?\ntractors and other team members.\nContent and format requirements of deliverables in  ?\nthe contract data requirements list. These require-\nments are specified in a data requirements document \n\nOrganize \nSOW\n\nDe?ne \nPerformance \n\nStandards\n\nDe?ne \nScope\n\nStep 1:  Analyze \nWork\n\nStep 2:  Analyze \nPerformance\n\nStep 3:  Analyze \nData\n\nWrite SOW\nRequirements \n\nDocument \nRationale\n\nDe?ne \nDeliverables\n\nFront-End\nAnalysis Baseline\n\nSOW\nRequirements \n\nIdentify \nStandards\n\n? De?ne Product\n? Specify Standards\n? Prepare Quality Assurance\n\nSurveillance Plan\n? De?ne Incentives\n\nNEED\n\nOther Related\nDocumentation \n\n?\n\nFigure 7.1?2 Contract requirements development process\n\n\n\n224 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nor data item description, usually as an attachment. \nRemember that you need to be able to edit data de-\nliverables.\nMetrics to gain visibility into technical progress for  ?\neach discipline (e.g., hardware, software, thermal, \noptics, electrical, mechanical). For guidance on met-\nrics to assess the contractor\u2019s performance and to as-\nsess adherence to product requirements on delivered \nproducts, refer to System and Software Metrics for Per-\nformance-Based Contracting.\nQuality incentives (defect, error count, etc.) to re- ?\nduce risk of poor quality deliverables. Be careful be-\ncause incentives can affect contractor behavior. For \nexample, if you reward early detection and correction \nof software defects, the contractor may expend effort \ncorrecting minor defects and saving major defects for \nlater.\n\nA continuous management program to include a pe- ?\nriodically updated risk list, joint risk reviews, and \nvendor risk approach.\nSurveillance activities (e.g., status meetings, reviews,  ?\naudits, site visits) to monitor progress and produc-\ntion, especially access to subcontractors and other \nteam members.\nSpecialty engineering (e.g., reliability, quality assur- ?\nance, cryogenics, pyrotechnics, biomedical, waste \nmanagement) that is needed to fulfill standards and \nverification requirements.\nProvisions to assign responsibilities between NASA  ?\nand contractor according to verification, validation, \nor similar plans that are not available prior to award.\nProvisions to cause a contractor to disclose changing  ?\na critical process. If a process is critical to human \nsafety, require the contractor to obtain approval from \n\nTable 7.1?2 Steps in the Requirements Development Process \n\nStep Task Detail\n\nStep 1:  \nAnalyze the \nWork\n\nDefine scope Document in the SOW that part of the project\u2019s scope that will be contracted. \nGive sufficient background information to orient offerors.\n\nOrganize SOW Organize the work by products and associated activities (i.e., product WBS).\n\nWrite SOW requirements Include activities necessary to:\n\nDevelop products defined in the requirements specification; and ?\n\nSupport, manage, and oversee development of the products. ?\n\nWrite SOW requirements in the form \u201cthe Contractor shall.\u201d \n\nWrite product requirements in the form \u201cthe system shall.\u201d\n\nDocument rationale Document separately from the SOW the reason(s) for including requirements \nthat may be unique, unusual, controversial, political, etc. The rationale is not \npart of the solicitation.\n\nStep 2:  \nAnalyze \nPerformance\n\nDefine performance \nstandards\n\nDefine what constitutes acceptable performance by the contractor. Common \nmetrics for use in performance standards include cost and schedule. For \nguidance on metrics to assess the contractor\u2019s performance and to assess \nadherence to product requirements on delivered products, refer to System and \nSoftware Metrics for Performance-Based Contracting.\n\nStep 3:  \nAnalyze Data\n\nIdentify standards Identify standards (e.g., EIA, IEEE, ISO) that apply to deliverable work products \nincluding plans, reports, specifications, drawings, etc. Consensus standards \nand codes (e.g., National Electrical Code, National Fire Protection Association, \nAmerican Society of Mechanical Engineers) that apply to product development \nand workmanship are included in specifications.\n\nDefine deliverables Ensure each deliverable data item (e.g., technical data \u2014requirements specifica-\ntions, design documents; management data\u2014plans, metrics reports) has a \ncorresponding SOW requirement for its preparation. Ensure each product has a \ncorresponding SOW requirement for its delivery.\n\n\n\n7.1 Engineering with Contracts\n\nNASA Systems Engineering Handbook ? 225\n\nthe contracting officer before a different process is im-\nplemented.\n\nis simplified because the contract already establishes base-\nline requirements for execution. First-time users need to \nunderstand the scope of the contract and the degree to \nwhich delivery and reporting requirements, performance \nmetrics, incentives, and so forth are already covered. \nTask contracts offer quick access (days or weeks instead \nof months) to engineering services for studies, analyses, \ndesign, development, and testing and to support services \nfor configuration management, quality assurance, main-\ntenance, and operations. Once a task order is issued, the \ntechnical team performs engineering activities associated \nwith managing contract performance and completing a \ncontract (discussed later) as they apply to the task order.\n\n7.1.3.4 Surveillance Plan\nThe surveillance plan defines the monitoring of the con-\ntractor effort and is developed at the same time as the \nSOW. The technical team works with mission assurance \npersonnel, generally from the local Safety and Mission As-\nsurance (SMA) organization, to prepare the surveillance \nplan for the contracted effort. Sometimes mission assur-\nance is performed by technical experts on the project. \nIn either case, mission assurance personnel should be \nengaged from the start of the project. Prior to contract \naward, the surveillance plan is written at a general level \nto cover the Government\u2019s approach to perceived pro-\ngrammatic risk. After contract award, the surveillance \nplan describes in detail inspection, testing, and other \nquality-related surveillance activities that will be per-\nformed to ensure the integrity of contract deliverables, \ngiven the current perspective on programmatic risks. \n\nRecommended items to include in the surveillance plan \nfollow:\n\nReview key deliverables within the first 30 days to en- ?\nsure adequate startup of activities.\nConduct contractor/subcontractor site visits to mon- ?\nitor production or assess progress.\nEvaluate effectiveness of the contractor\u2019s systems en- ?\ngineering processes.\n\nDrafting the surveillance plan when the SOW is devel-\noped promotes the inclusion of key requirements in the \nSOW that enable activities in the surveillance plan. For \nexample, in order for the technical team to conduct site \nvisits to monitor production of a subcontractor, then the \nSOW must include a requirement that permits site visits, \ncombined with a requirement for the contractor to flow \ndown requirements that directly affect subcontractors.\n\nNote: If you neglect to require something in the SOW, \nit can be costly to add it later.\n\nThe contractors must supply a SEMP that specifies their \nsystems engineering approach for requirements devel-\nopment, technical solution definition, design realization, \nproduct evaluation, product transition, and technical \nplanning, control, assessment, and decision analysis. It \nis best to request a preliminary SEMP in the solicita-\ntion. The source evaluation board can use the SEMP to \nevaluate the offeror\u2019s understanding of the requirements, \nas well as the offeror\u2019s capability and capacity to deliver \nthe system. After contract award, the technical team can \neliminate any gaps between the project\u2019s SEMP and the \ncontractor\u2019s SEMP that could affect smooth execution of \nthe integrated set of common technical processes.\n\nOften a technical team has experience developing tech-\nnical requirements, but little or no experience devel-\noping SOW requirements. If you give the contractor a \ncomplex set of technical requirements, but neglect to in-\nclude sufficient performance measures and reporting re-\nquirements, you will have difficulty monitoring progress \nand determining product and process quality. Under-\nstanding performance measures and reporting require-\nments will enable you to ask for the appropriate data or \nreports that you intend to use.\n\nTraditionally, NASA contracts require contractors to sat-\nisfy requirements in NASA policy directives, NASA pro-\ncedural requirements, NASA standards, and similar doc-\numents. These documents are almost never written in \nlanguage that can be used directly in a contract. Too often, \nthese documents contain requirements that do not apply \nto contracts. So, before the technical team boldly goes \nwhere so many have gone before, it is a smart idea to un-\nderstand what the requirements mean and if they apply to \ncontracts. The requirements that apply to contracts need \nto be written in a way that is suitable for contracts.\n\n7.1.3.3 Task Order Contracts\nSometimes, the technical team can obtain engineering \nproducts and services through an existing task order con-\ntract. The technical team develops a task order SOW and \ninteracts with the contracting officer\u2019s technical represen-\ntative to issue a task order. Preparing the task order SOW \n\n\n\n226 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\n7.1.3.5 Writing Proposal Instructions and \nEvaluation Criteria\n\nOnce the technical team has written the SOW, the Gov-\nernment cost estimate, and the preliminary surveillance \nplan and updated the SEMP, the solicitation can be de-\nveloped. Authors of the solicitation must understand the \ninformation that will be needed to evaluate the proposals \nand write instructions to obtain specifically needed in-\nformation. In a typical source selection, the source se-\nlection board evaluates the offerors\u2019 understanding of \nthe requirements, management approach, and cost and \ntheir relevant experience and past performance. This in-\nformation is required in the business and technical pro-\nposals. (This section discusses only the technical pro-\nposal.) The solicitation also gives the evaluation criteria \nthat the source evaluation board will use. This section \ncorresponds one-for-one to the items requested in the \nproposal instructions section.\n\nState instructions clearly and correctly. The goal is to \nobtain enough information to have common grounds \nfor evaluation. The challenge becomes how much in-\nformation to give the offerors. If you are too prescrip-\ntive, the proposals may look too similar. Be careful not \nto level the playing field too much, otherwise discrim-\ninating among offerors will be difficult. Because the \ntechnical merits of a proposal compete with nontech-\nnical items of similar importance (e.g., cost), the tech-\nnical team must wisely choose discriminators to facili-\ntate the source selection.\n\nThe source evaluation board evaluates nontechnical \n(business) and technical items. Items may be evaluated \nby themselves, or in the context of other technical or \nnontechnical items. Table 7.1-3 shows technical items \nto request from offerors and the evaluation criteria with \nwhich they correlate.\n\nEvaluation Considerations\nThe following are important to consider when evaluating \nproposals:\n\nGive adequate weight to evaluating the capability of  ?\ndisciplines that could cause mission failure (e.g., hard-\nware, software, thermal, optics, electrical, mechanical).\nConduct a preaward site visit of production/test facili- ?\nties that are critical to mission success.\nDistinguish between \u201cpretenders\u201d (good proposal  ?\nwriters) and \u201ccontenders\u201d (good performing organi-\nzations). Pay special attention to how process descrip-\ntions match relevant experience and past performance. \nWhile good proposals can indicate good future perfor-\nmance, lesser quality proposals usually predict lesser \nquality future work products and deliverables.\nAssess the contractor\u2019s SEMP and other items sub- ?\nmitted with the proposal based on evaluation criteria \nthat include quality characteristics (e.g., complete, un-\nambiguous, consistent, verifiable, and traceable).\n\nThe cost estimate that the technical team performs as \npart of the Technical Planning Process supports evalu-\nation of the offerors\u2019 cost proposals, helping the source \nevaluation board determine the realism of the offerors\u2019 \ntechnical proposals. (See Section 6.1.) The source eval-\nuation board can determine \u201cwhether the estimated \nproposed cost elements are realistic for the work to be \nperformed; reflect a clear understanding of the require-\nments; and are consistent with the unique methods of \nperformance and materials described in the offeror\u2019s \ntechnical proposal.\u201d1 \n\n7.1.3.6 Selection of COTS Products\nWhen COTS products are given as part of the technical \nsolution in a proposal, it is imperative that the selection \nof a particular product be evaluated and documented \nby applying the Decision Analysis Process. Bypassing \nthis task or neglecting to document the evaluation suf-\n\n1FAR 15.404-1(d) (1).\n\nSource Evaluation Board\n\nOne or more members of the technical team serve as \nmembers of the source evaluation board. They partic-\nipate in the evaluation of proposals following appli-\ncable NASA and Center source selection procedures. \nBecause source selection is so important, the pro-\ncurement office works closely with the source evalua-\ntion board to ensure that the source selection process \nis properly executed. The source evaluation board de-\nvelops a source evaluation plan that describes the \nevaluation factors, and the method of evaluating the \nofferors\u2019 responses. Unlike decisions made by systems \nengineers early in a product life cycle, source selec-\ntion decisions must be carefully managed in accor-\ndance with regulations governing the fairness of the \nselection process.\n\n\n\n7.1 Engineering with Contracts\n\nNASA Systems Engineering Handbook ? 227\n\nficiently could lead to a situation where NASA cannot \nsupport its position in the event of a vendor protest. \n\n7.1.3.7 Acquisition-Unique Risks \n\nTable 7.1-4 identifies a few risks that are unique to ac-\nquisition along with ways to manage them from an en-\ngineering perspective. Bear in mind, legal and procure-\nment aspects of these risks are generally covered in \ncontract clauses.\n\nThere may also be other acquisition risks not listed in \nTable 7.1-4. All acquisition risks should be identified and \nhandled the same as other project risks using the Con-\ntinuous Risk Management (CRM) process. A project can \nalso choose to separate out acquisition risks as a risk-list \nsubset and handle them using the risk-based acquisition \nmanagement process if so desired.\n\nWhen the technical team completes the activities prior \nto contract award, they will have an updated SEMP, the \nGovernment cost estimate, an SOW, and a preliminary \nsurveillance plan. Once the contract is awarded, the \ntechnical team begins technical oversight.\n\n7.1.4 During Contract Performance\n\n7.1.4.1 Performing Technical Surveillance \nSurveillance of a contractor\u2019s activities and/or documen-\ntation is performed to demonstrate fiscal responsibility, \nensure crew safety and mission success, and determine \naward fees for extraordinary (or penalty fees for substan-\ndard) contract execution. Prior to or outside of a con-\ntract award, a less formal agreement may be made for the \nGovernment to be provided with information for a trade \nstudy or engineering evaluation. Upon contract award, \nit may become necessary to monitor the contractor\u2019s ad-\nherence to contractual requirements more formally. (For \na greater understanding of surveillance requirements, \nsee NPR 8735.2, Management of Government Quality As-\nsurance Functions for NASA Contracts.)\n\nUnder the authority of the contracting officer, the tech-\nnical team performs technical surveillance as established \nin the NASA SEMP. The technical team assesses tech-\nnical work productivity, evaluates product quality, and \nconducts technical reviews of the contractor. (Refer to \nthe Technical Assessment Process.) Some of the key ac-\ntivities are discussed below.\n\nTable 7.1?3 Proposal Evaluation Criteria\n\nItem Criteria\n\nPreliminary contractor SEMP. How well the plan can be implemented given the resources, processes, \nand controls stated. Look at completeness (how well it covers all \nSOW requirements), internal consistency, and consistency with other \nproposal items. The SEMP should cover all resources and disciplines \nneeded to meet product requirements, etc.\n\nProcess descriptions, including subcontractor\u2019s \n(or team member\u2019s) processes.\n\nEffectiveness of processes and compatibility of contractor and sub-\ncontractor processes (e.g., responsibilities, decisionmaking, problem \nresolution, reporting).\n\nArtifacts (documents) of relevant work \ncompleted. Such documentation depicts the \nprobable quality of work products an offeror \nwill provide on your contract. Artifacts provide \nevidence (or lack) of systems engineering \nprocess capability.\n\nCompleteness of artifacts, consistency among artifacts on a given proj-\nect, consistency of artifacts across projects, conformance to standards.\n\nEngineering methods and tools. Effectiveness of the methods and tools. \n\nProcess and product metrics. How well the offeror measures performance of its processes and quality \nof its products.\n\nSubcontract management plan (may be part of \ncontractor SEMP).\n\nEffectiveness of subcontract monitoring and control and integration/\nseparation of risk management and CM.\n\nPhase-in plan (may be part of contractor SEMP). How well the plan can be implemented given the existing workload of \nresources.\n\n\n\n228 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nDevelop NASA-Contractor Technical Relationship: ?  \nAt the contract kick-off meeting, set expectations for \ntechnical excellence throughout the execution of the \ncontract. Highlight the requirements in the contract \nSOW that are the most important. Discuss the quality \nof work and products to be delivered against the tech-\n\nnical requirements. Mutually agree on the format of \nthe technical reviews and how to resolve misunder-\nstandings, oversights, and errors.\nConduct Technical Interchange Meetings: ?  Start early \nin the contract period and meet periodically with the \ncontractor (and subcontractors) to confirm that the \n\nTable 7.1?4 Risks in Acquisition\n\nRisk Mitigation\n\nSupplier goes bankrupt prior to \ndelivery\n\nThe source selection process is the strongest weapon. Select a supplier with a proven \ntrack record, solid financial position, and stable workforce. As a last resort, the Govern-\nment may take possession of any materials, equipment, and facilities on the work site \nnecessary for completing the work in-house or via another contract.\n\nSupplier acquired by another \nsupplier with different policies\n\nDetermine differences between policies before and after the acquisition. If there is a \ncritical difference, then consult with the procurement and legal offices. Meet with the \nsupplier and determine if the original policy will be honored at no additional cost. If \nthe supplier balks, then follow the advice from legal.\n\nDeliverables include software to \nbe developed\n\nInclude an experienced software manager on the technical team. Monitor the \ncontractor\u2019s adherence to software development processes. Discuss software prog-\nress, issues, and quality at technical interchange meetings.\n\nDeliverables include COTS prod-\nucts (especially software)\n\nUnderstand the quality of the product:\n\nLook at test results. When test results show a lot of rework to correct defects, then  ?\nusers will probably find more defects.\n\nExamine problem reports. These show whether or not users are finding defects  ?\nafter release.\n\nEvaluate user documentation. ?\n\nLook at product support. ?\n\nProducts depend on results from \nmodels or simulations\n\nEstablish the credibility and uncertainty of results. Determine depth and breadth of \npractices used in verification and validation of the model or simulation. Understand \nthe quality of software upon which the model or simulation is built. For more infor-\nmation, refer to NASA-STD-(I)-7009, Standard for Models and Simulations.\n\nBudget changes prior to delivery \nof all products (and contract \nwas written without interim \ndeliverables)\n\nOptions include:\n\nRemove deliverables or services from the contract scope in order to obtain key  ?\nproducts.\n\nRelax the schedule in exchange for reduced cost. ?\n\nAccept deliverables \u201cas is.\u201d ?\n\nTo avoid this situation, include electronic access to data, work products, and interim \ndeliverables to assess contractor progress on final deliverables in the SOW.\n\nContractor is a specialty supplier \nwith no experience in a particular \nengineering discipline; for ex-\nample, the contractor produces \ncryogenic systems that use alarm \nmonitoring software from another \nsupplier, but the contractor does \nnot have software expertise\n\nMitigate risks of COTS product deliverables as discussed earlier. If the contract is for \ndelivery of a modified COTS product or custom product, then include provisions in \nthe SOW to cover the following:\n\nSupplier support (beyond product warranty) that includes subsupplier support ?\n\nVersion upgrade/replacement plans ?\n\nSurveillance of subsupplier ?\n\nIf the product is inexpensive, simply purchasing spares may be more cost effective \nthan adding surveillance requirements.\n\n\n\n7.1 Engineering with Contracts\n\nNASA Systems Engineering Handbook ? 229\n\ncontractor has a correct and complete understanding \nof the requirements and operational concepts. Estab-\nlish day-to-day NASA-contractor technical commu-\nnications.\nControl and Manage Requirements:  ? Almost in-\nevitably, new or evolving requirements will affect a \nproject. When changes become necessary, the tech-\nnical team needs to control and manage changes and \nadditions to requirements proposed by either NASA \nor the contractor. (See Section 6.2.) Communicate \nchanges to any project participants that the changes \nwill affect. Any changes in requirements that affect \ncontract cost, schedule, or performance must be con-\nveyed to the contractor through a formal contract \nchange. Consult the contracting officer\u2019s technical \nrepresentative.\nEvaluate Systems Engineering Processes:  ? Evaluate \nthe effectiveness of defined systems engineering pro-\ncesses. Conduct audits and reviews of the processes. \nIdentify process deficiencies and offer assistance with \nprocess improvement.\nEvaluate Work Products: ?  Evaluate interim plans, re-\nports, specifications, drawings, processes, procedures, \nand similar artifacts that are created during the sys-\ntems engineering effort.\nMonitor Contractor Performance Against Key Met- ?\nrics: Monitoring contractor performance extends be-\nyond programmatic metrics to process and product \nmetrics. (See Section 6.7 on technical performance \nmeasures.) These metrics depend on acceptable \nproduct quality. For example, \u201c50 percent of design \ndrawings completed\u201d is misleading if most of them \nhave defects (e.g., incorrect, incomplete, inconsis-\ntent). The amount of work to correct the drawings af-\nfects cost and schedule. It is useful to examine reports \nthat show the amount of contractor time invested in \nproduct inspection and review.\nConduct Technical Reviews:  ? Assess contractor prog-\nress and performance against requirements through \ntechnical reviews. (See Section 6.7.)\nVerify and Validate Products:  ? Verify and validate \nthe functionality and performance of products before \ndelivery and prior to integration with other system \nproducts. To ensure that a product is ready for system \nintegration or to enable further system development, \nperform verification and validation as early as prac-\ntical. (See Sections 5.3 and 5.4.)\n\n7.1.4.2 Evaluating Work Products\nWork products and deliverables share common attri-\nbutes that can be used to assess quality. Additionally, re-\nlationships among work products and deliverables can \nbe used to assess quality. Some key attributes that help \ndetermine quality of work products are listed below:\n\nSatisfies content and format requirements, ?\nUnderstandable, ?\nComplete, ?\nConsistent (internally and externally) including ter- ?\nminology (an item is called the same thing throughout \nthe documents, and\nTraceable. ?\n\nTable 7.1-5 shows some typical work products from the \ncontractor and key attributes with respect to other docu-\nments that can be used as evaluation criteria.\n\n7.1.4.3 Issues with Contract-Subcontract \nArrangements\n\nIn the ideal world, a contractor manages its subcontrac-\ntors, each subcontract contains all the right require-\nments, and resources are adequate. In the real world, the \ntechnical team deals with contractors and subcontractors \nthat are motivated by profit, (sub)contracts with missing \nor faulty requirements, and resources that are consumed \nmore quickly than expected. These and other factors \ncause or influence two key issues in subcontracting: \n\nLimited or no oversight of subcontractors and ?\nLimited access to or inability to obtain subcontractor  ?\ndata.\n\nThese issues are exacerbated when they apply to second- \n(or lower) tier subcontractors. Table 7.1-6 looks at these \nissues more closely along with potential resolutions.\n\nScenarios other than those above are possible. Resolu-\ntions might include reducing contract scope or deliv-\nerables in lieu of cost increases or sharing information \ntechnology in order to obtain data. Even with the ad-\nequate flowdown requirements in (sub)contracts, legal \nwrangling may be necessary to entice contractors to sat-\nisfy the conditions of their (sub)contracts.\n\nActivities during contract performance will generate \nan updated surveillance plan, minutes documenting \nmeetings, change requests, and contract change orders. \nProcesses will be assessed, deliverables and work \nproducts evaluated, and results reviewed. \n\n\n\n230 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\n7.1.5 Contract Completion \nThe contract comes to completion with the delivery of \nthe contracted products, services, or systems and their \nenabling products or systems. Along with the product, \nas-built documentation must be delivered and opera-\ntional instructions including user manuals.\n\n7.1.5.1 Acceptance of Final Deliverables\nThroughout the contract period, the technical team re-\nviews and accepts various work products and interim \ndeliverables identified in the contract data requirements \nlist and schedule of deliverables. The technical team also \nparticipates in milestone reviews to finalize acceptance \nof deliverables. At the end of the contract, the technical \nteam ensures that each technical deliverable is received \nand that its respective acceptance criteria are satisfied. \n\nThe technical team records the acceptance of deliver-\nables against the contract data requirements list and the \nschedule of deliverables. These documents serve as an \ninventory of items and services to be accepted. Although \nrejections and omissions are infrequent, the technical \nteam needs to take action in such a case. Good data \nmanagement and configuration management practices \nfacilitate the effort.\n\nAcceptance criteria include:\nProduct verification and validation completed success- ?\nfully. The technical team performs or oversees verifica-\ntion and validation of products, integration of products \ninto systems, and system verification and validation. \nTechnical data package is current (as-built) and com- ?\nplete.\n\nTable 7.1?5 Typical Work Product Documents\n\nWork Product Evaluation Criteria\n\nSEMP Describes activities and products required in the SOW.\n\nThe SEMP is not complete unless it describes (or references) how each activity and product \nin the SOW will be accomplished.\n\nSoftware management/ \ndevelopment plan\n\nConsistent with the SEMP and related project plans. \n\nDescribes how each software-related activity and product in the SOW will be accomplished.\n\nDevelopment approach is feasible.\n\nSystem design Covers the technical requirements and operational concepts. \n\nSystem can be implemented.\n\nSoftware design Covers the technical requirements and operational concepts. \n\nConsistent with hardware design. \n\nSystem can be implemented.\n\nInstallation plans Covers all user site installation activities required in the SOW. \n\nPresents a sound approach. \n\nShows consistency with the SEMP and related project plans.\n\nTest plans Covers qualification requirements in the SOW. \n\nCovers technical requirements. \n\nApproach is feasible.\n\nTest procedures Test cases are traceable to technical requirements.\n\nTransition plans Describes all transition activities required in the SOW. \n\nShows consistency with the SEMP and related project plans.\n\nUser documentation Sufficiently and accurately describes installation, operation, or maintenance (depending on \nthe document) for the target audience.\n\nDrawings and documents \n(general)\n\nComply with content and format requirements specified in the SOW.\n\n\n\n7.1 Engineering with Contracts\n\nNASA Systems Engineering Handbook ? 231\n\nTransfer of certifications, spare parts, warranties, etc.,  ?\nis complete.\nTransfer of software products, licenses, data rights, in- ?\ntellectual property rights, etc., is complete.\nTechnical documentation required in contract clauses  ?\nis complete (e.g., new technology reports).\n\nIt is important for NASA personnel and facilities to be \nready to receive final deliverables. Key items to have pre-\npared include:\n\nA plan for support and to transition products to op- ?\nerations;\n\nTraining of personnel; ?\nConfiguration management system in place; and ?\nAllocation of responsibilities for troubleshooting, re- ?\npair, and maintenance.\n\n7.1.5.2 Transition Management\nBefore the contract was awarded, a product support \nstrategy was developed as part of the acquisition strategy. \nThe product support strategy outlines preliminary no-\ntions regarding integration, operations, maintenance, \nimprovements, decommissioning, and disposal. Later, \nafter the contract is awarded, a high-level transition plan \n\nTable 7.1?6 Contract?Subcontract Issues\n\nIssue Resolution\n\nOversight of subcontractor is \nlimited because requirement(s) \nmissing from contract\n\nThe technical team gives the SOW requirement(s) to the contracting officer who adds \nthe requirement(s) to the contract and negotiates the change order, including additional \ncosts to NASA. The contractor then adds the requirement(s) to the subcontract and \nnegotiates the change order with the subcontractor. If the technical team explicitly \nwants to perform oversight, then the SOW should indicate what the contractor, its \nsubcontractors, and team members are required to do and provide.\n\nOversight of subcontractor is \nlimited because requirement(s) \nnot flowed down from contrac-\ntor to subcontractor\n\nIt is the contractor\u2019s responsibility to satisfy the requirements of the contract. If the \ncontract includes provisions to flow down requirements to subcontractors, then the \ntechnical team can request the contracting officer to direct the contractor to execute the \nprovisions. The contractor may need to add requirements and negotiate cost changes \nwith the subcontractor. If NASA has a cost-plus contract, then expect the contractor to \nbill NASA for any additional costs incurred. If NASA has a fixed-price contract, then the \ncontractor will absorb the additional costs or renegotiate cost changes with NASA.\n\nIf the contract does not explicitly include requirements flowdown provisions, the \ncontractor is responsible for performing oversight.\n\nOversight of second-tier sub-\ncontractor is limited because \nrequirement(s) not flowed \ndown from subcontractor to \nsecond-tier subcontractor\n\nThis is similar to the previous case, but more complicated. Assume that the contractor \nflowed down requirements to its subcontractor, but the subcontractor did not flow \ndown requirements to the second-tier subcontractor. If the subcontract includes provi-\nsions to flow down requirements to lower tier subcontractors, then the technical team \ncan request the contracting officer to direct the contractor to ensure that subcontractors \nexecute the flowdown provisions to their subcontractors.\n\nIf the subcontract does not explicitly include requirements flowdown provisions, the \nsubcontractor is responsible for performing oversight of lower tier subcontractors.\n\nAccess to subcontractor data \nis limited or not provided \nbecause providing the data is \nnot required in the contract\n\nThe technical team gives the SOW requirement(s) to the contracting officer who adds \nthe requirement(s) to the contract and negotiates the change order, including additional \ncosts to NASA. The contractor then adds the requirement(s) to the subcontract and ne-\ngotiates the change order with the subcontractor. If the technical team explicitly wants \ndirect access to subcontractor data, then the SOW should indicate what the contractor, \nits subcontractors, and team members are required to do and provide.\n\nAccess to subcontractor data \nis limited or not provided \nbecause providing the data is \nnot required in the subcontract\n\nIt is the contractor\u2019s responsibility to obtain data (and data rights) necessary to satisfy \nthe conditions of its contract, including data from subcontractors. If the technical \nteam needs direct access to subcontractor data, then follow the previous case to add \nflowdown provisions to the contract so that the contractor will add requirements to the \nsubcontract.\n\n\n\n232 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nthat expands the product support strategy is recorded in \nthe SEMP. Details of product/system transition are sub-\nsequently documented in one or more transition plans. \nElements of transition planning are discussed in Sec-\ntion 5.5.\n\nTransition plans must clearly indicate responsibility for \neach action (NASA or contractor). Also, the contract \nSOW must have included a requirement that the con-\ntractor will execute responsibilities assigned in the tran-\nsition plan (usually on a cost-reimbursable basis).\n\nFrequently, NASA (or NASA jointly with a prime con-\ntractor) is the system integrator on a project. In this \nsituation, multiple contractors (or subcontractors) will \nexecute their respective transition plans. NASA is re-\nsponsible for developing and managing a system inte-\ngration plan that incorporates inputs from each transi-\ntion plan. The provisions that were written in the SOW \nmonths or years earlier accommodate the transfer of \nproducts and systems from the contractors to NASA.\n\n7.1.5.3 Transition to Operations and Support\nThe successful transition of systems to operations and \nsupport, which includes maintenance and improve-\nments, depends on clear transition criteria that the \nstakeholders agree on. The technical team participates \nin the transition, providing continuity for the customer, \nespecially when a follow-on contract is involved. When \nthe existing contract is used, the technical team conducts \na formal transition meeting with the contractor. Alter-\nnatively, the transition may involve the same contractor \nunder a different contract arrangement (e.g., modified or \nnew contract). Or the transition may involve a different \ncontractor than the developer, using a different contract \narrangement.\n\nThe key benefits of using the existing contract are that \nthe relevant stakeholders are familiar with the con-\ntractor and that the contractor knows the products and \nsystems involved. Ensure that the contractor and other \nkey stakeholders understand the service provisions (re-\nquirements) of the contract. This meeting may lead to \ncontract modifications in order to amend or remove ser-\nvice requirements that have been affected by contract \nchanges over the years.\n\nSeeking to retain the development contractor under a \ndifferent contract can be beneficial. Although it takes \ntime and resources to compete the contract, it permits \n\nNASA to evaluate the contractor and other offerors \nagainst operations and support requirements only. The \nincumbent contractor has personnel with development \nknowledge of the products and systems, while service \nproviders specialize in optimizing cost and availability \nof services. In the end, the incumbent may be retained \nunder a contract that focuses on current needs (not sev-\neral years ago), or else a motivated service provider will \nwork hard to understand how to operate and maintain \nthe systems. If a follow-on contract will be used, consult \nthe local procurement office and exercise the steps that \nwere used to obtain the development contract. Assume \nthat the amount of calendar time to award a follow-on \ncontract will be comparable to the time to award the de-\nvelopment contract. Also consider that the incumbent \nmay be less motivated upon losing the competition.\n\nSome items to consider for follow-on contracts during \nthe development of SOW requirements include:\n\nStaff qualifications; ?\nOperation schedules, shifts, and staffing levels; ?\nMaintenance profile (e.g., preventive, predictive, run- ?\nto-fail);\nMaintenance and improvement opportunities (e.g.,  ?\nschedule, turnaround time);\nHistorical data for similar efforts; and ?\nPerformance-based work. ?\n\nThe transition to operations and support represents a \nshift from the delivery of products to the delivery of ser-\nvices.\n\nService contracts focus on the contractor\u2019s performance \nof activities, rather than development of tangible prod-\nucts. Consequently, performance standards reflect cus-\ntomer satisfaction and service efficiency, such as:\n\nCustomer satisfaction ratings; ?\nEfficiency of service; ?\nResponse time to a customer request; ?\nAvailability (e.g., of system, Web site, facility); ?\nTime to perform maintenance action; ?\nPlanned versus actual staffing levels; ?\nPlanned versus actual cost; ?\nEffort and cost per individual service action; and ?\nPercent decrease in effort and cost per individual ser- ?\nvice action.\n\nFor more examples of standards to assess the contrac-\ntor\u2019s performance, refer to System and Software Metrics \nfor Performance-Based Contracting.\n\n\n\n7.1 Engineering with Contracts\n\nNASA Systems Engineering Handbook ? 233\n\n7.1.5.4 Decommissioning and Disposal\nContracts offer a means to achieve the safe and efficient \ndecommissioning and disposal of systems and products \nthat require specialized support systems, facilities, and \ntrained personnel, especially when hazardous materials \nare involved. Consider these needs during development \nof the acquisition strategy and solidify them before the \nfinal design phase. Determine how many contracts will \nbe needed across the product\u2019s life cycle.\n\nSome items to consider for decommissioning and dis-\nposal during the development of SOW requirements:\n\nHandling and disposal of waste generated during the  ?\nfabrication and assembly of the product.\nReuse and recycling of materials to minimize the dis- ?\nposal and transformation of materials.\nHandling and disposal of materials used in the prod- ?\nuct\u2019s operations.\nEnd-of-life decommissioning and disposal of the  ?\nproduct.\nCost and schedule to decommission and dispose of  ?\nthe product, waste, and unwanted materials.\nMetrics to measure decommissioning and disposal of  ?\nthe product. \n\nMetrics to assess the contractor\u2019s performance. (Refer  ?\nto System and Software Metrics for Performance-Based \nContracting.)\n\nFor guidelines regarding disposal, refer to the Systems \nEngineering Handbook: A \u201cWhat To\u201d Guide for all SE \nPractitioners.\n\n7.1.5.5 Final Evaluation of Contractor \nPerformance\n\nIn preparation for closing out a contract, the technical \nteam gives input to the procurement office regarding the \ncontractor\u2019s final performance evaluation. Although the \ntechnical team has performed periodic contractor per-\nformance evaluations, the final evaluation offers a means \nto document good and bad performance that continued \nthroughout the contract. Since the evaluation is retained \nin a database, it can be used as relevant experience and \npast performance input during a future source selection \nprocess.\n\nThis phase of oversight is complete with the closeout \nor modification of the existing contract, award of the \nfollow-on contract, and an operational system. Over-\nsight continues with follow-contract activities.\n\n\n\n234 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\n7.2.1 Introduction \nConcurrent Engineering (CE) and integrated design is a \nsystematic approach to integrated product development \nthat emphasizes response to stakeholder expectations \nand embodies team values of cooperation, trust, and \nsharing. The objective of CE is to reduce the product de-\nvelopment cycle time through a better integration of ac-\ntivities and processes. Parallelism is the prime concept in \nreducing design lead time and concurrent engineering \nbecomes the central focus. Large intervals of parallel \nwork on different parts of the design are synchronized \nby comparatively brief exchanges between teams to pro-\nduce consensus and decisions.1 CE has become a widely \naccepted concept and is regarded as an excellent alterna-\ntive approach to a sequential engineering process.\n\nThis section addresses the specific application of CE and \nintegrated design practiced at NASA in Capability for \nAccelerated Concurrent Engineering (CACE) environ-\nments. CACE is comprised of four essential components: \npeople, process, tools, and facility. The CACE environ-\nment typically involves the collocation of an in-place \nleadership team and core multidisciplinary engineering \nteam working with a stakeholder team using well defined \nprocesses in a dedicated collaborative, concurrent engi-\nneering facility with specialized tools. The engineering \nand collaboration tools are connected by the facility\u2019s in-\ntegrated infrastructure. The teams work synchronously \nfor a short period of time in a technologically intensive \nphysical environment to complete an instrument or mis-\nsion design. CACE is most often used to design space \ninstruments and payloads or missions including orbital \nconfiguration; hardware such as spacecraft, landers, \nrovers, probes, or launchers; data and ground communi-\ncation systems; other ground systems; and mission op-\nerations. But the CACE process applies beyond strict in-\nstrument and/or mission conceptual design.\n\nMost NASA centers have a CACE facility. NASA CACE \nis built upon a people/process/tools/facility paradigm \nthat enables the accelerated production of high-quality \nengineering design concepts in a concurrent, collabora-\ntive, rapid design environment. (See Figure 7.2-1.)\n\n1From Miao and Haake \u201cSupporting Concurrent Design \nby Integrating Information Sharing and Activity Synchroni-\nzation.\u201d\n\nAlthough CACE at NASA is based on a common phi-\nlosophy and characteristics, specific CACE implementa-\ntion varies in many areas. These variations include level \nof engineering detail, information infrastructure, knowl-\nedge base, areas of expertise, engineering staffing ap-\nproach, administrative and engineering tools, type of fa-\ncilitation, roles and responsibilities within CACE team, \nroles and responsibilities across CACE and stakeholder \nteams, activity execution approach, and duration of ses-\nsion. While primarily used to support early life-cycle \nphases such as pre-Formulation and Formulation, the \nCACE process has demonstrated applicability across the \nfull project life cycle.\n\n7.2.2 CACE Overview and Importance\nCACE design techniques can be an especially effective \nand efficient method of generating a rapid articulation of \nconcepts, architectures, and requirements. \n\nThe CACE approach provides an infrastructure for brain-\nstorming and bouncing ideas between the engineers and \nstakeholder team representatives, which routinely results \nin a high-quality product that directly maps to the cus-\ntomer needs. The collaboration design paradigm is so \nsuccessful because it enables a radical reduction in de-\ncision latency. In a non-CACE environment, questions, \nissues, or problems may take several days to resolve. If \na design needs to be changed or a requirement reeval-\nuated, significant time may pass before all engineering \nteam members get the information or stakeholder team \nmembers can discuss potential requirement changes. \nThese delays introduce the possibility, following initial \nevaluation, of another round of questions, issues, and \n\n7.2 Integrated Design Facilities\n\nFigure 7.2?1 CACE people/process/tools/facility \nparadigm\n\n\n\n7.2 Integrated Design Facilities\n\nNASA Systems Engineering Handbook ? 235\n\nchanges to design and requirements, adding further de-\nlays. \n\nThe tools, data, and supporting information technology \ninfrastructure within CACE provide an integrated sup-\nport environment that can be immediately utilized by \nthe team. The necessary skills and experience are gath-\nered and are resident in the environment to synchro-\nnously complete the design. In a collaborative environ-\nment, questions can be answered immediately, or key \nparticipants can explore assumptions and alternatives \nwith the stakeholder team or other design team mem-\nbers and quickly reorient the whole team when a design \nchange occurs. The collaboration triggers the creativity \nof the engineers and helps them close the loop and rap-\nidly converge on their ideas. Since the mid-1990s, the \nCACE approach has been successfully used at several \nNASA Centers as well as at commercial enterprises to \ndramatically reduce design development time and costs \nwhen compared to traditional methods. \n\nCACE stakeholders include NASA programs and proj-\nects, scientists, and technologists as well as other Govern-\nment agencies (civil and military), Federal laboratories, \nand universities. CACE products and services include:\n\nGenerating mission concepts in support of Center  ?\nproposals to science AO;\nFull end-to-end designs including system/subsystem  ?\nconcepts, requirements, and tradeoffs;\nFocused efforts assessing specific architecture sub-  ?\nelements and tradeoffs;\nIndependent assessments of customer-provided re- ?\nports, concepts, and costs;\nRoadmapping support; and ?\nTechnology and risk assessments. ?\n\nAs integrated design has become more accepted, col-\nlaborative engineering design efforts expanded from the \nparticipation of one or more Centers in a locally executed \nactivity; to geographically distributed efforts across a few \nNASA Centers with limited scope and participation; to \ntrue OneNASA efforts with participation from many \nNASA integrated design teams addressing broad, com-\nplex architectures. \n\nThe use of geographically distributed CACE teams is a \npowerful engineering methodology to achieve lower risk \nand more creative solutions by factoring in the best skills \nand capabilities across the Agency. Using a geographically \n\ndistributed process must build upon common CACE el-\nements while considering local CACE facility differences \nand the differences in the local Center cultures.\n\n7.2.3 CACE Purpose and Benefits\nThe driving forces behind the creation of NASA\u2019s early \nCACE environments were increased systems engineer-\ning efficiency and effectiveness. More specifically, the \nearly CACE environments addressed the need for:\n\nGenerating more conceptual design studies at re- ?\nduced cost and schedule, \nCreating a reusable process within dedicated facilities  ?\nusing well-defined tools,\nDeveloping a database of mission requirements and  ?\ndesigns for future use,\nDeveloping mission generalists from a pool of experi- ?\nenced discipline engineers, and\nInfusing a broader systems engineering perspective  ?\nacross the organization.\n\nAdditional resulting strategic benefits across NASA in-\ncluded:\n\nCore competency support (e.g., developing systems  ?\nengineers, maturing and broadening of discipline en-\ngineers, training environment, etc.);\nSensitizing the customer base to end-to-end issues  ?\nand implications of requirements upon design;\nTest-bed environment for improved tools and pro- ?\ncesses;\nEnvironment for forming partnerships; ?\nTechnology development and roadmapping support; ?\nImproved quality and consistency of conceptual de- ?\nsigns; and\nOneNASA environment that enables cooperative rather  ?\nthan competitive efforts among NASA organizations.\n\n7.2.4 CACE Staffing\nA management or leadership team, a multidisciplinary \nengineering team, a stakeholder team, and a facility sup-\nport team are all vital elements in achieving a successful \nCACE activity.\n\nA CACE team consists of a cadre of engineers, each rep-\nresenting a different discipline or specialty engineering \narea, along with a lead systems engineer and a team lead \nor facilitator. As required, the core engineering team is \nsupplemented with specialty and/or nonstandard engi-\n\n\n\n236 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nneering skills to meet unique stakeholder needs. These \nsupplementary engineering capabilities can be obtained \neither from the local Center or from an external source. \nThe team lead coordinates and facilitates the CACE ac-\ntivity and interacts with the stakeholders to ensure that \ntheir objectives are adequately captured and represented. \nEngineers are equipped with techniques and software \nused in their area of expertise and interact with the team \nlead, other engineers, and the stakeholder team to study \nthe feasibility of a proposed solution and produce a de-\nsign for their specific subsystem. \n\nA CACE operations manager serves as the Center advo-\ncate and manager, maintaining an operational capability, \nproviding initial coordination with potential customers \nthrough final delivery of CACE product, and infusing \ncontinuous process and product improvement as well as \nevolutionary growth into the CACE environment to en-\nsure its continued relevance to the customer base.\n\nA CACE facility support team maintains and develops the \ninformation infrastructure to support CACE activities.\n\n7.2.5 CACE Process\nThe CACE process starts with a customer requesting \nengineering support from CACE management. CACE \nmanagement establishes that the customer\u2019s request is \nwithin the scope of the team capabilities and availability \nand puts together a multidisciplinary engineering team \nunder the leadership of a team lead and lead systems en-\ngineer collaborating closely with the customer team. The \nfollowing subsections briefly describe the three major \nCACE activity phases: (1) planning and preparation, \n(2) execution, and (3) wrap-up. \n\n7.2.5.1 Planning and Preparation\nOnce a customer request is approved and team lead \nchosen, a planning meeting is scheduled. The key ex-\nperts attending the planning meeting may include the \nCACE manager, a team lead, and a systems engineer \nas well as key representatives from the customer/stake-\nholder team. Interactions with the customer/stakeholder \nteam and their active participation in the process are in-\ntegral to the successful planning, preparation, and exe-\ncution of a concurrent design session. Aspects addressed \ninclude establishing the activity scope, schedule, and \ncosts; a general agreement on the type of product to be \nprovided; and the success criteria and metrics. Agree-\nments reached at the planning meeting are documented \nand distributed for review and comment. \n\nProducts from the planning and preparation phase in-\nclude the identification of activities required by the cus-\ntomer/stakeholder team, the CACE team, or a com-\nbination of both teams, as well as the definition of the \nobjectives, the requirements, the deliverables, the es-\ntimated budget, and the proposed schedule. Under \nsome conditions, followup coordination meetings are \nscheduled that include the CACE team lead, the sys-\ntems engineer(s), a subset of the remaining team mem-\nbers, and customer/stakeholder representatives, as ap-\npropriate. The makeup of participants is usually based \non the elements that have been identified as the activity \ndrivers and any work identified that needs to be done be-\nfore the actual design activity begins. \n\nDuring the planning and preparation process, the stake-\nholder-provided data and the objectives and activity \nplan are reviewed, and the scope of the activity is final-\nized. A discussion is held of what activities need to be \ndone by each of the stakeholders and the design teams. \nFor example, for planning a mission design study, the \ncustomer identifies the mission objectives by defining \nthe measurement objectives and the instrument speci-\nfications, as applicable, and identifying the top-level re-\nquirements. A subset of the CACE engineering team \nmay perform some preliminary work before the actual \nstudy (e.g., launch vehicle performance trajectory anal-\nysis; thrust and navigation requirements; the entry, de-\nscent, and landing profile; optical analysis; mechanical \ndesign; etc.) as identified in the planning meetings to \nfurther accelerate the concurrent engineering process in \nthe study execution phase. The level of analysis in this \nphase is a function of many things, including the level of \nmaturity of the incoming design, the stated goals and ob-\njectives of the engineering activity, engineer availability, \nand CACE scheduling. \n\n7.2.5.2 Activity Execution Phase\nA typical activity or study begins with the customer pre-\nsentation of the overall mission concept and instrument \nconcepts, as applicable, to the entire team. Additional \ninformation provided by the customer/stakeholders in-\ncludes the team objectives, the science and technology \ngoals, the initial requirements for payload, spacecraft \nand mission design, the task breakdown between pro-\nviders of parts or functions, top challenges and concerns, \nand the approximate mission timeline. This information \nis often provided electronically in a format accessible to \nthe engineering team and is presented by the customer/\n\n\n\n7.2 Integrated Design Facilities\n\nNASA Systems Engineering Handbook ? 237\n\nstakeholder representatives at a high level. During this \npresentation, each of the subsystems engineers focuses \non the part of the overall design that is relevant to their \nsubsystem. The systems engineer puts the high-level \nsystem requirements into the systems spreadsheets and/\nor a database that is used throughout the process to track \nengineering changes. These data sources can be projected \non the displays to keep the team members synchronized \nand the customer/stakeholders aware of the latest devel-\nopments.\n\nThe engineering analysis is performed iteratively with the \nCACE team lead and systems engineer playing key roles \nto lead the process. Thus, issues are quickly identified, so \nconsensus on tradeoff decisions and requirements redef-\ninition can be achieved while maintaining momentum. \nThe customer team actively participates in the collabora-\ntive process (e.g., trade studies, requirements relaxation, \nclarifying priorities), contributing to the rapid develop-\nment of an acceptable product. \n\nOften, there are breakout sessions, or sidebars, in which \npart of the team discusses a particular tradeoff study. \nEach subsystem has a set of key parameters that are used \nfor describing its design. Because of the dependencies \namong the various subsystems, each discipline engineer \nneeds to know the value of certain parameters related to \nother subsystems. These parameters are shared via the \nCACE information infrastructure. Often, there are con-\nflicting or competing objectives for various subsystems. \nMany tradeoff studies, typically defined and led by the \nteam systems engineer, are conducted among subsystem \nexperts immediately as issues occur. Most of the com-\nmunication among team members is face to face or live \nvia video or teleconference. Additional subject matter \nexperts are consulted as required. In the CACE environ-\nment, subsystems that need to interact extensively are \nclustered in close proximity to facilitate the communica-\ntion process among the experts.\n\nThe team iterates on the requirements, and each sub-\nsystem expert refines or modifies design choices as \nschedule allows. This process continues until an accept-\nable solution is obtained. There may be occasions where \nit is not possible to iterate to an acceptable solution prior \nto the scheduled end of the activity. In those cases, the \navailable iterated results are documented and form the \nbasis of the delivered product. \n\nIn each iteration, activities such as the following take \nplace, sometimes sequentially and other times in par-\n\nallel. The subsystem experts of science, instruments, \nmission design, and ground systems collaboratively de-\nfine the science data strategy for the mission in question. \nThe telecommunications, ground systems, and com-\nmand and data-handling experts develop the data-return \nstrategy. The attitude control systems, power, propulsion, \nthermal, and structure experts iterate on the spacecraft \ndesign and the configuration expert prepares the initial \nconcept. The systems engineer interacts with all discipline \nengineers to ensure that the various subsystem designs fit \ninto the intended system architecture. Each subsystem ex-\npert provides design and cost information, and the cost \nexpert estimates the total cost for the mission.\n\nWhile design activity typically takes only days or weeks \nwith final products available within weeks after study \ncompletion, longer term efforts take advantage of the \nconcurrent, collaborative environment to perform more \ndetailed analyses than those performed in the shorter \nduration CACE exercises. \n\n7.2.5.3 Activity Wrap-Up \nAfter the completion of a CACE study, the product is de-\nlivered to the customer. In some CACE environments, \nthe wrap-up of the product is completed with minimal \nadditional resources: the engineers respond to cus-\ntomer/stakeholder feedback by incorporating additional \nrefinements or information emphasizing basic clean-\nup. In other CACE environments, significant time is ex-\npended to format the final report and review it with the \ncustomer/stakeholders to ensure that their expectations \nhave been addressed adequately. \n\nSome CACE environments have standardized their \nwrap-up activities to address the customer/stakeholder \nfeedback and develop products that are structured and \nuniform across different ranges of efforts. \n\nAs part of activity followup, customer/stakeholder feed-\nback is requested on processes, whether the product \nmet their needs, and whether there are any suggested \nimprovements. This feedback is factored back into the \nCACE environment as part of a continuous improve-\nment process.\n\n7.2.6 CACE Engineering Tools and \nTechniques \n\nEngineering tools and techniques vary within and across \nCACE environments in several technical aspects (e.g., \nlevel of fidelity, level of integration, generally available \n\n\n\n238 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\ncommercial applications versus custom tools versus cus-\ntomized knowledge-based Excel spreadsheets, degree of \nparametric design and/or engineering analysis). For ex-\nample, mechanical design tools range from white-board \ndiscussions to note pad translations to computer-aided \ndesign to 3D rapid design prototyping.\n\nImportant factors in determining which tools are appro-\npriate to an activity include the purpose and duration of \nthe activity, the engineers\u2019 familiarity or preference, the \nexpected product, the local culture, and the evolution of \nthe engineering environment. Factors to be considered \nin the selection of CACE tools and engineering tech-\nniques should also include flexibility, compatibility with \nthe CACE environment and process, and value and ease \nof use for the customer after the CACE activities.\n\nEngineering tools may be integrated into the CACE in-\nfrastructure, routinely provided by the supporting en-\ngineering staff, and/or utilized only on an activity-by-\nactivity basis, as appropriate. As required, auxiliary \nengineering analysis outside of the scope of the CACE \neffort can be performed external to the CACE environ-\nment and imported for reference and incorporation into \nthe CACE product.\n\n7.2.7 CACE Facility, Information \nInfrastructure, and Staffing \n\nEach CACE instantiation is unique to the Center, pro-\ngram, or project that it services. While the actual im-\nplementations vary, the basic character does not. Each \nimplementation concentrates on enabling engineers, \ndesigners, team leads, and customer/stakeholders to be \nmore productive during concurrent activities and com-\nmunication. This subsection focuses on three aspects of \nthis environment: the facility, the supporting informa-\ntion infrastructure, and the staff required to keep the fa-\ncility operational.\n\n7.2.7.1 Facility\nThe nature of communication among discipline special-\nists working together simultaneously creates a somewhat \nchaotic environment. Although it is the duty of the team \nlead to maintain order in the environment, the facility it-\nself has to be designed to allow the participants to main-\ntain order and remain on task while seeking to increase \ncommunication and collaboration. To do this effectively \nrequires a significant investment in infrastructure re-\nsources. \n\nThe room needs sufficient space to hold active partici-\npants from the disciplines required, customer/stake-\nholder representatives, and observers. CACE managers \nencourage observers to show potential future CACE \nusers the value of active CACE sessions. \n\nIt is also important to note that the room will get recon-\nfigured often. Processes and requirements change, and \nthe CACE facility must change with that. The facility \ncould appear to an onlooker as a work in progress. Ta-\nbles, chairs, computer workstations, network connec-\ntions, electrical supplies, and visualization systems will \ncontinually be assessed for upgrades, modification, or \nelimination.\n\nCACE requirements in the area of visualization are \nunique. When one subject matter expert wants to com-\nmunicate to either a group of other discipline specialists \nor to the whole group in general, the projection system \nneeds to be able to switch to different engineering work-\nstations. When more than one subject matter expert \nwants to communicate with different groups, multiple \nprojection systems need to be able to switch. This can \ntypically require three to six projection systems with \nswitching capability from any specific workstation to \nany specific projector. In addition, multiple projection \nsystems switchable to the engineering workstations need \nto be mounted so that they can be viewed without im-\npacting other activities in the room or so that the entire \ngroup can be refocused as required during the session. \nThe ease of this reconfiguration is one measure of the ef-\nficacy of the environment.\n\n7.2.7.2 Information Infrastructure\nA CACE system not only requires a significant invest-\nment in the facility but relies heavily on the information \ninfrastructure. Information infrastructure requirements \ncan be broken down into three sections: hardware, soft-\nware, and network infrastructure.\n\nThe hardware portion of the information infrastructure \nused in the CACE facility is the most transient element \nin the system. The computational resources, the commu-\nnication fabric, servers, storage media, and the visualiza-\ntion capabilities benefit from rapid advances in tech-\nnology. A CACE facility must be able to take advantage \nof the economy produced by those advances and must \nalso be flexible enough to take advantage of the new ca-\npabilities. \n\n\n\n7.2 Integrated Design Facilities\n\nNASA Systems Engineering Handbook ? 239\n\nOne of the major costs of a CACE infrastructure is \nsoftware. Much of the software currently used by engi-\nneering processes is modeling and simulation, usually \nproduced by commercial software vendors. Infrastruc-\nture software to support exchange of engineering data; \nto manage the study archive; and to track, administer, \nand manage facility activities is integral to CACE suc-\ncess. One of the functions of the CACE manager is to de-\ntermine how software costs can be paid, along with what \nsoftware should be the responsibility of the participants \nand customers.\n\nThe network infrastructure of a CACE facility is critical. \nInformation flowing among workstations, file servers, \nand visualization systems in real time requires a signifi-\ncant network infrastructure. In addition, the network \ninfrastructure enables collaboration with outside con-\nsultants, external discipline experts, and intra-Center \ncollaboration. The effective use of the network infra-\nstructure requires a balance between network security \nand collaboration and, as such, will always be a source \nof modification, upgrade, and reconfiguration. A natural \nextension of this collaboration is the execution of geo-\ngraphically distributed CACE efforts; therefore it is es-\nsential that a CACE facility have the tools, processes, and \ncommunications capabilities to support such distributed \nstudies.\n\n7.2.7.3 Facility Support Staff Responsibilities\nA core staff of individuals is required to maintain an op-\nerational CACE environment. The responsibilities to be \ncovered include end-to-end CACE operations and the \nmanagement and administration of the information in-\nfrastructure. \n\nCACE information infrastructure management and ad-\nministration includes computer workstation configura-\ntion; network system administration; documentation \ndevelopment; user help service; and software support to \nmaintain infrastructure databases, tools, and Web sites.\n\n7.2.8 CACE Products \nCACE products are applicable across project life-cycle \nphases and can be clearly mapped to the various outputs \nassociated with the systems engineering activities such \nas requirements definition, trade studies, decision anal-\nysis, and risk management. CACE products from a typ-\nical design effort include a requirements summary with \ndriving requirements identified; system and subsystem \n\nanalysis; functional architectures and data flows; mass/\npower/data rackups; mission design and ConOps; engi-\nneering trades and associated results; technology matu-\nrity levels; issues, concerns, and risks; parametric and/or \ngrassroots cost estimates; engineering analyses, models, \nand applicable tools to support potential future efforts; \nand a list of suggested future analyses.\n\nCACE product format and content vary broadly both \nwithin and across CACE environments. The particular \nCACE environment, the goals/objectives of the supported \nactivity, whether the activity was supported by multiple \nCACE teams or not, the customer\u2019s ultimate use, and the \nschedule requirements are some aspects that factor into \nthe final product content and format. A primary goal in \nthe identification and development of CACE products \nand in the packaging of the final delivery is to facilitate \ntheir use after the CACE activity.\n\nProducts include in-study results presentation, Power-\nPoint packages, formal reports and supporting com-\nputer-aided design models, and engineering analysis. \nRegardless of format, the CACE final products typically \nsummarize the incoming requirements, study goal ex-\npectation, and study final results.\n\nCACE environment flexibility enables support activities \nbeyond that of a traditional engineering design study \n(e.g., independent technical reviews, cost validation, risk \nand technology assessments, roadmapping, and require-\nments review). Product contents for such activities might \ninclude feasibility assessment, technical recommenda-\ntions, risk identification, recosting, technology infusion \nimpact and implementation approach, and architectural \noptions. \n\nIn addition to formal delivery of the CACE product to \nthe customer team, the final results and planning data \nare archived within the CACE environment for future \nreference and for inclusion in internal CACE cross-study \nanalyses.\n\n7.2.9 CACE Best Practices\nThis subsection contains general CACE best practices \nfor a successful CACE design activity. Three main topic \nareas\u2014people, process, technologies\u2014are applicable \nto both local and geographically distributed activities. \nMany lessons learned about the multi-CACE collabo-\nration activities were learned through the NASA Ex-\nploration Design Team (NEDT) effort, a OneNASA \n\n\n\n240 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nmulti-Center distributed collaborative design activity \nperformed during FY05.\n\n7.2.9.1 People\nTraining: ?  Individuals working in CACE environ-\nments benefit from specialized training. This training \nshould equip individuals with the basic skills neces-\nsary for efficient and effective collaboration. Training \nshould include what is required technically as well as \norientation to the CACE environment and processes.\nCharacteristics: ?  Collaborative environment skills in-\nclude being flexible, working with many unknowns, \nand willingness to take risks. Ability and willingness \nto think and respond in the moment is required as \nwell as the ability to work as part of a team and to \ninteract directly with customer representatives to ne-\ngotiate requirements and to justify design decisions. \nSupporting engineers also need the ability to quickly \nand accurately document their final design as well as \npresent this design in a professional manner. In addi-\ntion, the CACE team leads or facilitators should have \nadditional qualities to function well in a collaborative \ndesign environment. These include organizational \nand people skills, systems engineering skills and back-\nground, and broad general engineering knowledge.\n\n7.2.9.2 Process and Tools\nCustomer Involvement: ?  Managing customer expec-\ntations is the number one factor in positive study out-\ncome. It is important to make the customers continu-\nously aware of the applications and limitations of the \nCACE environment and to solicit their active partici-\npation in the collaborative environment. \nAdaptability:  ? The CACE environments must adapt \nprocesses depending on study type and objectives, as \ndetermined in negotiations prior to study execution. \nIn addition to adapting the processes, engineers with \nappropriate engineering and collaborative environ-\nment skills must be assigned to each study. \nStaffing: ?  Using an established team has the benefit of \nthe team working together and knowing each other \nand the tools and processes. A disadvantage is that a \nstanding army can get \u201cstale\u201d and not be fluent with \nthe latest trends and tools in their areas of expertise. \nSupporting a standing army full time is also an expen-\nsive proposition and often not possible. A workable \ncompromise is to have a full-time (or nearly full-time) \nleadership team complemented by an engineering \n\nteam. This engineering team could be composed of \nengineers on rotational assignments or long-term de-\ntail to the team, as appropriate. An alternative para-\ndigm is to partially staff the engineering team with \npersonnel provided through the customer team.\nTools and Data Exchange: ?  In general, each engineer \nshould use the engineering tools with which he or she \nis most familiar to result in an effective and efficient \nprocess. The CACE environment should provide an \ninformation infrastructure to integrate resulting engi-\nneering parameters.\nDecision Process: ?  Capturing the decisionmaking \nand design rationale is of great interest and of value \nto CACE customers as well as being a major challenge \nin the rapid engineering environment. The benefit of \nthis is especially important as a project progresses and \nmakes the CACE product more valuable to the cus-\ntomer. Further along in the life cycle of a mission or \ninstrument, captured decisions and design rationale \nare more useful than a point-design from some ear-\nlier time. \nCommunication: ?  CACE environments foster rapid \ncommunication among the team members. Because \nof the fast-paced environment and concurrent engi-\nneering activities, keeping the design elements \u201cin \nsynch\u201d is a challenge. This challenge can be addressed \nby proactive systems engineers, frequent tag-ups, ad-\nditional systems engineering support and the use of \nappropriate information infrastructure tools.\nStandards Across CACE Environments: ?  Establishing \nminimum requirements and standard sets of tools \nand techniques across the NASA CACE environment \nwould facilitate multi-Center collaborations.\nPlanning: ?  Proper planning and preparation are cru-\ncial for efficient CACE study execution. Customers \nwanting to forgo the necessary prestudy activity or \nplanning and preparation must be aware of and ac-\ncept the risk of a poor or less-than-desired outcome.\n\n7.2.9.3 Facility\nCommunication Technologies: ?  The communication \ninfrastructure is the backbone of the collaborative \nCACE environment. Certain technologies should be \navailable to allow efficient access to resources external \nto a CACE facility. It is important to have \u201cplug and \nplay\u201d laptop capability, for example. Multiple phones \nshould be available to the team and cell phone access \nis desirable.\n\n\n\n7.2 Integrated Design Facilities\n\nNASA Systems Engineering Handbook ? 241\n\nDistributed Team Connectivity: ?  Real-time transfer of \ninformation for immediate access between geographi-\ncally distributed teams or for multi-Center activities can \n\nbe complicated due to firewall and other networking is-\nsues. Connectivity and information transfer methods \nshould be reviewed and tested before study execution.\n\n\n\n242 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\n7.3 Selecting Engineering Design Tools\n\nNASA utilizes cutting-edge design tools and techniques \nto create the advanced analyses, designs, and concepts \nrequired to develop unique aerospace products, space-\ncraft, and science experiments. The diverse nature of the \ndesign work generated and overseen by NASA requires \nuse of a broad spectrum of robust electronic tools such \nas computer-aided design tools and computer-aided sys-\ntems engineering tools. Based on the distributed and \nvaried nature of NASA projects, selection of a single suite \nof tools from only one vendor to accomplish all design \ntasks is not practical. However, opportunities to improve \nstandardization of design policy, processes, and tools re-\nmain a focus for continuous improvement activities at all \nlevels within the Agency.\n\nThese guidelines serve as an aid to help in the selection \nof appropriate tools in the design and development of \naerospace products and space systems and when se-\nlecting tools that affect multiple Centers.\n\n7.3.1 Program and Project Considerations\nWhen selecting a tool to support a program or project, \nall of the upper level constraints and requirements must \nbe identified early in the process. Pertinent information \nfrom the project that affects the selection of the tools will \ninclude the urgency, schedule, resource restrictions, ex-\ntenuating circumstances, and constraints. A tool that \ndoes not support meeting the program master schedule \nor is too costly to be bought in sufficient numbers will not \nsatisfy the project manager\u2019s requirements. For example, \na tool that requires extensive modification and training \nthat is inconsistent with the master schedule should not \nbe selected by the technical team. If the activity to be \nundertaken is an upgrade to an existing project, legacy \ntools and availability of trained personnel are factors to \nbe considered.\n\n7.3.2 Policy and Processes\nWhen selecting a tool, one must consider the applicable \npolicies and processes at all levels, including those at the \nCenter level, within programs and projects, and at other \nCenters when a program or project is a collaborative ef-\nfort. In the following discussion, the term \u201corganization\u201d \nwill be used to represent any controlling entity that estab-\nlishes policy and/or processes for the use of tools in the de-\nsign or development of NASA products. In other words, \n\n\u201corganization\u201d can mean the user\u2019s Center, another col-\nlaborating Center, a program, a project, in-line engi-\nneering groups, or any combination of these entities.\n\nPolicies and processes affect many aspects of a tool\u2019s \nfunctionality. First and foremost, there are policies that \ndictate how designs are to be formally or informally con-\ntrolled within the organization. These policies address \nconfiguration management processes that must be fol-\nlowed as well as the type of data object that will be for-\nmally controlled (e.g., drawings or models). Clearly this \nwill affect the types of tools that will be used and how \ntheir designs will be annotated and controlled.\n\nThe Information Technology (IT) policy of the organi-\nzation also needs to be considered. Data security and \nexport control (e.g., International Traffic in Arms Reg-\nulations (ITAR)) policies are two important IT policy \nconsiderations that will influence the selection of a par-\nticular design tool.\n\nThe policy of the organization may also dictate require-\nments on the format of the design data that is produced \nby a tool. A specific format may be required for sharing \ninformation with collaborating parties. Other consid-\nerations are the organizations\u2019 quality processes, which \ncontrol the versions of the software tools as well as their \nverification and validation. There are also policies on \ntraining and certifying users of tools supporting critical \nflight programs and projects. This is particularly impor-\ntant when the selection of a new tool results in the transi-\ntion from a legacy tool to a new tool. Therefore, the quality \nof the training support provided by the tool vendor is an \nimportant consideration in the selection of any tool.\n\nAlso, if a tool is being procured to support a multi-Center \nprogram or project, then program policy may dictate \nwhich tool must be used by all participating Centers. If \nCenters are free to select their own tool in support of a \nmulti-Center program or project, then consideration of \nthe policies of all the other Centers must be taken into \naccount to ensure compatibility among Centers.\n\n7.3.3 Collaboration\nThe design process is highly collaborative due to the com-\nplex specialties that must interact to achieve a successful \nintegrated design. Tools are an important part of a suc-\n\n\n\n7.3 Selecting Engineering Design Tools\n\nNASA Systems Engineering Handbook ? 243\n\ncessful collaboration. To successfully select and integrate \ntools in this environment requires a clear understanding \nof the intended user community size, functionality re-\nquired, nature of the data to be shared, and knowledge \nof tools to be used. These factors will dictate the number \nof licenses, hosting capacity, tool capabilities, IT secu-\nrity requirements, and training required. The sharing of \ncommon models across a broad group requires mecha-\nnisms for advancing the design in a controlled way. Ef-\nfective use of data management tools can help control \nthe collaborative design by requiring common naming \nconventions, markings, and design techniques to ensure \ncompatibility among distributed design tools.\n\n7.3.4 Design Standards\nDepending on the specific domain or discipline, there \nmay be industry and Center-specific standards that must \nbe followed, particularly when designing hardware. This \ncan be evident in the design of a mechanical part, where \na mechanical computer-aided design package selected to \nmodel the parts must have the capability to meet specific \nstandards, such as model accuracy, dimensioning and \ntolerancing, the ability to create different geometries, and \nthe capability to produce annotations describing how to \nbuild and inspect the part. However, these same issues \nmust be considered regardless of the product.\n\n7.3.5 Existing IT Architecture\nAs with any new tool decision, an evaluation of defined \nAgency and Center IT architectures should be made that \nfocuses on compatibility with and duplication of existing \ntools. Typical architecture considerations would include \ndata management tools, middleware or integration in-\nfrastructure, network transmission capacity, design anal-\nysis tools, manufacturing equipment, approved hosting, \nand client environments.\n\nWhile initial focus is typically placed on current needs, \nthe scalability of the tools and the supporting IT infra-\nstructure should be addressed too. Scalability applies to \nboth the number of users and capacity of each user to \nsuccessfully use the system over time.\n\n7.3.6 Tool Interfaces\nInformation interfaces are ubiquitous, occurring when-\never information is exchanged.\n\nThis is particularly characteristic of any collaborative \nenvironment. It is here that inefficiencies arise, infor-\n\nmation is lost, and mistakes are made. There may be an \norganizational need to interface with other capabilities \nand/or analysis tools, and understanding the tools used \nby the design teams with which your team interfaces \nand how the outputs of your team drive other down-\nstream design functions is critical to ensure compat-\nibility of data. \n\nFor computer-aided systems engineering tools, users are \nencouraged to select tools that are compatible with the \nObject Management Group System Modeling Language \n(SysML) standard. SysML is a version of the Unified \nModeling Language (UML) that has been specifically \ndeveloped for systems engineering.\n\n7.3.7 Interoperability and Data Formats\nInteroperability is an important consideration when se-\nlecting tools. The tools must represent the designs in for-\nmats that are acceptable to the end user of the data. It is \nimportant that any selected tool include associative data \nexchange and industry-standard data formats. As the \nAgency increasingly engages in multi-Center programs \nand projects, the need for interoperability among dif-\nferent tools, and different versions of the same tool, be-\ncomes even more critical. True interoperability reduces \nhuman error and the complexity of the integration task, \nresulting in reduced cost, increased productivity, and a \nquality product.\n\nWhen considering all end users\u2019 needs, it is clear that in-\nteroperability becomes a difficult challenge. Three broad \napproaches, each with their own strengths and weak-\nnesses, are: \n\nHave all employees become proficient in a variety of  ?\ndifferent tool systems and the associated end use ap-\nplications. While this provides a broad capability, it \nmay not be practical or affordable.\n\nRequire interoperability among whatever tools are  ?\nused, i.e., requiring that each tool be capable of trans-\nferring model data in a manner that can be easily and \ncorrectly interpreted by all the other tools. Consid-\nerable progress has been made in recent years in the \nstandards for the exchange of model data. While this \nwould be the ideal solution for many, standard data \nformats that contain the required information for all \nend users do not yet exist.\n\nDictate that all participating organizations use the  ?\nsame version of the same tool.\n\n\n\n244 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\n7.3.8 Backward Compatibility\nOn major programs and projects that span several years, \nit is often necessary to access design data that are more \nthan 3 to 5 years old. However, access to old design data \ncan be extremely difficult and expensive, either because \ntool vendors end their support or later versions of the tool \ncan no longer read the data. Strategies for maintaining \naccess include special contracts with vendors for longer \nsupport, archiving design data in neutral formats, con-\ntinuous migration of archives into current formats, and \nrecreating data on demand. Organizations should select \nthe strategy that works best for them, after a careful con-\nsideration of the cost and risk.\n\n7.3.9 Platform\nWhile many tools will run on multiple hardware plat-\nforms, some perform better in specific environments \nor are only supported by specified versions of operating \nsystems. In the case of open-source operating systems, \nmany different varieties are available that may not fully \nsupport the intended tools. If the tool being considered \nrequires a new platform, the additional procurement cost \nand administration support costs should be factored in.\n\n7.3.10 Tool Configuration Control\nTool configuration control is a tradeoff between respon-\nsive adoption of the new capabilities in new versions and \nsmooth operation across tool chain components. This \nis more difficult with heterogeneous (multiple vendor) \ntool components. An annual or biannual block upgrade \nstrategy requires significant administrative effort. On the \nother hand, the desktop diversity resulting from user-\nmanaged upgrade timing also increases support require-\nments.\n\n7.3.11 Security/Access Control\nSpecial consideration should be given to the sensitivity \nand required access of all design data. Federal Govern-\nment and Agency policy requires the assessment of all \ntools to ensure appropriate security controls are ad-\ndressed to maintain the integrity of the data.\n\n7.3.12 Training\nMost of the major design tools have similar capabilities \nthat will not be new concepts to a seasoned designer. \nHowever, each design tool utilizes different techniques \nto perform design functions, and each contains some \n\nunique tool sets that will require training. The more \nresponsive vendors will provide followup access to in-\nstructors and onsite training with liberal distribution of \ntraining materials and worked examples. The cost and \ntime to perform the training and time for the designer to \nbecome proficient can be significant and should be care-\nfully factored in when making decisions on new design \ntools. \n\nThe disruptive aspect of training is an important con-\nsideration in adapting to a different tool. Before transi-\ntioning to a new tool, an organization must consider the \nschedule of deliverables to major programs and projects. \nCan commitments still be met in a timely fashion? It is \nsuggested that organizations implement a phase-in ap-\nproach to a new tool, where the old tool is retained for \nsome time to allow people to learn the new tool and be-\ncome proficient in its use. The transition of a fully func-\ntional and expert team using any one system, to the same \nteam fully functional using another system, is a signifi-\ncant undertaking. Some overlap between the old tool \nand the new tool will ensure flexibility in the transition \nand ensure that the program and project work proceeds \nuninterrupted.\n\n7.3.13 Licenses\nLicenses provide and control access to the various mod-\nules or components of a product or product family. Con-\nsideration of the license scheme should be taken into \naccount while selecting a tool package. Licenses are \nsometimes physical, like a hardware key that plugs into \na serial or parallel port, or software that may or may not \nrequire a whole infrastructure to administer. Software li-\ncenses may be floating (able to be shared on many com-\nputers on a first-come, first-served basis) or locked (ded-\nicated to a particular computer). A well-thought-out \nstrategy for licenses must be developed in the beginning \nof the tool selection process. This strategy must take into \nconsideration program and project requirements and \nconstraints as well as other factors such as training and \nuse.\n\n7.3.14 Stability of Vendor and Customer \nSupport\n\nAs in the selection of any support device or tool, vendor \nstability is of great importance. Given the significant in-\nvestment in the tools (directly) and infrastructure (indi-\nrectly), it is important to look at the overall company sta-\n\n\n\n7.3 Selecting Engineering Design Tools\n\nNASA Systems Engineering Handbook ? 245\n\nbility to ensure the vendor will be around to support the \ntools. Maturity of company products, installed user base, \ntraining, and financial strength can all provide clues to \nthe company\u2019s ability to remain in the marketplace with \na viable product. In addition, a responsive vendor pro-\nvides customer support in several forms. A useful venue \nis a Web-based user-accessible knowledge base that in-\ncludes resolved issues, product documentation, manuals, \n\nwhite papers, and tutorials. Live telephone support can be \nvaluable for customers who don\u2019t provide support inter-\nnally. An issue resolution and escalation process involves \ncustomers directly in prioritizing and following closure \nof critical issues. Onsite presence by the sales team and \napplication engineers, augmented by post-sales support \nengineers, can significantly shorten the time to discovery \nand resolution of issues and evolving needs.\n\n\n\n246 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\n7.4 Human Factors Engineering\n\nThe discipline of Human Factors (HF) is devoted to the \nstudy, analysis, design, and evaluation of human-system \ninterfaces and human organizations, with an emphasis \non human capabilities and limitations as they impact \nsystem operation. HF engineering issues relate to all as-\npects of the system life, including design, build, test, op-\nerate, and maintain, across the spectrum of operating \nconditions (nominal, contingency, and emergency). \n\nPeople are critical components in complex aerospace \nsystems: designers, manufacturers, operators, ground \nsupport, and maintainers. All elements of the system \nare influenced by human performance. In the world of \nhuman-system interaction, there are four avenues for \nimproving performance, reducing error, and making \nsystems more error tolerant: (1) personnel selection; \n(2) system, interface, and task design; (3) training; and \n(4) procedure improvement. Most effective performance \nimprovement involves all four avenues. People can be \nhighly selected for the work they are to perform and the \nenvironment they are to perform it in. Second, equipment \nand systems can be designed to be easy to use, error re-\nsistant, and quickly learned. Third, people can be trained \nto proficiency on their required tasks. Fourth, improving \ntasks or procedures can be an important intervention. \n\nHF focuses on those aspects where people interface \nwith the system. It considers all personnel who must in-\nteract with the system, not just the operator; deals with \norganizational systems as well as hardware; and exam-\nines all types of interaction, not just hardware or soft-\nware interfaces. The role of the HF specialist is to advo-\ncate for the human component and to ensure that the \ndesign of hardware, software, tasks, and environment is \ncompatible with the sensory, perceptual, cognitive, and \nphysical attributes of those interacting with the system. \nThe HF specialist should elucidate why human-related \nissues or features should be included in analyses, de-\nsign decisions, or tests and explain how design options \nwill affect human performance in ways that impact total \nsystem performance and/or cost. As system complexity \ngrows, the potential for conflicts between requirements \nincreases. Sophisticated human-system interfaces create \nconflicts such as the need to create systems that are easy \nfor novices to learn while also being efficient for experts \nto use. The HF specialist recognizes these tradeoffs and \nconstraints and provides guidance on balancing these \n\ncompeting requirements. The domain of application is \nanywhere there are concerns regarding human and orga-\nnizational performance, error, safety, and comfort. The \ngoal is always to inform and improve the design. \n\nWhat distinguishes an HF specialist is the particular \nknowledge and methods used, the domain of employ-\nment, and the goal of the work. HF specialists have ex-\npertise in the knowledge of human performance, both \ngeneral and specific. There are many academic special-\nties concerned with applying knowledge of human be-\nhavior. These include psychology, cognitive science, cog-\nnitive psychology, sociology, economics, instructional \nsystem development, education, physiology, industrial \npsychology, organizational behavior, communication, \nand industrial engineering. Project and/or process man-\nagers should consult with their engineering or SMA di-\nrectorates to get advice and recommendations on spe-\ncific HF specialists who would be appropriate for their \nparticular activity.\n\nIt is recommended to consider having HF specialists \non the team throughout all the systems engineering \ncommon technical processes so that they can construct \nthe specific HF analysis techniques and tests custom-\nized to the specific process or project. Not only do the \nHF specialists help in the development of the end items \nin question, but they should also be used to make sure \nthat the verification test and completeness techniques \nare compatible and accurate for humans to undertake. \nParticipation early in the process is especially important. \nEntering the system design process early ensures that \nhuman systems requirements are \u201cdesigned in\u201d rather \nthan corrected later. Sometimes the results of analyses \nperformed later call for a reexamination of earlier anal-\nyses. For example, functional allocation typically must \nbe refined as design progresses because of technological \nbreakthroughs, unforeseen technical difficulties in de-\nsign or programming, or task analysis may indicate that \nsome tasks assigned to humans exceed human capabili-\nties under certain conditions. \n\nDuring requirements definition, HF specialists ensure \nthat HF-related goals and constraints are included in \nthe overall plans for the system. The HF specialist must \nidentify the HF-related issues, design risks, and trad-\neoffs pertinent to each human-system component, and \ndocument these as part of the project\u2019s requirements so \n\n\n\n7.4 Human Factors Engineering\n\nNASA Systems Engineering Handbook ? 247\n\nthey are adequately addressed during the design phase. \nFor stakeholder expectation definition from the HF per-\nspective, the stakeholders include not only those who are \nspecifying the system to be built, but also those who will \nbe utilizing the system when it is put into operation. This \napproach yields requirements generated from the top \ndown\u2014what the system is intended to accomplish\u2014and \nfrom the bottom up\u2014how the system is anticipated to \nfunction. It is critical that the HF specialist contribute to \nthe ConOps. The expectations of the role of the human in \nthe system and the types of tasks the human is expected \nto perform underlie all the hardware and software re-\nquirements. The difference between a passive passenger \nand an active operator will drive major design decisions. \nThe number of crewmembers will drive subsequent de-\ncisions about habitable volume and storage and about \ncrew time available for operations and maintenance. \nHF specialists ensure appropriate system design that de-\nfines the environmental range in which the system will \noperate and any factors that impact the human compo-\nnents. Many of these factors will need to accommodate \nhuman, as well as machine, tolerances. The requirements \nmay need to specify acceptable atmospheric conditions, \nincluding temperature, pressure, composition, and hu-\nmidity, for example. The requirements might also address \nacceptable ranges of acoustic noise, vibration, accelera-\ntion, and gravitational forces, and the use of protective \nclothing. The requirements may also need to accommo-\ndate adverse or emergency conditions outside the range \nof normal operation.\n\n7.4.1 Basic HF Model\nA key to conducting human and organizational analysis, \ndesign, and testing is to have an explicit framework that \nrelates and scopes the work in question. The following \nmodel identifies the boundaries and the components in-\nvolved in assessing human impacts.\n\nThe HF interaction model (Figure 7.4-1) provides a refer-\nence point of items to be aware of in planning, analyzing, \ndesigning, testing, operating, and maintaining systems. \nDetailed checklists should be generated and customized \nfor the particular system under development. The model \npresented in this module is adapted from David Meister\u2019s \nHuman Factors: Theory and Practice and is one depiction \nof how humans and systems interact. Environmental in-\nfluences on that interaction have been added. The model \nillustrates a typical information flow between the human \nand machine components of a system.\n\nFigure 7.4-2 provides a reference point of human factors \nprocess phases to be aware of in planning, analyzing, de-\nsigning, testing, operating, and maintaining systems.\n\n7.4.2 HF Analysis and Evaluation \nTechniques\n\nTable 7.4-1 provides a set of techniques for human and \norganizational analysis and evaluation that can help to \nensure that appropriate human and organizational fac-\ntors have been considered and accommodated. These \nHF analysis methods are used to analyze systems, pro-\nvide data about human performance, make predictions \nabout human-system performance, and evaluate if the \nhuman-machine system performance meets design cri-\nteria. Most methods involve judgment and so are highly \ndependent on the skill and expertise of the analyst. In \naddition, both experienced and inexperienced opera-\ntors provide valuable information about the strengths \nand weaknesses of old systems and how the new system \nmight be used.\n\nThese methods are appropriate to all phases of system \ndesign with increasing specificity and detail as devel-\nopment progresses. While HF principles are often re-\nsearched and understood at a generic level, their appli-\ncation is only appropriate when tailored to fit the design \nphase. Each type of analysis yields different kinds of in-\nformation and so they are not interchangeable. The out-\nputs or products of the analyses go into specification doc-\numents (operational needs document, ConOps, System \nRequirements Document (SRD), etc.) and formal review \nprocesses (e.g., ORR, SRR, SDR, PDR, CDR, PRR, PIR). \n\nHuman Factors\nInteraction Model\n\nMachine Display\nComponent\n\nHuman Sensory\nComponent\n\nM\nac\n\nhi\nne\n\n C\nPU\n\nCo\nm\n\npo\nne\n\nnt\n\nHuman Musculoskeletal\nComponent\n\nH\num\n\nan Cognitive\nCom\n\nponent\n\nMachine Input\nDevice Component\n\nFigure 7.4?1 Human factors interaction model\n\n\n\n248 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nThe list shown in Table 7.4-1 is not exhaustive. The main \npoint is to show examples that demonstrate the scope \n\nPre-Phase A: Concept Studies/\nPhase A: Concept & Tech Development\n\nNASA Program/Project\nLife Cycle \n\nPhase A: Concept & Tech Development/\nPhase B: Prelim Design & Tech Completion\n\nPhase C: \nFinal Design & Fabrication\n\nPhase D: System Assembly,\nIntegration & Test, Launch\n\nPhase E:\nOperations & Sustainment\n\nPhase F:\nCloseout\n\nHuman Factors Engineering\nProcess Integrating Points \n\n7. Track Use of the System;\nValidation Testing\n\n6. Usability Testing of Procedures &\nIntegration Assessment\n\n1. Operational Analysis &\nAnalysis of Similar Systems\n\n2. Preliminary Function\nAllocation & Task Analysis\n\n5. Formal Usability Testing\nof Full System\n\n4. Usability Study of\nComponents, Prototypes, Mockups\n\n3. Human Factors\nRequirements De?nitions\n\nFigure 7.4?2 HF engineering process and its links to the NASA program/\nproject life cycle\n\nand usefulness of common methods used to evaluate \nsystem design and development.\n\n\n\n7.4 Human Factors Engineering\n\nNASA Systems Engineering Handbook ? 249\n\nTable 7.4?1 Human and Organizational Analysis Techniques \n\nProcess Human/Individual Analysis Additional Organizational Analysis\n\nA. Operational Analysis\n\nDefinition When applied to HF, it is analysis of projected operations.\n\nPurpose Obtain information about situations or events that may confront \noperators and maintainers using the new system. Systems \nengineers or operations analysts have typically done operational \nanalyses. HF specialists should also be members of the analysis \nteam to capture important operator or maintainer activities.\n\nInputs RFPs, planning documents, system requirements documents, and \nexpert opinion.\n\nProcess Consult the systems engineer and projected users to extract \nimplications for operators and maintainers.\n\nAssess interactions and logistics \nbetween individuals and organizations. \nEvaluate operations under different \ntypes of organizations, structures, or \ndistributions.\n\nOutputs Detailed scenarios (for nominal operations, hard and soft failures, \nand emergencies) including consequences; verbal descriptions \nof events confronting operators and maintainers; anticipated \noperations (list of feasible operations and those that may overstress \nthe system); assumptions; constraints that may affect system per-\nformance; environments; list of system operation and maintenance \nrequirements.\n\nNew or adjusted workflows to com-\npensate for organizational impacts as \nappropriate.\n\nB. Similar Systems Analysis\n\nDefinition When applied to HF, it examines previous systems or systems in use \nfor information useful for the new system.\n\nPurpose To obtain lessons learned and best practices useful in planning for \na new system. Experiences gained from systems in use is valuable \ninformation that should be capitalized on.\n\nInputs Structured observations, interviews, questionnaires, activity analy-\nsis, accident/incident reports, maintenance records, and training \nrecords.\n\nProcess Obtain data on the operability, maintainability, and number of \npeople required to staff the system in use. Identify skills required \nto operate and maintain the system and training required to bring \noperators to proficiency. Obtain previous data on HF design prob-\nlems and problems encountered by previous users of the previous \nsystem or system in use.\n\nIdentify the existing system\u2019s organi-\nzational hierarchies and management \ndistribution schemes (centralized versus \ndecentralized).\n\nOutputs Identification of environmental factors that may affect personnel; \npreliminary assessments of workload and stress levels; assessment \nof skills required and their impact on selection, training, and \ndesign; estimates of future staffing and manpower requirements; \nidentification of operator and maintainer problems to avoid; \nassessment of desirability and consequences of reallocation of \nsystems functions.\n\nEvaluation of the configuration\u2019s impact \non performance and its potential risks.\n\n (continued)\n\n\n\n250 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nProcess Human/Individual Analysis Additional Organizational Analysis\n\nC. Critical Incident Study\n\nDefinition When applied to HF, it identifies sources of difficulties for operators or \nmaintenance or in the operational systems (or simulations of them).\n\nPurpose To analyze and hypothesize sources of errors and difficulties in a \nsystem. This is particularly useful when a system has been opera-\ntional and difficulties are observed or suspected, but the nature \nand severity of those difficulties is not known.\n\nInputs Operator/maintainer accounts of accidents, near-accidents, \nmistakes, and near-mistakes.\n\nProcess Interview large numbers of operators/maintainers; categorize inci-\ndents/accidents; use HF knowledge and experience to hypothesize \nsources of difficulty and how each one could be further studied; \nmitigate or redesign to eliminate difficulty.\n\nTrace difficulties between individuals \nand organizations and map associated \nresponsibilities and process assign-\nments.\n\nOutputs Sources of serious HF difficulties in the operation of a system or its \nmaintenance with suggested solutions to those difficulties.\n\nIdentification of potential gaps or \ndisconnects based on the mapping.\n\nD. Functional Flow Analysis\n\nDefinition When applied to HF, it is a structured technique for determining \nsystem requirements. Decomposes the sequence of functions or \nactions that a system must perform.\n\nPurpose Provides a sequential ordering of functions that will achieve system \nrequirements and a detailed checklist of system functions that \nmust be considered in ensuring that the system will be able to \nperform its intended mission. These functions are needed for the \nsolution of trade studies and determinations of their allocation \namong operators, equipment, software, or some combination of \nthem. Decision-action analysis is often used instead of a functional \nflow analysis when the system requires binary decisions (e.g., \nsoftware-oriented).\n\nInputs Operational analyses, analyses of similar systems, activity analyses.\n\nProcess Top-level functions are progressively expanded to lower levels \ncontaining more and more detailed information. If additional \nelaboration is needed about information requirements, sources of \ninformation, potential problems, and error-inducing features, for \nexample, then an action-information analysis is also performed.\n\nMap functional flows to associated \norganizational structures.\n\nOutputs Functional flow diagrams. Identification of any logistics or respon-\nsibility gaps based on the integrated \nmap.\n\nE. Action-Information Analysis\n\nDefinition When applied to HF, it elaborates each function or action in \nfunctional flows or decision-action diagrams by identifying the \ninformation that is needed for each action or decision to occur. \nThis analysis is often supplemented with sources of data, potential \nproblems, and error-inducing features associated with each func-\ntion or action.\n\n (continued)\n\nTable 7.4?1 Human and Organizational Analysis Techniques (continued) \n\n\n\n7.4 Human Factors Engineering\n\nNASA Systems Engineering Handbook ? 251\n\nProcess Human/Individual Analysis Additional Organizational Analysis\n\nPurpose Provides more detail before allocating functions to agents.\n\nInputs Data from the analysis of similar systems, activity analyses, critical \nincident studies, functional flow and decision-action analyses, and \ncomments and data from knowledgeable experts.\n\nProcess Each function or action identified in functional flows or decision-\naction analyses is elaborated.\n\nMap associated components (function, \naction, decisions) to the responsible \norganizational structures.\n\nOutputs Detailed lists of information requirements for operator-system in-\nterfaces, early estimates of special personnel provisions likely to be \nneeded, support requirements, and lists of potential problems and \nprobable solutions. Often produces suggestions for improvements \nin design of hardware, software, or procedures.\n\nIdentification of any logistics or respon-\nsibility gaps based on the integrated \nmap.\n\nF. Functional Allocation\n\nDefinition When applied to HF, it is a procedure for assigning each system \nfunction, action, and decision to hardware, software, operators, \nmaintainers, or some combination of them.\n\nPurpose To help identify user skill needs and provide preliminary estimates \nof staffing, training, and procedures requirements and workload \nassessments. Functional flows and decision-action analyses do \nnot identify the agent (person or machine) that will execute the \nfunctions.\n\nInputs Functional flow analyses, decision-action analyses, action-informa-\ntion analyses, past engineering experience with similar systems, \nstate-of-the-art performance capabilities of machines and \nsoftware, and store of known human capabilities and limitations.\n\nProcess Identify and place to the side all those functions that must be \nallocated to personnel or equipment for reasons of safety, limita-\ntions of engineering technology, human limitations, or system \nrequirements. List the remaining functions\u2014those that could be \neither performed manually or by some combination of personnel \nand equipment. Prepare descriptions of implementation. Establish \nweighting criteria for each design alternative. Compare alternative \nconfigurations in terms of their effectiveness in performing the \ngiven function according to those criteria.\n\nAfter initial assignment configuration \nis completed, evaluate allocations \nagainst relevant organizational norms, \nvalues, and organizational interfaces for \nlogistics and management impacts.\n\nOutputs Allocations of system functions to hardware, software, operators, \nmaintainers, or some combination of them. Task analyses are then \nperformed on those functions allocated to humans.\n\nList of potential impacts with recom-\nmended modifications in either func-\ntions or management or both.\n\nG. Task Analysis\n\nDefinition When applied to HF, it is a method for producing an ordered list of \nall the things people will do in a system.\n\nPurpose To develop input to all the analyses that come next. A subsequent \ntimeline analysis chart can provide the temporal relationship \namong tasks\u2014sequences of operator or maintainer actions, the \ntimes required for each action, and the time at which each action \nshould occur.\n\n (continued)\n\nTable 7.4?1 Human and Organizational Analysis Techniques (continued) \n\n\n\n252 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nProcess Human/Individual Analysis Additional Organizational Analysis\n\nInputs Data from all the methods above supplemented with information \nprovided by experts who have had experience with similar systems.\n\nProcess HF specialists and subject matter experts list and describe all tasks, \nsubdividing them into subtasks with the addition of supplemen-\ntary information.\n\nGroup all tasks assigned to a given \norganization and evaluate the range of \nskills, communications, and manage-\nment capabilities required. Evaluate \nnew requirements against existing \norganization\u2019s standard operating \nprocedures, norms, and values.\n\nOutputs Ordered list of all the tasks people will perform in a system. Details \non information requirements, evaluations and decisions that must \nbe made, task times, operator actions, and environmental condi-\ntions.\n\nIdentify group-level workloads, \nmanagement impacts, and training \nrequirements.\n\nH. Fault Tree Analysis\n\nDefinition When applied to HF, it determines those combinations of events \nthat could cause specific system failures, faults, or catastrophes. \nFault tree and failure mode and effects analysis are concerned with \nerrors.\n\nPurpose Anticipate mistakes that operators or maintainers might make and \ntry to design against those mistakes. Limitation for HF is that each \nevent must be described in terms of only two possible conditions, \nand it is extremely difficult to attach exact probabilities to human \nactivities.\n\nInputs All outputs of the methods described above, supplemented with \ndata on human reliability.\n\nProcess Construct a tree with symbols (logic gates) to represent events \nand consequences and describe the logical relationship between \nevents.\n\nAnticipate mistakes and disconnects \nthat may occur in the workflow between \nindividuals and organizations including \nunanticipated interactions between \nstandard organization operating proce-\ndures and possible system events.\n\nOutputs Probabilities of various undesirable workflow-related events, the \nprobable sequences that would produce them, and the identifica-\ntion of sensitive elements that could reduce the probability of a \nmishap.\n\nProbabilities of various undesirable \nworkflow-related events arranged by \norganizational interface points for the \nworkflows, the probable sequences that \nwould produce them, and the identifica-\ntion of sensitive elements that could \nreduce the probability of a mishap.\n\nI. Failure Modes and Effects Analysis\n\nDefinition When applied to HF, it is a methodology for identifying error-induc-\ning features in a system.\n\nPurpose Deduce the consequences for system performance of a failure in \none or more components (operators and maintainers) and the \nprobabilities of those consequences occurring.\n\n (continued)\n\nTable 7.4?1 Human and Organizational Analysis Techniques (continued) \n\n\n\n7.4 Human Factors Engineering\n\nNASA Systems Engineering Handbook ? 253\n\nProcess Human/Individual Analysis Additional Organizational Analysis\n\nInputs All outputs of methods described above, supplemented with data \non human reliability.\n\nProcess Analyst identifies the various errors operators or maintainers may \nmake in carrying out subtasks or functions. Estimates are made \nof the probabilities or frequencies of making each kind of error. \nThe consequences of each kind of error are deduced by tracing its \neffects through a functional flow diagram to its final outcome.\n\nIdentify possible organizational entities \nand behaviors (i.e., political) involved \nwith the system. Estimate the prob-\nabilities of occurrence and the impact of \nconsequence.\n\nOutputs List of human failures that would have critical effects on system \noperation, the probabilities of system or subsystem failures due to \nhuman errors, and identification of those human tasks or actions \nthat should be modified or replaced to reduce the probability of \nserious system failures.\n\nList organizational behaviors that \nwould have a critical effect on the \nsystem operation; the probabilities of \nthe system or subsystem failures due \nto the organizational behaviors; and \nthose organizational values, culture, or \nactions/standard operating procedures \nthat should be modified or replaced to \nreduce the probability of serious system \nfailures.\n\nJ. Link Analysis\n\nDefinition When applied to HF, it is an examination of the relationships \nbetween components, including the physical layout of instrument \npanels, control panels, workstations, or work areas to meet certain \nobjectives.\n\nPurpose To determine the efficiency and the effectiveness of the physical \nlayout of the human-machine interface.\n\nInputs Data from activity and task analysis and observations of functional \nor simulated systems.\n\nProcess List all personnel and items. Estimate frequencies of linkages \nbetween items, operators, or items and operators. Estimate the \nimportance of each link. Compute frequency-importance values \nfor each link. Starting with the highest link values, successively add \nitems with lower link values and readjust to minimize linkages. Fit \nthe layout into the allocated space. Evaluate the new layout against \nthe original objectives.\n\nAssess links at individual versus organi-\nzational levels.\n\nOutputs Recommended layouts of panels, workstations, or work areas. Adjusted layouts of panels, \nworkstations, or work areas based on \noptimum individual and organizational \nperformance priorities.\n\nK. Simulation\n\nDefinition When applied to HF, it is a basic engineering or HF method to predict \nperformance of systems. Includes usability testing and prototyping.\n\nPurpose To predict the performance of systems, or parts of systems, that do \nnot exist or to allow users to experience and receive training on sys-\ntems, or parts of systems, that are complex, dangerous, or expensive.\n\nInputs Hardware, software, functions, and tasks elucidated in task analysis, \noperating procedures.\n\n (continued)\n\nTable 7.4?1 Human and Organizational Analysis Techniques (continued) \n\n\n\n254 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nProcess Human/Individual Analysis Additional Organizational Analysis\n\nProcess Users perform typical tasks on models or mockups prepared to \nincorporate some or all of the inputs.\n\nAssess individual performance within \npossible organizational models.\n\nOutputs Predictions about system performance, assessment of workloads, \nevaluation of alternative configurations, evaluation of operating \nprocedures, training, and identification of accident- or error-\nprovocative situations and mismatches between personnel and \nequipment.\n\nPredict system performance under \nvaried organizational conditions, assess \nworkloads, evaluate alternative configu-\nrations and operating procedures, train, \nand identify accident- or error-provoca-\ntive situations and mismatches between \npersonnel and equipment.\n\nL. Controlled Experimentation\n\nDefinition When applied to HF, it is a highly controlled and structured version \nof simulation with deliberate manipulation of some variables.\n\nPurpose To answer one or more hypotheses and narrow number of alterna-\ntives used in simulation.\n\nInputs From any or all methods listed thus far.\n\nProcess Select experimental design; identify dependent, independent, \nand controlled variables; set up test, apparatus, facilities, and tasks; \nprepare test protocol and instructions; select subjects; run tests; \nand analyze results statistically.\n\nScale to organizational levels where \nappropriate and feasible.\n\nOutputs Quantitative statements of the effects of some variables on others \nand differences between alternative configurations, procedures, or \nenvironments.\n\nQuantitative statements of the effects of \nsome organizational variables on others \nand differences between alternative or-\nganizational configurations, procedures, \nor environments.\n\nM. Operational Sequence Analysis\n\nDefinition When applied to HF, it is a powerful technique used to simulate \nsystems.\n\nPurpose To permit visualization of interrelationships between operators \nand operators and equipment, identify interface problems, and \nexplicitly identify decisions that might otherwise go unrecognized. \nLess expensive than mockups, prototypes, or computer programs \nthat attempt to serve the same purpose.\n\nInputs Data from all above listed methods.\n\nProcess Diagram columns show timescale, external inputs, operators, \nmachines, external outputs. Flow of events (actions, functions, \ndecisions) is then plotted from top to bottom against the timescale \nusing special symbology.\n\nAfter initial analysis is complete, group \nresults by responsible organizations.\n\nOutputs Time-based chart showing the functional relationships among \nsystem elements, the flow of materials or information, the physical \nand sequential distribution of operations, the inputs and outputs of \nsubsystems, the consequences of alternative design configurations, \npotential sources of human difficulties.\n\nAssessment of range of skills, commu-\nnication processes, and management \ncapabilities required. Evaluation of per-\nformance under various organizational \nstructures.\n\n (continued)\n\nTable 7.4?1 Human and Organizational Analysis Techniques (continued) \n\n\n\n7.4 Human Factors Engineering\n\nNASA Systems Engineering Handbook ? 255\n\nProcess Human/Individual Analysis Additional Organizational Analysis\n\nN. Workload Assessment\n\nDefinition When applied to HF, it is a procedure for appraising operator \nand crew task loadings or the ability of personnel to carry out all \nassigned tasks in the time allotted or available.\n\nPurpose To keep operator workloads at reasonable levels and to ensure that \nworkloads are distributed equitably among operators.\n\nInputs Task time, frequency, and precision data are obtained from many \nof the above listed methods supplemented with judgments and \nestimates from knowledgeable experts.\n\nProcess DOD-HDBK-763 recommends a method that estimates the time \nrequired to perform a task divided by the time available or allotted \nto perform it. There are three classes of methods: performance \nmeasures, physiological measures, and subjective workloads either \nduring or after an activity.\n\nAfter initial analysis is complete, group \nresults by responsible organizations.\n\nOutputs Quantitative assessments of estimated workloads for particular \ntasks at particular times.\n\nAssessment of range of skills, communica-\ntion processes, and management capabili-\nties required. Evaluation of performance \nunder various organizational structures.\n\nO. Situational Awareness\n\nDefinition When applied to HF, it is a procedure for appraising operator and \ncrew awareness of tasks and current situation.\n\nPurpose To raise operator and maintainer awareness to maintain safety and \nefficiency.\n\nInputs All of the above listed analyses.\n\nProcess Different methods have been proposed including: situation aware-\nness rating technique, situation awareness behaviorally anchored \nrating scale, situation awareness global assessment technique, \nsituation awareness verification and analysis tool.\n\nCollect organizational decisionmaking \nstructures and processes and map the \norganization\u2019s situational awareness \nprofiles.\n\nOutputs Quantitative estimates of situational awareness for particular tasks \nat particular times.\n\nIdentification of possible gaps, discon-\nnects, and shortfalls.\n\nP. Performance Modeling\n\nDefinition When applied to HF, it is a computational process for predicting \nhuman behavior based on current cognitive research.\n\nPurpose To predict human limitations and capabilities before prototyping.\n\nInputs All of the above listed analyses.\n\nProcess Input results from above analyses. Input current relevant environ-\nmental and machine parameters. Can be interleaved with fast-time \nsimulation to obtain frequency of error types.\n\nScale as appropriate to relevant organi-\nzational behaviors.\n\nOutputs Interrelationships between operators and operators and equip-\nment and identification of interface problems and decisions that \nmight otherwise go unrecognized.\n\nInterrelationships between individuals \nand organizations and identification \nof organizational interface problems \nand decisions that might otherwise go \nunrecognized.\n\nTable 7.4?1 Human and Organizational Analysis Techniques (continued) \n\n\n\n256 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\n7.5.1 NEPA and EO 12114\n\n7.5.1.1 National Environmental Policy Act\n\nThe National Environmental Policy Act (NEPA) declares \nthe basic national policy for protecting the human en-\nvironment. NEPA sets the Nation\u2019s goals for enhancing \nand preserving the environment. NEPA also provides \nthe procedural requirements to ensure compliance by \nall Federal agencies. NEPA compliance can be a critical \npath item in project or mission implementation. NEPA \nrequires all Federal agencies to consider, before an action \nis taken, environmental values in the planning of actions \nand activities that may have a significant impact upon \nthe quality of the human environment. NEPA directs \nagencies to consider alternatives to their proposed activ-\nities. In essence, NEPA requires NASA decisionmakers \nto integrate the NEPA process into early planning to en-\nsure appropriate consideration of environmental fac-\ntors, along with technical and economic ones. NEPA is \nalso an environmental disclosure statute. It requires that \navailable information be adequately addressed and made \navailable to the NASA decisionmakers in a timely manner \nso they can consider the environmental consequences of \nthe proposed action or activity before taking final action. \nEnvironmental information must also be made available \nto the public as well as to other Federal, state, and local \nagencies. NEPA does not require that the proposed action \nor activity be free of environmental impacts, be the most \nenvironmentally benign of potential alternatives, or be \nthe most environmentally wise decision. NEPA requires \nthe decisionmaker to consider environmental impacts as \none factor in the decision to implement an action.\n\nNASA activities are implemented through specific spon-\nsoring entities, such as NASA HQ, NASA Centers (in-\ncluding component facilities, e.g., Wallops Flight Facility, \nWhite Sands Test Facility, and Michoud Assembly Fa-\ncility), mission directorates, program, or mission support \noffices. The lead officials for these entities, the officials \nin charge, have the primary responsibility for ensuring \nthat the NEPA process is integrated into their organiza-\ntions\u2019 project planning activities before the sponsoring \nentities implement activities and actions. The spon-\nsoring entities also are responsible for ensuring that re-\ncords management requirements are met. NEPA func-\n\ntions are not performed directly by lead officials. Each \nNASA Center has an Environmental Management Of-\nfice (EMO), which is usually delegated the responsibility \nfor implementing NEPA. The EMO performs the pri-\nmary or working-level functions of the NEPA process, \nsuch as evaluating proposed activities, developing and/\nor reviewing and approving required documentation, \nadvising project managers, and signing environmental \ndecision documents on projects and programs having \nlittle or no environmental impact. However, ultimate re-\nsponsibility for complying with NEPA and completing \nthe process in a timely manner lies with the program or \nproject manager. Since the EMO provides essential func-\ntional support to the sponsoring entity, and because its \nimplementation responsibilities are delegated, the term \n\u201csponsoring entity\u201d will be used throughout to include \nthe implementing NEPA organization at any NASA fa-\ncility. In cases where the sponsoring entity needs to be \nfurther defined, it will be specifically noted. For proposals \nmade by tenants or entities using services or facilities at a \nNASA Center or component facility, the sponsoring en-\ntity shall be that Center or, if such authority is delegated \nto the component facility, the component facility.\n\nNEPA compliance documentation must be completed \nbefore project planning reaches a point where NASA\u2019s \nability to implement reasonable alternatives is effec-\ntively precluded (i.e., before hard decisions are made re-\ngarding project implementation). Environmental plan-\nning factors should be integrated into the Pre-Phase A \nconcept study phase when a broad range of alternative \napproaches is being considered. In the Phase A concept \ndevelopment stage, decisions are made that could affect \nthe Phase B preliminary design stage. At a minimum, \nan environmental evaluation should be initiated in the \nPhase A concept development stage. During this stage, \nthe responsible project manager will have the greatest \nlatitude in making adjustments in the plan to mitigate \nor avoid important environmental sensitivities and in \nplanning the balance of the NEPA process to avoid un-\npleasant surprises later in the project cycle which may \nhave schedule and/or cost implications. Before com-\npleting the NEPA process, no NASA official can take an \naction that would (1) affect the environment or (2) limit \nthe choice of reasonable alternatives.\n\n7.5 Environmental, Nuclear Safety, Planetary Protection, and Asset Protection \nPolicy Compliance\n\n\n\n7.5 Environmental, Nuclear Safety, Planetary Protection, and Asset Protection Policy Compliance\n\nNASA Systems Engineering Handbook ? 257\n\nAccommodating environmental requirements early in \nproject planning ultimately conserves both budget and \nschedule. Further detail regarding NEPA compliance \nrequirements for NASA programs and projects can be \nfound in NPR 8580.1, Implementing The National Envi-\nronmental Policy Act and Executive Order 12114.\n\n7.5.1.2 EO 12114 Environmental Effects \nAbroad of Major Federal Actions \n\nExecutive Order (EO) 12114 was issued \u201csolely for the pur-\npose of establishing internal procedures for Federal agen-\ncies to consider the significant effects of their actions on \nthe environment outside the United States, its territories \nand possessions.\u201d The EO also specifically provided that \nits purpose is to enable the decisionmakers of the Federal \nagencies to be informed of pertinent environmental con-\nsiderations, and factor such considerations in their deci-\nsions; however, such decisionmakers must still take into \naccount considerations such as foreign policy, national se-\ncurity, and other relevant special circumstances.\n\nThe NASA Office of the General Counsel (OGC), or des-\nignee, is the NASA point of contact and official NASA \nrepresentative on any matter involving EO 12114. Accord-\ningly, any action by, or any implementation or legal inter-\npretation of EO 12114 requires consultations with and the \nconcurrence of the designee of the OGC. The sponsoring \nentity and local EMO contemplating an action that would \nhave global environmental effects or effects outside the \nterritorial jurisdiction of the United States must notify the \nNASA Headquarters/Environmental Management Divi-\nsion (HQ/EMD). The HQ/EMD will, in turn, coordinate \nwith the Office of the General Counsel, the Assistant Ad-\nministrator for External Relations, and other NASA orga-\nnizations as appropriate; and assist the sponsoring entity \nto develop a plan of action. (Such a plan is subject to the \nconcurrence of the OGC.) Further detail regarding EO \n12114 compliance requirements for NASA programs and \nprojects can be found in NPR 8580.1. \n\n7.5.2 PD/NSC-25\nNASA has procedural requirements for characterizing \nand reporting potential risks associated with a planned \nlaunch of radioactive materials into space, on launch ve-\nhicles and spacecraft, during normal or abnormal flight \nconditions. Procedures and levels of review and analysis \nrequired for nuclear launch safety approval vary with the \nquantity of radioactive material planned for use and po-\ntential risk to the general public and the environment. \n\nSpecific details concerning these requirements can be \nfound in NPR 8715.3, NASA General Safety Program Re-\nquirements.\n\nFor any U.S. space mission involving the use of radioiso-\ntope power systems, radioisotope heating units, nuclear \nreactors, or a major nuclear source, launch approval \nmust be obtained from the Office of the President per \nPresidential Directive/National Security Council Mem-\norandum No. 25 (PD/NSC-25), \u201cScientific or Techno-\nlogical Experiments with Possible Large-Scale Adverse \nEnvironmental Effects and Launch of Nuclear Systems \ninto Space,\u201d paragraph 9, as amended May 8, 1996. The \napproval decision is based on an established and proven \nreview process that includes an independent evaluation \nby an ad hoc Interagency Nuclear Safety Review Panel \n(INSRP) comprised of representatives from NASA, the \nDepartment of Energy (DOE), the Department of De-\nfense, and the Environmental Protection Agency, with \nan additional technical advisor from the Nuclear Regula-\ntory Commission. The process begins with development \nof a launch vehicle databook (i.e., a compendium of in-\nformation describing the mission, launch system, and \npotential accident scenarios including their environments \nand probabilities). DOE uses the databook to prepare a \nPreliminary Safety Analysis Report for the space mission. \nIn all, three Safety Analysis Reports (SARs) are typically \nproduced and submitted to the mission\u2019s INSRP\u2014the \nPSAR, an updated SAR (draft final SAR), and a final SAR. \nThe DOE project office responsible for providing the nu-\nclear power system develops these documents.\n\nThe ad hoc INSRP conducts its nuclear safety/risk eval-\nuation and documents their results in a nuclear Safety \nEvaluation Report (SER). The SER contains an indepen-\ndent evaluation of the mission radiological risk. DOE \nuses the SER as its basis for accepting the SAR. If the \nDOE Secretary formally accepts the SAR-SER package, \nit is forwarded to the NASA Administrator for use in the \nlaunch approval process.\n\nNASA distributes the SAR and SER to the other cogni-\nzant Government agencies involved in the INSRP, and \nsolicits their assessment of the documents. After re-\nceiving responses from these agencies, NASA conducts \ninternal management reviews to address the SAR and \nSER and any other nuclear safety information pertinent \nto the launch. If the NASA Administrator recommends \nproceeding with the launch, then a request for nuclear \nsafety launch approval is sent to the director of the Office \n\n\n\n258 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nof Science and Technology Policy (OSTP) within the Of-\nfice of the President.\n\nNASA HQ is responsible for implementing this process \nfor NASA missions. It has traditionally enlisted the Jet \nPropulsion Laboratory (JPL) to assist in this activity. \nDOE supports the process by analyzing the response \nof redundant power system hardware to the different \naccident scenarios identified in the databook and pre-\nparing a probabilistic risk assessment of the potential \nradiological consequences and risks to the public and \nthe environment for the mission. KSC is responsible for \noverseeing development of databooks and traditionally \nuses JPL to characterize accident environments and inte-\ngrate databooks. Both KSC and JPL subcontractors pro-\nvide information relevant to supporting the development \nof databooks. The development team ultimately selected \nfor a mission would be responsible for providing payload \ndescriptions, describing how the nuclear hardware inte-\ngrates into the spacecraft, describing the mission, and sup-\nporting KSC and JPL in their development of databooks. \n\nMission directorate associate administrators, Center Di-\nrectors, and program executives involved with the control \nand processing of radioactive materials for launch into \nspace must ensure that basic designs of vehicles, space-\ncraft, and systems utilizing radioactive materials provide \nprotection to the public, the environment, and users such \nthat radiation risk resulting from exposures to radioac-\ntive sources are as low as reasonably achievable. Nuclear \nsafety considerations must be incorporated from the Pre-\nPhase A concept study stage throughout all project stages \nto ensure that the overall mission radiological risk is ac-\nceptable. All space flight equipment (including medical \nand other experimental devices) that contain or use ra-\ndioactive materials must be identified and analyzed for \nradiological risk. Site-specific ground operations and ra-\ndiological contingency plans must be developed commen-\nsurate with the risk represented by the planned launch of \nnuclear materials. Contingency planning, as required by \nthe National Response Plan, includes provisions for emer-\ngency response and support for source recovery efforts. \nNPR 8710.1, Emergency Preparedness Program and NPR \n8715.2, NASA Emergency Preparedness Plan Procedural \nRequirements\u2014Revalidated address the NASA emergency \npreparedness policy and program requirements.\n\n7.5.3 Planetary Protection\nThe United States is a signatory to the United Nations\u2019 \nTreaty of Principles Governing the Activities of States \n\nin the Exploration and Use of Outer Space, Including \nthe Moon and Other Celestial Bodies. Known as the \nOuter Space Treaty, it states in part (Article IX) that ex-\nploration of the Moon and other celestial bodies shall \nbe conducted \u201cso as to avoid their harmful contami-\nnation and also adverse changes in the environment \nof the Earth resulting from the introduction of extra-\nterrestrial matter.\u201d NASA policy (NPD 8020.7, Biolog-\nical Contamination Control for Outbound and Inbound \nPlanetary Spacecraft) specifies that the purpose of pre-\nserving solar system conditions is for future biological \nand organic constituent exploration. This NPD also es-\ntablishes the basic NASA policy for the protection of \nthe Earth and its biosphere from planetary and other \nextraterrestrial sources of contamination. The general \nregulations to which NASA flight projects must adhere \nare set forth in NPR 8020.12, Planetary Protection Pro-\nvisions for Robotic Extraterrestrial Missions. Different \nrequirements apply to different missions, depending \non which solar system object is targeted or encoun-\ntered and the spacecraft or mission type (flyby, orbiter, \nlander, sample return, etc.). For some bodies (such as \nthe Sun, Moon, and Mercury), there are minimal plan-\netary protection requirements. Current requirements \nfor the outbound phase of missions to Mars and Eu-\nropa, however, are particularly rigorous. Table 7.5-\n1 shows the current planetary protection categories, \nwhile Table 7.5-2 provides a brief summary of their as-\nsociated requirements. \n\nAt the core, planetary protection is a project manage-\nment responsibility and a systems engineering activity. \nThe effort cuts across multiple WBS elements, and failure \nto adopt and incorporate a viable planetary protection \napproach during the early planning phases will add cost \nand complexity to the mission. Planning for planetary \nprotection begins in Phase A, during which feasibility of \nthe mission is established. Prior to the end of Phase A, \nthe project manager must send a letter to the Planetary \nProtection Officer (PPO) stating the mission type and \nplanetary targets and requesting that the mission be as-\nsigned a planetary protection category. \n\nPrior to the PDR, at the end of Phase B, the project man-\nager must submit to the NASA PPO a planetary protec-\ntion plan detailing the actions that will be taken to meet \nthe requirements. The project\u2019s progress and completion \nof the requirements are reported in a planetary protec-\ntion pre-launch report submitted to the NASA PPO for \napproval. The approval of this report at the FRR con-\n\n\n\n7.5 Environmental, Nuclear Safety, Planetary Protection, and Asset Protection Policy Compliance\n\nNASA Systems Engineering Handbook ? 259\n\nTable 7.5?1 Planetary Protection Mission Categories\n\nPlanet Priorities Mission Type Category Example\n\nNot of direct interest for understanding the \nprocess of chemical evolution. No protection of \nsuch planets is warranted (no requirements).\n\nAny I Lunar missions\n\nOf significant interest relative to the process \nof chemical evolution, but only a remote \nchance that contamination by spacecraft could \njeopardize future exploration.\n\nAny II Stardust (outbound)\n\nGenesis (outbound)\n\nCassini\n\nOf significant interest relative to the process \nof chemical evolution and/or the origin of life \nor for which scientific opinion provides a sig-\nnificant chance of contamination which could \njeopardize a future biological experiment.\n\nFlyby, Orbiter III Odyssey\n\nMars Global Surveyor\n\nMars Reconnaissance Orbiter\n\nLander, Probe IV Mars Exploration Rover\nPhoenix\nEuropa Explorer \nMars Sample Return \n(outbound)\n\nAny solar system body. Unrestricted Earth returna V Stardust (return)\n\nGenesis (return)\n\nRestricted Earth returnb V Mars Sample Return (return)\n\na. No special precautions needed for returning material/samples back to Earth. \n\nb. Special precautions need to be taken for returning material/samples back to Earth. See NPR 8020.12.\n\nTable 7.5?2 Summarized Planetary Protection Requirements\n\nMission Category Summarized Requirements\n\nI Certification of category.\n\nII Avoidance of accidental impact by spacecraft and launch vehicle. Documentation of final disposi-\ntion of launched hardware.\n\nIII Stringent limitations on the probability of impact. Requirements on orbital lifetime or require-\nments for microbial cleanliness of spacecraft.\n\nIV Stringent limitations on the probability of impact and/or the contamination of the object. Micro-\nbial cleanliness of landed hardware surfaces directly established by bioassays.\n\nV Outbound requirements per category of a lander mission to the target. Detailed restricted Earth \nreturn requirements will depend on many factors, but will likely include sterilization of any \nhardware that contacted the target planet before its return to Earth, and the containment of any \nreturned sample.\n\nstitutes the final planetary protection approval for the \nproject and must be obtained for permission to launch. \nAn update to this report, the planetary protection post-\nlaunch report, is prepared to report any deviations from \nthe planned mission due to actual launch or early mis-\nsion events. For sample return missions only, additional \n\nreports and reviews are required: prior to launch toward \nthe Earth, prior to commitment to Earth reentry, and \nprior to the release of any extraterrestrial sample to the \nscientific community for investigation. Finally, at the for-\nmally declared End of Mission (EOM), a planetary pro-\ntection EOM report is prepared. This document reviews \n\n\n\n260 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nthe entire history of the mission in comparison to the \noriginal planetary protection plan and documents the \ndegree of compliance with NASA\u2019s planetary protection \nrequirements. This document is typically reported on by \nthe NASA PPO at a meeting of the Committee on Space \nResearch (COSPAR) to inform other spacefaring nations \nof NASA\u2019s degree of compliance with international plan-\netary protection requirements.\n\n7.5.4 Space Asset Protection\nThe terrorist attacks on the World Trade Center in New \nYork and on the Pentagon on September 11, 2001 have \ncreated an atmosphere for greater vigilance on the part \nof Government agencies to ensure that sufficient secu-\nrity is in place to protect their personnel, physical assets, \nand information, especially those assets that contribute \nto the political, economic, and military capabilities of the \nUnited States. Current trends in technology proliferation, \naccessibility to space, globalization of space programs \nand industries, commercialization of space systems and \nservices, and foreign knowledge about U.S. space sys-\ntems increase the likelihood that vulnerable U.S. space \nsystems may come under attack, particularly vulnerable \nsystems. The ability to restrict or deny freedom of access \nto and operations in space is no longer limited to global \nmilitary powers. The reality is that there are many ex-\nisting capabilities to deny, disrupt, or physically destroy \norbiting spacecraft and the ground facilities that com-\nmand and control them. Knowledge of U.S. space sys-\ntems\u2019 functions, locations, and physical characteristics, \nas well as the means to conduct counterspace operations \nis increasingly available on the international market. Na-\ntions or groups hostile to the United States either possess \nor can acquire the means to disrupt or destroy U.S. space \nsystems by attacking satellites in space, their communi-\ncations nodes on the ground and in space, the ground \nnodes that command these satellites or process their \ndata, and/or the commercial infrastructure that supports \na space system\u2019s operations.\n\n7.5.4.1 Protection Policy\nThe new National Space Policy authorized by the Presi-\ndent on August 31, 2006, states that space capabilities are \nvital to the Nation\u2019s interests and that the United States \nwill \u201ctake those actions necessary to protect its space ca-\npabilities.\u201d The policy also gives responsibility for Space \nSituational Awareness (SSA) to the Secretary of Defense. \nIn that capacity the Secretary of Defense will conduct \n\nSSA for civil space capabilities and operations, particu-\nlarly human space flight activities. SSA provides an in-\ndepth knowledge and understanding of the threats posed \nto U.S., allied, and coalition space systems by adversaries \nand the environment, and is essential in developing and \nemploying protection measures. Therefore, NASA\u2019s space \nasset protection needs will drive the requirements that \nthe NASA levies on DOD for SSA.\n\n7.5.4.2 Goal\nThe overall space asset protection goal for NASA is to \nsupport sustained mission assurance through the reduc-\ntion of susceptibilities and the mitigation of vulnerabili-\nties, relative to risk, and within fiscal constraints.\n\n7.5.4.3 Scoping\nSpace asset protection involves the planning and imple-\nmentation of measures to protect NASA space assets \nfrom intentional or unintentional disruption, exploita-\ntion, or attack, whether natural or manmade. It is essen-\ntial that protection is provided for all segments of a space \nsystem (ground, communications/information, space, and \nlaunch) and covers the entire life cycle of a project. Space \nasset protection includes aspects of personnel, physical, \ninformation, communications, information technology, \nand operational security, as well as counterintelligence ac-\ntivities. The role of the systems engineer is to integrate se-\ncurity competencies with space systems engineering and \noperations expertise to develop mission protection strat-\negies consistent with payload classifications as defined in \nNPR 8705.4, Risk Classification for NASA Payloads.\n\n7.5.4.4 Protection Planning\nSystems engineers use protection planning processes and \nproducts (which include engineering trade studies and \ncost-benefit analyses) to meet NASA\u2019s needs for acquiring, \nfielding, and sustaining secure and uncompromised space \nsystems. Project protection plans are single-source docu-\nments that coordinate and integrate protection efforts and \nprevent inadvertent or uncontrolled disclosure of sensitive \nprogram information. Protection plans provide project \nmanagement personnel (project manager, project scien-\ntist, mission systems engineer, operations manager, user \ncommunity, etc.) with an overall view of the valid threats \nto a space system (both hostile and environmental), iden-\ntify infrastructure vulnerabilities, and propose security \ncountermeasures to mitigate risks and enhance surviv-\nability of the mission. An outline for a typical protection \nplan can be found in Appendix Q.\n\n\n\nNASA Systems Engineering Handbook ? 261\n\n7.6 Use of Metric System \n\nThe decision whether a project or program could or \nshould implement the System Internationale (SI), often \ncalled the \u201cmetric system,\u201d requires consideration of a \nnumber of factors, including cost, technical, risk, and \nother programmatic aspects. \n\nThe Metric Conversion Act of 1975 (Public Law 94-168) \namended by the Omnibus Trade and Competitiveness \nAct of 1988 (Public Law 100-418) establishes a national \ngoal of establishing the metric system as the preferred \nsystem of weights and measures for U.S. trade and com-\nmerce. NASA has developed NPD 8010.2, Use of the SI \n(Metric) System of Measurement in NASA Programs, \nwhich implements SI and provides specific requirements \nand responsibilities for NASA. \n\nHowever, a second factor to consider is that there are \npossible exceptions to the required implementation ap-\nproach. Both EO 12770 and NPD 8010.2 allow excep-\ntions and, because full SI implementation may be dif-\nficult, allow the use of \u201chybrid\u201d systems. Consideration \nof the following factors will have a direct impact on the \nimplementation approach and use of exceptions by the \nprogram or project.\n\nPrograms or projects must do analysis during the early \nlife-cycle phases when the design solutions are being de-\nveloped to identify where SI is feasible or recommended \nand where exceptions will be required. A major factor to \nconsider is the capability to actually produce or provide \nmetric-based hardware components. Results and recom-\nmendations from these analyses must be presented by \nSRR for approval.\n\nIn planning program or project implementation to pro-\nduce metric-based systems, issues to be addressed should \ninclude the following:\n\nInterfaces with heritage components (e.g., valves, py- ?\nrotechnic devices, etc.) built to English-based units:\n\nWhether conversion from English to SI and/or in- ?\nterface to English-based hardware is required. \nThe team should review design implementation to  ?\nensure there is no certification impact with heritage \nhardware or identify and plan for any necessary re-\ncertification efforts.\n\nDimensioning and tolerancing: ?\nCan result in parts that do not fit. ?\n\nRounding errors have occurred when converting  ?\nunits from one unit system to the other.\nThe team may require specific additional proce- ?\ndures, steps, and drawing Quality Assurance (QA) \npersonnel when converting units.\n\nTooling: ?\nNot all shops have full metric tooling (e.g., drill bits,  ?\ntaps, end mills, reamers, etc.).\nThe team needs to inform potential contractors of  ?\nintent to use SI and obtain feedback as to potential \nimpacts.\n\nFasteners and miscellaneous parts: ?\nHigh-strength fastener choices and availability are  ?\nmore limited in metric sizes.\nBearings, pins, rod ends, bushings, etc., are readily  ?\navailable in English with minimal lead times.\nThe team needs to ascertain availability of accept- ?\nable SI-based fasteners in the timeframe needed.\n\nReference material: ?\nSome key aerospace reference materials are built  ?\nonly in English units, e.g., MIL-HDBK-5 (metallic \nmaterial properties), and values will need to be con-\nverted when used.\nOther key reference materials or commercial data- ?\nbases are built only in SI units.\nThe team needs to review the reference material to  ?\nbe used and ensure acceptable conversion controls \nare in place, if necessary.\n\nCorporate knowledge: ?\nMany engineers presently think in English units,  ?\ni.e., can relate to pressure in PSI, can relate to mate-\nrial strength in KSI, can relate to a tolerance of 0.003 \ninches, etc.\nHowever, virtually all engineers coming out of  ?\nschool in this day and era presently think in SI \nunits and have difficulty relating to English-based \nunits such as slugs (for mass) and would require re-\ntraining with attendant increase in conversion er-\nrors. \nThe team needs to be aware of their program-  ?\nor project-specific knowledge in English and SI \nunits and obtain necessary training and experi-\nence.\n\n\n\n262 ? NASA Systems Engineering Handbook\n\n7.0 Special Topics\n\nIndustry practices: ?\nCertain industries work exclusively in English units,  ?\nand sometimes have their own jargon associated \nwith English material properties. The parachute \nindustry falls in this category, e.g., \u201c600-lb braided \nKevlar line.\u201d\nOther industries, especially international suppliers,  ?\nmay work exclusively in metric units, e.g., \u201c30-mm-\nthick raw bar stock.\u201d\nThe team needs to be aware of these unique cases  ?\nand ensure both procurement and technical design \nand integration have the appropriate controls to \navoid errors.\n\nProgram or project controls: The team needs to  ?\nconsider, early in the SE process, what program- or \nproject-specific risk management controls (such as \nconfiguration management steps) are required. This \nwill include such straightforward concerns as the \nconversion(s) between system elements that are in \nEnglish units and those in SI units or other, more \ncomplex, issues.\n\nSeveral NASA projects have taken the approach of using \nboth systems, which is allowed by NPD 8010.2. For ex-\nample, the Mars soil drill project designed and devel-\n\noped their hardware using English-based components, \nwhile accomplishing their analyses using SI-based units. \nOther small-scale projects have successfully used a sim-\nilar approach. \n\nFor larger or more dispersed projects or programs, a \nmore systematic and complete risk management ap-\nproach may be needed to successfully implement an SI-\nbased system. Such things as standard conversion factors \n(e.g., from pounds to kilograms) should be documented, \nas should standard SI nomenclature. Many of these risk \nmanagement aspects can be found in such documents \nas the National Institute of Standards and Technology\u2019s \nGuide for the Use of the International System of Units (SI) \nand the DOD Guide for Identification and Development \nof Metric Standards. \n\nUntil the Federal Government and the aerospace indus-\ntrial base are fully converted to an SI-based unit system, \nthe various NASA programs and projects will have to ad-\ndress their own level of SI implementation on a case-by-\ncase basis. It is the responsibility of each NASA program \nand project management team, however, to comply with \nall laws and executive orders while still maintaining a \nreasonable level of risk for cost, schedule, and perfor-\nmance.\n\n\n\nNASA Systems Engineering Handbook ? 263\n\nAppendix A: Acronyms\n\nACS Attitude Control Systems\nACWP Actual Cost of Work Performed\nAD2 Advance ment Degree of Difficulty Assess-\n\nment \nAHP Analytic Hierarchy Process\nAIAA American Institute of Aeronautics and As-\n\ntronautics\nAO  Announcement of Opportunity\nASME American Society of Mechanical Engineers\nBAC Budget at Completion\nBCWP Budgeted Cost for Work Performed\nBCWS Budgeted Cost for Work Scheduled\nC&DH Command and Data Han dling\nCACE Capability for Accelerated Concurrent Engi-\n\nneering\nCAIB Columbia Accident Investigation Board\nCAM Control Account Manager or Cost Account \n\nManager\nCCB Configuration Control Board\nCDR Critical Design Review\nCE Concurrent Engineering\nCERR Critical Event Readiness Review\nCI Configuration Item\nCM  Configuration Management\nCMC Center Management Council\nCMMI Capability Maturity Model\u00ae Integration\nCMO Configuration Management Organization\nCNSI Classified National Security Information\nCoF Construction of Facilities\nConOps Concept of Operations\nCOSPAR Committee on Space Research\nCOTS Commercial Off the Shelf\nCPI Critical Program Information or Cost Per-\n\nformance Index\nCRM Continuous Risk Management\nCSA Configuration Status Accounting\nCWBS Contract Work Breakdown Structure\nDCR Design Certification Review\nDGA Designated Governing Authority\nDLA Defense Logistics Agency\nDM Data Management\nDOD Department of Defense\nDOE Department of Energy\nDODAF DOD Architecture Framework\nDR Decommissioning Review\n\nDRM Design Reference Mission\nEAC Estimate at Completion\nECP Engineering Change Proposal\nECR Environmental Compliance and Restoration \n\nor Engineering Change Request\nEEE Electrical, Electronic, and Electromechanical\nEFFBD Enhanced Functional Flow Block Diagram \nEIA Electronic Industries Alliance\nEMC Electromagnetic Compatibility\nEMI Electromagnetic Interference\nEMO Environmental Management Office\nEO Executive Order\nEOM End of Mission\nEV Earned Value\nEVM Earned Value Management\nFAD Formulation Authorization Document\nFAR Federal Acquisition Requirement\nFCA Functional Configuration Audit\nFDIR Failure Detection, Isolation, And Recovery\nFFBD Functional Flow Block Diagram\nFMEA Failure Modes and Effects Analysis\nFMECA Failure Modes, Effects, and Criticality Anal-\n\nysis\nFMR Financial Management Requirements\nFRR Flight Readiness Review\nFS&GS Flight Systems and Ground Support\nGEO Geostationary\nGFP Government-Furnished Property\nGMIP Government Mandatory Inspection Point\nGPS Global Positioning Satellite \nHF Human Factors\nHQ Headquarters\nHQ/EMD NASA Headquar ters/Environmental Man-\n\nagement Division\nHWIL Hardware in the Loop\nICA Independent Cost Analysis\nICD Interface Control Document/Drawing\nICE Independent Cost Estimate\nICP Interface Control Plan\nIDD Interface Definition Document\nIEEE Institute of Electrical and Electronics Engi-\n\nneers\nILS Integrated Logistics Support\nINCOSE International Council on Systems Engi-\n\nneering\n\n\n\n264 ? NASA Systems Engineering Handbook\n\nAppendix A: Acronyms\n\nINSRP Interagency Nuclear Safety Review Panel\nIPT Integrated Product Team\nIRD Interface Requirements Document\nIRN Interface Revision Notice\nISO International Organization for Standardization\nIT Information Technology or Iteration\nITA Internal Task Agreement.\nITAR International Traffic in Arms Regulation\nI&V Integration and Verification\nIV&V Independent Verification and Validation\nIWG Interface Working Group\nJPL Jet Propulsion Laboratory\nKDP Key Decision Point\nKSC Kennedy Space Center\nLCCE Life-Cycle Cost Estimate\nLEO Low Earth Orbit or Low Earth Orbiting\nLLIL Limited Life Items List\nLLIS Lessons Learned Information System\nM&S Modeling and Simulation\nMAUT Multi-Attribute Utility Theory\nMCDA Multi-Criteria Decision Analysis\nMCR Mission Concept Review\nMDAA  Mission Directorate Associate Administrator\nMDR Mission Definition Review\nMOE  Measure of Effectiveness\nMOP  Measure of Performance\nMOU Memorandum of Understanding\nNASA National Aeronautics and Space Administra-\n\ntion\nNEDT NASA Ex ploration Design Team\nNEPA National Environmental Policy Act \nNFS NASA FAR Supple ment\nNODIS NASA On-Line Directives Information \n\nSystem \nNIAT NASA Integrated Action Team\nNOAA National Oceanic and Atmospheric Admin-\n\nistration\nNPD NASA Policy Directive\nNPR NASA Procedural Requirements\nOCE  Office of the Chief Engineer\nOGC Office of the General Counsel\nOMB Office of Management and Budget\nORR  Operational Readiness Review\nOSTP Office of Science and Technology Policy\nOTS Off-the-Shelf\nPAR Program Approval Review\nPBS Product Breakdown Structure\nPCA Physical Configuration Audit or Program \n\nCommitment Agreement\nPD/NSC Presidential Directive/National Security \n\nCouncil\nPDR  Preliminary Design Review\n\nPERT Program Evaluation and Review Technique\nPFAR Post-Flight Assessment Review\nPHA Preliminary Hazard Analysis\nPI Performance Index/Principal Investigator\nPIR Program Implementation Review\nPIRN Preliminary Interface Revision Notice\nPKI Public Key Infrastructure\nPLAR Post-Launch Assessment Review\nP(LOC) Probability of Loss of Crew\nP(LOM) Probability of Loss of Mission\nPMC Program Management Council\nPPBE Planning, Programming, Budgeting, and Ex-\n\necution\nPPO Planetary Protection Officer\nPQASP Program/Project Quality Assurance Surveil-\n\nlance Plan\nPRA Probabilistic Risk Assessment\nPRD Project Requirements Document\nPRR Production Readiness Review\nP/SDR Program/System Definition Review\nPSR Program Status Review\nP/SRR Program/System Requirements Review\nPTR Periodic Technical Reviews\nQA Quality Assurance\nR&T Research and Technology\nRF Radio Frequency\nRFA  Requests for Action\nRFI Request for Information\nRFP Request for Proposal\nRID Review Item Discrepancy\nSAR  System Acceptance Review or Safety Anal-\n\nysis Report\nSBU Sensitive But Unclassified\nSDR  System Definition Review\nSE Systems Engineering\nSEE Single-Event Effects\nSEMP  Systems Engineering Management Plan\nSER Safety Evaluation Report\nSI System Internationale (metric system)\nSIR System Integration Review\nSMA Safety and Mission Assurance\nSOW Statement of Work\nSP Special Publication\nSPI Schedule Performance Index\nSRB Standing Review Board\nSRD System Requirements Document\nSRR  System Requirements Review\nSSA Space Situational Awareness\nSTI Scientific and Technical Information\nSTS Space Transportation System\nSysML System Modeling Language\nT&E Test and Evaluation\n\n\n\nAppendix A: Acronyms\n\nNASA Systems Engineering Handbook ? 265\n\nTA Technology As sessment \nTBD To Be Determined\nTBR To Be Resolved\nTDRS Tracking and Data Relay Satellite\nTDRSS Tracking and Data Relay Satellite System\nTLA Timeline Analysis\nTLS Timeline Sheet \nTMA Technology Maturity Assessment\nTPM  Technical Performance Measure\n\nTRAR Technology Readiness Assessment Report \nTRL  Technology Readiness Level\nTRR  Test Readiness Review\nTVC Thrust Vector Controller\nUML Unified Modeling Language\nUSML United States Munitions List\nV&V Verification and Validation\nVAC Variance at Completion\nWBS Work Breakdown Structure\n\n\n\n266 ? NASA Systems Engineering Handbook\n\nAppendix B: Glossary\n\nTerm Definition/Context\n\nAcceptable Risk The risk that is understood and agreed to by the program/project, governing authority, mission \ndirectorate, and other customer(s) such that no further specific mitigating action is required.\n\nAcquisition The acquiring by contract with appropriated funds of supplies or services (including construction) by \nand for the use of the Government through purchase or lease, whether the supplies or services are al-\nready in existence or must be created, developed, demonstrated, and evaluated. Acquisition begins at \nthe point when Agency needs are established and includes the description of requirements to satisfy \nAgency needs, solicitation and selection of sources, award of contracts, contract financing, contract \nperformance, contract administration, and those technical and management functions directly related \nto the process of fulfilling Agency needs by contract.\n\nActivity (1) Any of the project components or research functions that are executed to deliver a product or ser-\nvice or provide support or insight to mature technologies. (2) A set of tasks that describe the technical \neffort to accomplish a process and help generate expected outcomes.\n\nAdvancement De-\ngree of Difficulty \nAssessment (AD2)\n\nThe process to develop an understanding of what is required to advance the level of system maturity.\n\nAllocated Base-\nline (Phase C)\n\nThe allocated baseline is the approved performance-oriented configuration documentation for a CI \nto be developed that describes the functional and interface characteristics that are allocated from a \nhigher level requirements document or a CI and the verification required to demonstrate achievement \nof those specified characteristics. The allocated baseline extends the top-level performance require-\nments of the functional baseline to sufficient detail for initiating manufacturing or coding of a CI. \nThe allocated baseline is controlled by the NASA. The allocated baseline(s) is typically established at \nthe Preliminary Design Review. Control of the allocated baseline would normally occur following the \nFunctional Configuration Audit.\n\nAnalysis Use of mathematical modeling and analytical techniques to predict the compliance of a design to \nits requirements based on calculated data or data derived from lower system structure end product \nvalidations.\n\nAnalysis of \nAlternatives\n\nA formal analysis method that compares alternative approaches by estimating their ability to satisfy \nmission requirements through an effectiveness analysis and by estimating their life-cycle costs \nthrough a cost analysis. The results of these two analyses are used together to produce a cost-effec-\ntiveness comparison that allows decisionmakers to assess the relative value or potential programmatic \nreturns of the alternatives.\n\nAnalytic Hierarchy \nProcess\n\nA multi-attribute methodology that provides a proven, effective means to deal with complex decision-\nmaking and can assist with identifying and weighting selection criteria, analyzing the data collected \nfor the criteria, and expediting the decisionmaking process.\n\nApproval Authorization by a required management official to proceed with a proposed course of action. Ap-\nprovals must be documented. \n\nApproval (for \nImplementation)\n\nThe acknowledgment by the decision authority that the program/project has met stakeholder \nexpectations and formulation requirements, and is ready to proceed to implementation. By approving \na program/project, the decision authority commits the budget resources necessary to continue into \nimplementation. \n\nAs-Deployed \nBaseline\n\nThe as-deployed baseline occurs at the Operational Readiness Review. At this point, the design is \nconsidered to be functional and ready for flight. All changes will have been incorporated into the \ndocumentation.\n\n\n\nAppendix B: Glossary\n\nNASA Systems Engineering Handbook ? 267\n\nTerm Definition/Context\n\nBaseline An agreed-to set of requirements, designs, or documents that will have changes controlled through a \nformal approval and monitoring process.\n\nBidirectional \nTraceability\n\nAn association among two or more logical entities that is discernible in either direction (i.e., to and \nfrom an entity).\n\nBrassboard A research configuration of a system, suitable for field testing, that replicates both the function and \nconfiguration of the operational systems with the exception of nonessential aspects such as packag-\ning.\n\nBreadboard A research configuration of a system, generally not suitable for field testing, that replicates both the \nfunction but not the actual configuration of the operational system and has major differences in \nactual physical layout.\n\nComponent \nFacilities\n\nComplexes that are geographically separated from the NASA Center or institution to which they are \nassigned.\n\nConcept of Op-\nerations (ConOps) \n(sometimes \nOperations \nConcept)\n\nThe ConOps describes how the system will be operated during the life-cycle phases to meet stake-\nholder expectations. It describes the system characteristics from an operational perspective and helps \nfacilitate an understanding of the system goals. It stimulates the development of the requirements \nand architecture related to the user elements of the system. It serves as the basis for subsequent \ndefinition documents and provides the foundation for the long-range operational planning activities.\n\nConcurrence A documented agreement by a management official that a proposed course of action is acceptable. \n\nConcurrent \nEngineering\n\nDesign in parallel rather than serial engineering fashion.\n\nConfiguration \nItems\n\nA Configuration Item is any hardware, software, or combination of both that satisfies an end use \nfunction and is designated for separate configuration management. Configuration items are typically \nreferred to by an alphanumeric identifier which also serves as the unchanging base for the assignment \nof serial numbers to uniquely identify individual units of the CI.\n\nConfiguration \nManagement \nProcess\n\nA process that is a management discipline that is applied over a product\u2019s life cycle to provide visibility \ninto and to control changes to performance and functional and physical characteristics. It ensures \nthat the configuration of a product is known and reflected in product information, that any product \nchange is beneficial and is effected without adverse consequences, and that changes are managed.\n\nContext Diagram A diagram that shows external systems that impact the system being designed.\n\nContinuous Risk \nManagement\n\nAn iterative process to refine risk management measures. Steps are to analyze risk, plan for tracking \nand control measures, track risk, carry out control measures, document and communicate all risk \ninformation, and deliberate throughout the process to refine it.\n\nContract A mutually binding legal relationship obligating the seller to furnish the supplies or services (including \nconstruction) and the buyer to pay for them. It includes all types of commitments that obligate the \nGovernment to an expenditure of appropriated funds and that, except as otherwise authorized, are in \nwriting.\n\nContractor An individual, partnership, company, corporation, association, or other service having a contract with \nthe Agency for the design, development, manufacture, maintenance, modification, operation, or \nsupply of items or services under the terms of a contract to a program or project.\n\nControl Account \nManager\n\nThe person responsible for controlling variances at the control account level, which is typically at the \nsubsystem WBS level. The CAM develops work and product plans, schedules, and time-phased re-\nsource plans. The technical subsystem manager/lead often takes on this role as part of their subsystem \nmanagement responsibilities.\n\nControl Gate (or \nmilestone)\n\nSee \u201cKey Decision Point.\u201d\n\nCost-Benefit \nAnalysis\n\nA methodology to determine the advantage of one alternative over another in terms of equivalent \ncost or benefits. It relies on totaling positive factors and subtracting negative factors to determine a \nnet result.\n\nCost-Effectiveness \nAnalysis\n\nA systematic quantitative method for comparing the costs of alternative means of achieving the same \nequivalent benefit for a specific objective.\n\n\n\n268 ? NASA Systems Engineering Handbook\n\nAppendix B: Glossary\n\nTerm Definition/Context\n\nCritical Design \nReview\n\nA review that demonstrates that the maturity of the design is appropriate to support proceeding \nwith full-scale fabrication, assembly, integration, and test, and that the technical effort is on track to \ncomplete the flight and ground system development and mission operations in order to meet mission \nperformance requirements within the identified cost and schedule constraints.\n\nCritical Event (or \nkey event)\n\nAn event that requires monitoring throughout the projected life cycle of a product that will generate \ncritical requirements that would affect system design, development, manufacture, test, and opera-\ntions (such as with an MOE, MOP, or TPM).\n\nCritical Event \nReadiness Review\n\nA review that confirms the project\u2019s readiness to execute the mission\u2019s critical activities during flight \noperation.\n\nCustomer The organization or individual that has requested a product and will receive the product to be \ndelivered. The customer may be an end user of the product, the acquiring agent for the end user, or \nthe requestor of the work products from a technical effort. Each product within the system hierarchy \nhas a customer.\n\nData Manage-\nment \n\nDM is used to plan for, acquire, access, manage, protect, and use data of a technical nature to support \nthe total life cycle of a system.\n\nDecision Analysis \nProcess\n\nA process that is a methodology for making decisions. It also offers techniques for modeling decision \nproblems mathematically and finding optimal decisions numerically. The methodology entails \nidentifying alternatives, one of which must be decided upon; possible events, one of which occurs \nthereafter; and outcomes, each of which results from a combination of decision and event.\n\nDecision Author-\nity\n\nThe Agency\u2019s responsible individual who authorizes the transition at a KDP to the next life-cycle phase \nfor a program/project.\n\nDecision Matrix A methodology for evaluating alternatives in which valuation criteria typically are displayed in rows \non the left side of the matrix, and alternatives are the column headings of the matrix. Criteria \u201cweights\u201d \nare typically assigned to each criterion.\n\nDecision Support \nPackage\n\nDocumentation submitted in conjunction with formal reviews and change requests.\n\nDecision Trees A portrayal of a decision model that displays the expected consequences of all decision alternatives \nby making discreet all \u201cchance\u201d nodes, and, based on this, calculating and appropriately weighting the \npossible consequences of all alternatives.\n\nDecommissioning \nReview\n\nA review that confirms the decision to terminate or decommission the system and assess the readi-\nness for the safe decommissioning and disposal of system assets. The DR is normally held near the \nend of routine mission operations upon accomplishment of planned mission objectives. It may be \nadvanced if some unplanned event gives rise to a need to prematurely terminate the mission, or \ndelayed if operational life is extended to permit additional investigations.\n\nDeliverable Data \nItem\n\nConsists of technical data\u2013requirements specifications, design documents, management data\u2013plans, \nand metrics reports.\n\nDemonstration Use of a realized end product to show that a set of stakeholder expectations can be achieved.\n\nDerived Require-\nments\n\nFor a program, requirements that are required to satisfy the directorate requirements on the program. \nFor a project, requirements that are required to satisfy the program requirements on the project. \n\nDescope Taken out of the scope of a project.\n\nDesign Solution \nDefinition Process\n\nThe process by which high-level requirements derived from stakeholder expectations and outputs of \nthe Logical Decomposition Process are translated into a design solution. \n\nDesignated Gov-\nerning Authority\n\nThe management entity above the program, project, or activity level with technical oversight respon-\nsibility.\n\nDoctrine of \nSuccessive \nRefinement\n\nA recursive and iterative design loop driven by the set of stakeholder expectations where a strawman \narchitecture/design, the associated ConOps, and the derived requirements are developed.\n\nEarned Value The sum of the budgeted cost for tasks and products that have actually been produced (completed or \nin progress) at a given time in the schedule.\n\n\n\nAppendix B: Glossary\n\nNASA Systems Engineering Handbook ? 269\n\nTerm Definition/Context\n\nEarned Value \nManagement\n\nA tool for measuring and assessing project performance through the integration of technical scope \nwith schedule and cost objectives during the execution of the project. EVM provides quantification of \ntechnical progress, enabling management to gain insight into project status and project completion \ncosts and schedules. Two essential characteristics of successful EVM are EVM system data integrity and \ncarefully targeted monthly EVM data analyses (i.e., risky WBS elements).\n\nEnabling Products The life-cycle support products and services (e.g., production, test, deployment, training, mainte-\nnance, and disposal) that facilitate the progression and use of the operational end product through \nits life cycle. Since the end product and its enabling products are interdependent, they are viewed as \na system. Project responsibility thus extends to responsibility for acquiring services from the relevant \nenabling products in each life-cycle phase. When a suitable enabling product does not already exist, \nthe project that is responsible for the end product may also be responsible for creating and using the \nenabling product.\n\nTechnical Cost \nEstimate\n\nThe cost estimate of the technical work on a project created by the technical team based on its \nunderstanding of the system requirements and operational concepts and its vision of the system \narchitecture.\n\nEnhanced Func-\ntional Flow Block \nDiagram\n\nA block diagram that represents control flows and data flows as well as system functions and flow.\n\nEntry Criteria Minimum accomplishments each project needs to fulfill to enter into the next life-cycle phase or level \nof technical maturity.\n\nEnvironmental \nImpact\n\nThe direct, indirect, or cumulative beneficial or adverse effect of an action on the environment. \n\nEnvironmental \nManagement\n\nThe activity of ensuring that program and project actions and decisions that potentially impact or \ndamage the environment are assessed and evaluated during the formulation and planning phase \nand reevaluated throughout implementation. This activity must be performed according to all NASA \npolicy and Federal, state, and local environmental laws and regulations. \n\nEstablish (with \nrespect to \nprocesses)\n\nThe act of developing policy, work instructions, or procedures to implement process activities.\n\nEvaluation The continual, independent (i.e., outside the advocacy chain of the program/project) evaluation of the \nperformance of a program or project and incorporation of the evaluation findings to ensure adequacy \nof planning and execution according to plan. \n\nExtensibility The ability of a decision to be extended to other applications.\n\nFlexibility The ability of a decision to support more than one current application.\n\nFlight Readiness \nReview\n\nA review that examines tests, demonstrations, analyses, and audits that determine the system\u2019s readi-\nness for a safe and successful flight/launch and for subsequent flight operations. It also ensures that all \nflight and ground hardware, software, personnel, and procedures are operationally ready.\n\nFlight Systems \nand Ground \nSupport\n\nFS&GS is one of four interrelated NASA product lines. FS&GS projects result in the most complex and \nvisible of NASA investments. To manage these systems, the Formulation and Implementation phases \nfor FS&GS projects follow the NASA project life-cycle model consisting of Phases A (concept develop-\nment) through F (closeout). Primary drivers for FS&GS projects are safety and mission success.\n\nFloat Extra time built into a schedule.\n\nFormulation \nPhase\n\nThe first part of the NASA management life cycle defined in NPR 7120.5 where system requirements \nare baselined, feasible concepts are determined, a system definition is baselined for the selected \nconcept(s), and preparation is made for progressing to the Implementation phase.\n\nFunctional \nAnalysis\n\nThe process of identifying, describing, and relating the functions a system must perform to fulfill its \ngoals and objectives.\n\nFunctional Base-\nline (Phase B)\n\nThe functional baseline is the approved configuration documentation that describes a system\u2019s or \ntop-level CIs\u2019 performance requirements (functional, interoperability, and interface characteristics) and \nthe verification required to demonstrate the achievement of those specified characteristics.\n\n\n\n270 ? NASA Systems Engineering Handbook\n\nAppendix B: Glossary\n\nTerm Definition/Context\n\nFunctional \nConfiguration \nAudit (FCA)\n\nExamines the functional characteristics of the configured product and verifies that the product has \nmet, via test results, the requirements specified in its functional baseline documentation approved at \nthe PDR and CDR. FCAs will be conducted on both hardware- or software-configured products and \nwill precede the PCA of the configured product.\n\nFunctional \nDecomposition\n\nA subfunction under logical decomposition and design solution definition, it is the examination of a \nfunction to identify subfunctions necessary for the accomplishment of that function and functional \nrelationships and interfaces.\n\nFunctional Flow \nBlock Diagram\n\nA block diagram that defines system functions and the time sequence of functional events.\n\nGantt Chart Bar chart depicting start and finish dates of activities and products in the WBS.\n\nGoal Quantitative and qualitative guidance on such things as performance criteria, technology gaps, \nsystem context, effectiveness, cost, schedule, and risk.\n\nGovernment \nMandatory \nInspection Points \n\nInspection points required by Federal regulations to ensure 100 percent compliance with safety/mis-\nsion-critical attributes when noncompliance can result in loss of life or loss of mission.\n\nHeritage (or \nlegacy)\n\nRefers to the original manufacturer\u2019s level of quality and reliability that is built into the parts which \nhave been proven by (1) time in service, (2) number of units in service, (3) mean time between failure \nperformance, and (4) number of use cycles.\n\nHuman Factors \nEngineering\n\nThe discipline that studies human-system interfaces and provides requirements, standards, and \nguidelines to ensure the human component of an integrated system is able to function as intended.\n\nImplementation \nPhase\n\nThe part of the NASA management life cycle defined in NPR 7120.5 where the detailed design of \nsystem products is completed and the products to be deployed are fabricated, assembled, integrated, \nand tested and the products are deployed to their customers or users for their assigned use or mis-\nsion.\n\nIncommensu-\nrable Costs\n\nCosts that cannot be easily measured, such as controlling pollution on launch or mitigating debris.\n\nInfluence \nDiagram \n\nA compact graphical and mathematical representation of a decision state.\n\nInspection Visual examination of a realized end product to validate physical design features or specific manufac-\nturer identification.\n\nIntegrated Logis-\ntics Support\n\nActivities within the SE process that ensure the product system is supported during development \n(Phase D) and operations (Phase E) in a cost-effective manner. This is primarily accomplished by early, \nconcurrent consideration of supportability characteristics, performing trade studies on alternative \nsystem and ILS concepts, quantifying resource requirements for each ILS element using best-practice \ntechniques, and acquiring the support items associated with each ILS element.\n\nInterface Man-\nagement Process\n\nThe process to assist in controlling product development when efforts are divided among parties \n(e.g., Government, contractors, geographically diverse technical teams) and/or to define and maintain \ncompliance among the products that must interoperate.\n\nIterative Application of a process to the same product or set of products to correct a discovered discrepancy or \nother variation from requirements. (See \u201cRecursive\u201d and \u201cRepeatable.\u201d)\n\nKey Decision \nPoint (or mile-\nstone)\n\nThe event at which the decision authority determines the readiness of a program/project to progress \nto the next phase of the life cycle (or to the next KDP).\n\nKey Event See \u201cCritical Event.\u201d\n\nKnowledge \nManagement\n\nGetting the right information to the right people at the right time without delay while helping people \ncreate knowledge and share and act upon information in ways that will measurably improve the \nperformance of NASA and its partners.\n\nLeast-Cost \nAnalysis\n\nA methodology that identifies the least-cost project option for meeting the technical requirements.\n\n\n\nAppendix B: Glossary\n\nNASA Systems Engineering Handbook ? 271\n\nTerm Definition/Context\n\nLiens Requirements or tasks not satisfied that have to be resolved within a certain assigned time to allow \npassage through a control gate to proceed.\n\nLife-Cycle Cost The total cost of ownership over the project\u2019s or system\u2019s life cycle from Formulation through \nImplementation. The total of the direct, indirect, recurring, nonrecurring, and other related expenses \nincurred, or estimated to be incurred, in the design, development, verification, production, deploy-\nment, operation, maintenance, support, and disposal of a project.\n\nLogical Decom-\nposition Models\n\nRequirements decomposed by one or more different methods (e.g., function, time, behavior, data \nflow, states, modes, system architecture).\n\nLogical Decom-\nposition Process\n\nThe process for creating the detailed functional requirements that enable NASA programs and \nprojects to meet the ends desired by Agency stakeholders. This process identifies the \u201cwhat\u201d that must \nbe achieved by the system at each level to enable a successful project. It utilizes functional analysis to \ncreate a system architecture and to decompose top-level (or parent) requirements and allocate them \ndown to the lowest desired levels of the project.\n\nLogistics The management, engineering activities, and analysis associated with design requirements defini-\ntion, material procurement and distribution, maintenance, supply replacement, transportation, and \ndisposal that are identified by space flight and ground systems supportability objectives. \n\nMaintain (with \nrespect to \nestablishment of \nprocesses)\n\nThe act of planning the process, providing resources, assigning responsibilities, training people, man-\naging configurations, identifying and involving stakeholders, and monitoring process effectiveness.\n\nMaintainability The measure of the ability of an item to be retained in or restored to specified conditions when \nmaintenance is performed by personnel having specified skill levels, using prescribed procedures and \nresources, at each prescribed level of maintenance.\n\nMargin The allowances carried in budget, projected schedules, and technical performance parameters (e.g., \nweight, power, or memory) to account for uncertainties and risks. Margin allocations are baselined in \nthe Formulation process, based on assessments of risks, and are typically consumed as the program/\nproject proceeds through the life cycle. \n\nMeasure of \nEffectiveness\n\nA measure by which a stakeholder\u2019s expectations will be judged in assessing satisfaction with prod-\nucts or systems produced and delivered in accordance with the associated technical effort. The MOE is \ndeemed to be critical to not only the acceptability of the product by the stakeholder but also critical to \noperational/mission usage. An MOE is typically qualitative in nature or not able to be used directly as a \ndesign-to requirement.\n\nMeasure of \nPerformance\n\nA quantitative measure that, when met by the design solution, will help ensure that an MOE for a \nproduct or system will be satisfied. These MOPs are given special attention during design to ensure \nthat the MOEs to which they are associated are met. There are generally two or more measures of \nperformance for each MOE.\n\nMetric The result of a measurement taken over a period of time that communicates vital information about \nthe status or performance of a system, process, or activity. A metric should drive appropriate action.\n\nMission A major activity required to accomplish an Agency goal or to effectively pursue a scientific, techno-\nlogical, or engineering opportunity directly related to an Agency goal. Mission needs are independent \nof any particular system or technological solution. \n\nMission Concept \nReview\n\nA review that affirms the mission need and examines the proposed mission\u2019s objectives and the \nconcept for meeting those objectives. It is an internal review that usually occurs at the cognizant \norganization for system development.\n\nMission Definition \nReview\n\nA review that examines the functional and performance requirements defined for the system and the \npreliminary program or project plan and ensures that the requirements and the selected concept will \nsatisfy the mission.\n\nNASA Life-\nCycle Phases (or \nprogram life-cycle \nphases)\n\nConsists of Formulation and Implementation phases as defined in NPR 7120.5.\n\n\n\n272 ? NASA Systems Engineering Handbook\n\nAppendix B: Glossary\n\nTerm Definition/Context\n\nObjective Func-\ntion (sometimes \nCost Function)\n\nA mathematical expression that expresses the values of combinations of possible outcomes as a single \nmeasure of cost-effectiveness.\n\nOperational \nReadiness Review\n\nA review that examines the actual system characteristics and the procedures used in the system or \nproduct\u2019s operation and ensures that all system and support (flight and ground) hardware, software, \npersonnel, procedures, and user documentation accurately reflects the deployed state of the system.\n\nOptimal Solution A feasible solution that minimizes (or maximizes, if that is the goal) an objective function.\n\nOther Interested \nParties (Stake-\nholders)\n\nA subset of \u201cstakeholders,\u201d other interested parties are groups or individuals who are not customers \nof a planned technical effort but may be affected by the resulting product, the manner in which the \nproduct is realized or used, or have a responsibility for providing life-cycle support services.\n\nPeer Review Independent evaluation by internal or external subject matter experts who do not have a vested \ninterest in the work product under review. Peer reviews can be planned, focused reviews conducted \non selected work products by the producer\u2019s peers to identify defects and issues prior to that work \nproduct moving into a milestone review or approval cycle.\n\nPerformance \nIndex \n\nAn overall measure of effectiveness for each alternative.\n\nPerformance \nStandards\n\nCommon metrics for use in performance standards include cost and schedule.\n\nPhysical Configu-\nration Audits (or \nconfiguration \ninspection)\n\nThe PCA examines the physical configuration of the configured product and verifies that the product \ncorresponds to the build-to (or code-to) product baseline documentation previously approved at the \nCDR. PCAs will be conducted on both hardware- and software-configured products.\n\nPost-Flight As-\nsessment Review\n\nA review that evaluates the activities from the flight after recovery. The review identifies all anomalies \nthat occurred during the flight and mission and determines the actions necessary to mitigate or \nresolve the anomalies for future flights.\n\nPost-Launch As-\nsessment Review\n\nA review that evaluates the status, performance, and capabilities of the project evident from the flight \noperations experience since launch. This can also mean assessing readiness to transfer responsibility \nfrom the development organization to the operations organization. The review also evaluates the \nstatus of the project plans and the capability to conduct the mission with emphasis on near-term \noperations and mission-critical events. This review is typically held after the early flight operations and \ninitial checkout.\n\nPrecedence \nDiagram\n\nWorkflow diagram that places activities in boxes, connected by dependency arrows; typical of a Gantt \nchart.\n\nPreliminary \nDesign Review\n\nA review that demonstrates that the preliminary design meets all system requirements with accept-\nable risk and within the cost and schedule constraints and establishes the basis for proceeding with \ndetailed design. It will show that the correct design option has been selected, interfaces have been \nidentified, and verification methods have been described.\n\nProcess A set of activities used to convert inputs into desired outputs to generate expected outcomes and \nsatisfy a purpose.\n\nProducibility A system characteristic associated with the ease and economy with which a completed design can be \ntransformed (i.e., fabricated, manufactured, or coded) into a hardware and/or software realization.\n\nProduct A part of a system consisting of end products that perform operational functions and enabling \nproducts that perform life-cycle services related to the end product or a result of the technical efforts \nin the form of a work product (e.g., plan, baseline, or test result).\n\nProduct Baseline \n(Phase D/E)\n\nThe product baseline is the approved technical documentation that describes the configuration of \na CI during the production, fielding/deployment, and operational support phases of its life cycle. \nThe product baseline describes detailed physical or form, fit, and function characteristics of a CI; the \nselected functional characteristics designated for production acceptance testing; the production \nacceptance test requirements.\n\n\n\nAppendix B: Glossary\n\nNASA Systems Engineering Handbook ? 273\n\nTerm Definition/Context\n\nProduct Break-\ndown Structure\n\nA hierarchical breakdown of the hardware and software products of the program/project.\n\nProduct \nImplementation \nProcess\n\nThe first process encountered in the SE engine, which begins the movement from the bottom of the \nproduct hierarchy up toward the Product Transition Process. This is where the plans, designs, analysis, \nrequirement development, and drawings are realized into actual products.\n\nProduct Integra-\ntion Process\n\nOne of the SE engine product realization processes that make up the system structure. In this process, \nlower level products are assembled into higher level products and checked to make sure that the \nintegrated product functions properly. It is the first element of the processes that lead from realized \nproducts from a level below to realized end products at a level above, between the Product Imple-\nmentation, Verification, and Validation Processes.\n\nProduct Realiza-\ntion\n\nThe act of making, buying, or reusing a product, or the assembly and integration of lower level real-\nized products into a new product, as well as the verification and validation that the product satisfies its \nappropriate set of requirements and the transition of the product to its customer.\n\nProduct Transition \nProcess\n\nA process used to transition a verified and validated end product that has been generated by product \nimplementation or product integration to the customer at the next level in the system structure for \nintegration into an end product or, for the top-level end product, transitioned to the intended end \nuser.\n\nProduct Valida-\ntion Process\n\nThe second of the verification and validation processes conducted on a realized end product. While \nverification proves whether \u201cthe system was done right,\u201d validation proves whether \u201cthe right system \nwas done.\u201d In other words, verification provides objective evidence that every \u201cshall\u201d was met, whereas \nvalidation is performed for the benefit of the customers and users to ensure that the system functions \nin the expected manner when placed in the intended environment. This is achieved by examining the \nproducts of the system at every level of the structure.\n\nProduct Verifica-\ntion Process\n\nThe first of the verification and validation processes conducted on a realized end product. As used in \nthe context of systems engineering common technical processes, a realized product is one provided \nby either the Product Implementation Process or the Product Integration Process in a form suitable for \nmeeting applicable life-cycle phase success criteria.\n\nProduction Readi-\nness Review\n\nA review that is held for FS&GS projects developing or acquiring multiple or similar systems greater \nthan three or as determined by the project. The PRR determines the readiness of the system develop-\ners to efficiently produce the required number of systems. It ensures that the production plans; \nfabrication, assembly, and integration-enabling products; and personnel are in place and ready to \nbegin production.\n\nProgram A strategic investment by a mission directorate (or mission support office) that has defined goals, ob-\njectives, architecture, funding level, and a management structure that supports one or more projects.\n\nProgram/System \nDefinition Review \n\nA review that examines the proposed program architecture and the flowdown to the functional \nelements of the system. The proposed program\u2019s objectives and the concept for meeting those \nobjectives are evaluated. Key technologies and other risks are identified and assessed. The baseline \nprogram plan, budgets, and schedules are presented.\n\nProgram/System \nRequirements \nReview\n\nA review that is used to ensure that the program requirements are properly formulated and correlated \nwith the Agency and mission directorate strategic objectives.\n\nProgrammatic \nRequirements\n\nRequirements set by the mission directorate, program, project, and PI, if applicable. These include \nstrategic scientific and exploration requirements, system performance requirements, and schedule, \ncost, and similar nontechnical constraints. \n\nProject (1) A specific investment having defined goals, objectives, requirements, life-cycle cost, a beginning, \nand an end. A project yields new or revised products or services that directly address NASA\u2019s strategic \nneeds. They may be performed wholly in-house; by Government, industry, academia partnerships; \nor through contracts with private industry. (2) A unit of work performed in programs, projects, and \nactivities.\n\nProject Plan The document that establishes the project\u2019s baseline for implementation, signed by the cognizant \nprogram manager, Center Director, project manager, and the MDAA, if required. \n\n\n\n274 ? NASA Systems Engineering Handbook\n\nAppendix B: Glossary\n\nTerm Definition/Context\n\nProject Technical \nTeam\n\nThe whole technical team for the project.\n\nSolicitation The vehicle by which information is solicited from contractors to let a contract for products or services.\n\nPrototype Items (mockups, models) built early in the life cycle that are made as close to the flight item in form, fit, \nand function as is feasible at that stage of the development. The prototype is used to \u201cwring out\u201d the \ndesign solution so that experience gained from the prototype can be fed back into design changes \nthat will improve the manufacture, integration, and maintainability of a single flight item or the \nproduction run of several flight items.\n\nQuality Assurance An independent assessment needed to have confidence that the system actually produced and \ndelivered is in accordance with its functional, performance, and design requirements. \n\nRealized Product The desired output from the application of the four product realization processes. The form of this \nproduct is dependent on the phase of the product-line life cycle and the phase success criteria.\n\nRecursive Value is added to the system by the repeated application of processes to design next lower layer sys-\ntem products or to realize next upper layer end products within the system structure. This also applies \nto repeating application of the same processes to the system structure in the next life-cycle phase to \nmature the system definition and satisfy phase exit criteria.\n\nRelevant Stake-\nholder\n\nSee \u201cStakeholder.\u201d\n\nReliability The measure of the degree to which a system ensures mission success by functioning properly over \nits intended life. It has a low and acceptable probability of failure, achieved through simplicity, proper \ndesign, and proper application of reliable parts and materials. In addition to long life, a reliable system \nis robust and fault tolerant.\n\nRepeatable A characteristic of a process that can be applied to products at any level of the system structure or \nwithin any life-cycle phase.\n\nRequirement The agreed-upon need, desire, want, capability, capacity, or demand for personnel, equipment, facili-\nties, or other resources or services by specified quantities for specific periods of time or at a specified \ntime expressed as a \u201cshall\u201d statement. Acceptable form for a requirement statement is individually \nclear, correct, feasible to obtain, unambiguous in meaning, and can be validated at the level of the \nsystem structure at which stated. In pairs of requirement statements or as a set, collectively, they are \nnot redundant, are adequately related with respect to terms used, and are not in conflict with one \nanother.\n\nRequirements \nAllocation Sheet\n\nDocuments the connection between allocated functions, allocated performance, and the physical \nsystem.\n\nRequirements \nManagement \nProcess\n\nA process that applies to the management of all stakeholder expectations, customer requirements, \nand technical product requirements down to the lowest level product component requirements.\n\nRisk The combination of the probability that a program or project will experience an undesired event \n(some examples include a cost overrun, schedule slippage, safety mishap, health problem, malicious \nactivities, environmental impact, or failure to achieve a needed scientific or technological break-\nthrough or mission success criteria) and the consequences, impact, or severity of the undesired event, \nwere it to occur. Both the probability and consequences may have associated uncertainties.\n\nRisk Assessment An evaluation of a risk item that determines (1) what can go wrong, (2) how likely is it to occur, \n(3) what the consequences are, and (4) what are the uncertainties associated with the likelihood and \nconsequences. \n\nRisk Management An organized, systematic decisionmaking process that efficiently identifies, analyzes, plans, tracks, \ncontrols, communicates, and documents risk and establishes mitigation approaches and plans to \nincrease the likelihood of achieving program/project goals. \n\nRisk-Informed \nDecision Analysis \nProcess\n\nA five-step process focusing first on objectives and next on developing decision alternatives with \nthose objectives clearly in mind and/or using decision alternatives that have been developed under \nother systems engineering processes. The later steps of the process interrelate heavily with the Techni-\ncal Risk Management Process.\n\n\n\nAppendix B: Glossary\n\nNASA Systems Engineering Handbook ? 275\n\nTerm Definition/Context\n\nSafety Freedom from those conditions that can cause death, injury, occupational illness, damage to or loss of \nequipment or property, or damage to the environment.\n\nSearch Space (or \nAlternative Space)\n\nThe envelope of concept possibilities defined by design constraints and parameters within which \nalternative concepts can be developed and traded off.\n\nSoftware As defined in NPD 2820.1, NASA Software Policy.\n\nSpecification A document that prescribes completely, precisely, and verifiably the requirements, design, behavior, \nor characteristics of a system or system component.\n\nStakeholder A group or individual who is affected by or is in some way accountable for the outcome of an under-\ntaking. The term \u201crelevant stakeholder\u201d is a subset of the term \u201cstakeholder\u201d and describes the people \nidentified to contribute to a specific task. There are two main classes of stakeholders. See \u201cCustomers\u201d \nand \u201cOther Interested Parties.\u201d\n\nStakeholder \nExpectations \n\nA statement of needs, desires, capabilities, and wants that are not expressed as a requirement (not \nexpressed as a \u201cshall\u201d statement) is to be referred to as an \u201cexpectation.\u201d Once the set of expectations \nfrom applicable stakeholders is collected, analyzed, and converted into a \u201cshall\u201d statement, the \nexpectation becomes a requirement. Expectations can be stated in either qualitative (nonmeasurable) \nor quantitative (measurable) terms. Requirements are always stated in quantitative terms. Expecta-\ntions can be stated in terms of functions, behaviors, or constraints with respect to the product being \nengineered or the process used to engineer the product.\n\nStakeholder \nExpectations \nDefinition Process\n\nThe initial process within the SE engine that establishes the foundation from which the system is de-\nsigned and the product realized. The main purpose of this process is to identify who the stakeholders \nare and how they intend to use the product. This is usually accomplished through use-case scenarios, \ndesign reference missions, and operational concepts.\n\nStanding Review \nBoard\n\nThe entity responsible for conducting independent reviews of the program/project per the life-cycle \nrequirements. The SRB is advisory and is chartered to objectively assess the material presented by the \nprogram/project at a specific review. \n\nState Diagram A diagram that shows the flow in the system in response to varying inputs.\n\nSuccess Criteria Specific accomplishments that must be satisfactorily demonstrated to meet the objectives of a \ntechnical review so that a technical effort can progress further in the life cycle. Success criteria are \ndocumented in the corresponding technical review plan.\n\nSurveillance \n(or Insight or \nOversight)\n\nThe monitoring of a contractor\u2019s activities (e.g., status meetings, reviews, audits, site visits) for progress \nand production and to demonstrate fiscal responsibility, ensure crew safety and mission success, and \ndetermine award fees for extraordinary (or penalty fees for substandard) contract execution. \n\nSystem (1) The combination of elements that function together to produce the capability to meet a need. The \nelements include all hardware, software, equipment, facilities, personnel, processes, and procedures \nneeded for this purpose. (2) The end product (which performs operational functions) and enabling \nproducts (which provide life-cycle support services to the operational end products) that make up a \nsystem.\n\nSystem Accep-\ntance Review\n\nA review that verifies the completeness of the specific end item with respect to the expected maturity \nlevel and to assess compliance to stakeholder expectations. The SAR examines the system, its end \nitems and documentation, and test data and analyses that support verification and validation. It also \nensures that the system has sufficient technical maturity to authorize its shipment to the designated \noperational facility or launch site.\n\nSystem Definition \nReview\n\nA review that examines the proposed system architecture/design and the flowdown to all functional \nelements of the system.\n\nSystem Integra-\ntion Review\n\nA review that ensures that the system is ready to be integrated; segments, components, and \nsubsystems are available and ready to be integrated; and integration facilities, support personnel, and \nintegration plans and procedures are ready for integration. SIR is conducted at the end of the final \ndesign phase (Phase C) and before the systems assembly, integration, and test phase (Phase D) begins.\n\nSystem Require-\nments Review\n\nA review that examines the functional and performance requirements defined for the system and the \npreliminary program or project plan and ensures that the requirements and the selected concept will \nsatisfy the mission.\n\n\n\n276 ? NASA Systems Engineering Handbook\n\nAppendix B: Glossary\n\nTerm Definition/Context\n\nSystem Safety \nEngineering \n\nThe application of engineering and management principles, criteria, and techniques to achieve ac-\nceptable mishap risk within the constraints of operational effectiveness and suitability, time, and cost, \nthroughout all phases of the system life cycle.\n\nSystem Structure A system structure is made up of a layered structure of product-based WBS models. (See \u201cWork \nBreakdown Structure.\u201d)\n\nSystems Analysis The analytical process by which a need is transformed into a realized, definitive product, able to \nsupport compatibility with all physical and functional requirements and support the operational \nscenarios in terms of reliability, maintainability, supportability, serviceability, and disposability, \nwhile maintaining performance and affordability. Systems analysis is responsive to the needs of the \ncustomer at every phase of the life cycle, from pre-Phase A to realizing the final product and beyond.\n\nSystems Ap-\nproach \n\nThe application of a systematic, disciplined engineering approach that is quantifiable, recursive, \niterative, and repeatable for the development, operation, and maintenance of systems integrated into \na whole throughout the life cycle of a project or program.\n\nSystems Engi-\nneering Engine\n\nThe technical processes framework for planning and implementing the technical effort within \nany phase of a product-line life cycle. The SE engine model in Figure 2.1-1 shows the 17 technical \nprocesses that are applied to products being engineered to drive the technical effort.\n\nSystems Engi-\nneering Manage-\nment Plan\n\nThe SEMP identifies the roles and responsibility interfaces of the technical effort and how those \ninterfaces will be managed. The SEMP is the vehicle that documents and communicates the technical \napproach, including the application of the common technical processes; resources to be used; and key \ntechnical tasks, activities, and events along with their metrics and success criteria.\n\nTailoring The documentation and approval of the adaptation of the process and approach to complying with \nrequirements underlying the specific program or project.\n\nTechnical Assess-\nment Process\n\nThe crosscutting process used to help monitor technical progress of a program/project through \nperiodic technical reviews. It also provides status information in support of assessing system design, \nproduct realization, and technical management decisions.\n\nTechnical Data \nManagement \nProcess\n\nThe process used to plan for, acquire, access, manage, protect, and use data of a technical nature to \nsupport the total life cycle of a system. This includes its development, deployment, operations and \nsupport, eventual retirement, and retention of appropriate technical data beyond system retirement \nas required by current NASA policies.\n\nTechnical Data \nPackage\n\nAn output of the Design Solution Definition Process, it evolves from phase to phase, starting with con-\nceptual sketches or models and ending with complete drawings, parts list, and other details needed \nfor product implementation or product integration.\n\nTechnical \nMeasures\n\nAn established set of measures based on the expectations and requirements that will be tracked and \nassessed to determine overall system or product effectiveness and customer satisfaction. Common \nterms for these measures are MOEs, MOPs, and TPMs. \n\nTechnical Perfor-\nmance Measures \n\nThe set of critical or key performance parameters that are monitored by comparing the current actual \nachievement of the parameters with that anticipated at the current time and on future dates. Used \nto confirm progress and identify deficiencies that might jeopardize meeting a system requirement. \nAssessed parameter values that fall outside an expected range around the anticipated values indicate \na need for evaluation and corrective action. Technical performance measures are typically selected \nfrom the defined set of MOPs.\n\nTechnical Plan-\nning Process\n\nThe first of the eight technical management processes contained in the SE engine, the Technical Plan-\nning Process establishes a plan for applying and managing each of the common technical processes \nthat will be used to drive the development of system products and associated work products. This \nprocess also establishes a plan for identifying and defining the technical effort required to satisfy the \nproject objectives and life-cycle-phase success criteria within the cost, schedule, and risk constraints \nof the project.\n\nTechnical \nRequirements \nDefinition Process\n\nThe process used to transform the stakeholder expectations into a complete set of validated technical \nrequirements expressed as \u201cshall\u201d statements that can be used for defining a design solution for the \nPBS model and related enabling products.\n\n\n\nAppendix B: Glossary\n\nNASA Systems Engineering Handbook ? 277\n\nTerm Definition/Context\n\nTechnical Risk Risk associated with the achievement of a technical goal, criterion, or objective. It applies to undesired \nconsequences related to technical performance, human safety, mission assets, or environment.\n\nTechnical Risk \nManagement \nProcess\n\nThe process for measuring or assessing risk and developing strategies to manage it, an important \ncomponent of managing NASA programs under its charter to explore and expand knowledge. Critical \nto this process is the proactive identification and control of departures from the baseline program, \nproject, or activity.\n\nTechnical Team A group of multidisciplinary individuals with appropriate domain knowledge, experience, competen-\ncies, and skills assigned to a specific technical task.\n\nTechnology  \nReadiness Assess-\nment Report \n\nA document required for transition from Phase B to Phase C/D demonstrating that all systems, subsys-\ntems, and components have achieved a level of technological maturity with demonstrated evidence \nof qualification in a relevant environment.\n\nTechnology \nAssessment\n\nA systematic process that ascertains the need to develop or infuse technological advances into a \nsystem. The technology assessment process makes use of basic systems engineering principles and \nprocesses within the framework of the PBS. It is a two-step process comprised of (1) the determination \nof the current technological maturity in terms of technology readiness levels and (2) the determina-\ntion of the difficulty associated with moving a technology from one TRL to the next through the use of \nthe AD2. \n\nTechnology \nDevelopment \nPlan\n\nA document required for transition from Phase A to Phase B identifying technologies to be developed, \nheritage systems to be modified, alternative paths to be pursued, fallback positions and correspond-\ning performance descopes, milestones, metrics, and key decision points. It is incorporated in the \npreliminary project plan.\n\nTechnology Matu-\nrity Assessment\n\nThe process to determine a system\u2019s technological maturity via TRLs.\n\nTechnology \nReadiness Level \n\nProvides a scale against which to measure the maturity of a technology. TRLs range from 1, Basic \nTechnology Research, to 9, Systems Test, Launch, and Operations. Typically, a TRL of 6 (i.e., technology \ndemonstrated in a relevant environment) is required for a technology to be integrated into an SE \nprocess.\n\nTest The use of a realized end product to obtain detailed data to validate performance or to provide suf-\nficient information to validate performance through further analysis.\n\nTest Readiness \nReview\n\nA review that ensures that the test article (hardware/software), test facility, support personnel, and test \nprocedures are ready for testing and data acquisition, reduction, and control.\n\nTraceability A discernible association among two or more logical entities such as requirements, system elements, \nverifications, or tasks. \n\nTrade Study A means of evaluating system designs by devising alternative means to meet functional requirements, \nevaluating these alternatives in terms of the measures of effectiveness and system cost, ranking the \nalternatives according to appropriate selection criteria, dropping less promising alternatives, and \nproceeding to the next level of resolution, if needed.\n\nTrade Study \nReport\n\nA report written to document a trade study. It should include: he system under analysis; system goals, \nobjectives (or requirements, as appropriate to the level of resolution), and constraints; measures and \nmeasurement methods (models) used; all data sources used; the alternatives chosen for analysis; \ncomputational results, including uncertainty ranges and sensitivity analyses performed; the selection \nrule used; and the recommended alternative.\n\nTrade Tree A representation of trade study alternatives in which each layer represents some system aspect that \nneeds to be treated in a trade study to determine the best alternative.\n\nTransition The act of delivery or moving of a product from the location where the product has been imple-\nmented or integrated, as well as verified and validated, to a customer. This act can include packaging, \nhandling, storing, moving, transporting, installing, and sustainment activities.\n\nUtility A measure of the relative value gained from an alternative. The theoretical unit of measurement for \nutility is the util.\n\n\n\n278 ? NASA Systems Engineering Handbook\n\nAppendix B: Glossary\n\nTerm Definition/Context\n\nValidated \nRequirements \n\nA set of requirements that are well formed (clear and unambiguous), complete (agree with customer \nand stakeholder needs and expectations), consistent (conflict free), and individually verifiable and \ntraceable to a higher level requirement or goal.\n\nValidation Testing, possibly under simulated conditions, to ensure that a finished product works as required.\n\nValidation (of a \nproduct)\n\nProof that the product accomplishes the intended purpose. Validation may be determined by a \ncombination of test, analysis, and demonstration.\n\nVariance In program control terminology, a difference between actual performance and planned costs or \nschedule status.\n\nVerification The process of proving or demonstrating that a finished product meets design specifications and \nrequirements.\n\nVerification (of a \nproduct) \n\nProof of compliance with specifications. Verification may be determined by test, analysis, demonstra-\ntion, or inspection.\n\nWaiver A documented agreement intentionally releasing a program or project from meeting a requirement. \n(Some Centers use deviations prior to Implementation and waivers during Implementation).\n\nWBS Model Model that describes a system that consists of end products and their subsystems (which perform \nthe operational functions of the system), the supporting or enabling products, and any other work \nproducts (plans, baselines) required for the development of the system.\n\nWork Breakdown \nStructure (WBS)\n\nA product-oriented hierarchical division of the hardware, software, services, and data required to \nproduce the program/project\u2019s end product(s) structured according to the way the work will be \nperformed, reflecting the way in which program/project costs, schedule, technical, and risk data are to \nbe accumulated, summarized, and reported.\n\nWorkflow \nDiagram\n\nA scheduling chart that shows activities, dependencies among activities, and milestones.\n\n\n\nNASA Systems Engineering Handbook ? 279\n\nAppendix C: How to Write a Good Requirement\n\nUse of Correct Terms\nShall ?  = requirement\nWill ?  = facts or declaration of purpose\nShould ?  = goal\n\nEditorial Checklist\n\nPersonnel Requirement\nThe requirement is in the form \u201cresponsible party shall perform such and such.\u201d In other words, use the active, 1. \nrather than the passive voice. A requirement must state who shall (do, perform, provide, weigh, or other verb) fol-\nlowed by a description of what must be performed.\n\nProduct Requirement\nThe requirement is in the form \u201cproduct ABC shall XYZ.\u201d A requirement must state \u201cThe product shall\u201d (do, per-1. \nform, provide, weigh, or other verb) followed by a description of what must be done.\nThe requirement uses consistent terminology to refer to the product and its lower level entities.2. \nComplete with tolerances for qualitative/performance values (e.g., less than, greater than or equal to, plus or minus, 3. \n3 sigma root sum squares).\nIs the requirement free of implementation? (Requirements should state WHAT is needed, NOT HOW to provide 4. \nit; i.e., state the problem not the solution. Ask, \u201cWhy do you need the requirement?\u201d The answer may point to the \nreal requirement.)\nFree of descriptions of operations? (Is this a need the product must satisfy or an activity involving the product? Sen-5. \ntences like \u201cThe operator shall\u2026\u201d are almost always operational statements not requirements.)\n\nExample Product Requirements\nThe system shall operate at a power level of\u2026 ?\nThe software shall acquire data from the\u2026 ?\nThe structure shall withstand loads of\u2026 ?\nThe hardware shall have a mass of\u2026 ?\n\nGeneral Goodness Checklist\nThe requirement is grammatically correct.1. \nThe requirement is free of typos, misspellings, and punctuation errors.2. \nThe requirement complies with the project\u2019s template and style rules.3. \nThe requirement is stated positively (as opposed to negatively, i.e., \u201cshall not\u201d).4. \nThe use of \u201cTo Be Determined\u201d (TBD) values should be minimized. It is better to use a best estimate for a value 5. \nand mark it \u201cTo Be Resolved\u201d (TBR) with the rationale along with what must be done to eliminate the TBR, who is \nresponsible for its elimination, and by when it must be eliminated.\nThe requirement is accompanied by an intelligible rationale, including any assumptions. Can you validate (concur 6. \nwith) the assumptions? Assumptions must be confirmed before baselining.\nThe requirement is located in the proper section of the document (e.g., not in an appendix).7. \n\n\n\n280 ? NASA Systems Engineering Handbook\n\nAppendix C: How to Write a Good Requirement\n\nRequirements Validation Checklist\n\nClarity\nAre the requirements clear and unambiguous? (Are all aspects of the requirement understandable and not subject 1. \nto misinterpretation? Is the requirement free from indefinite pronouns (this, these) and ambiguous terms (e.g., \u201cas \nappropriate,\u201d \u201cetc.,\u201d \u201cand/or,\u201d \u201cbut not limited to\u201d)?)\nAre the requirements concise and simple?2. \nDo the requirements express only one thought per requirement statement, a standalone statement as opposed to 3. \nmultiple requirements in a single statement, or a paragraph that contains both requirements and rationale?\nDoes the requirement statement have one subject and one predicate?4. \n\nCompleteness\nAre requirements stated as completely as possible? Have all incomplete requirements been captured as TBDs or 1. \nTBRs and a complete listing of them maintained with the requirements?\nAre any requirements missing? For example have any of the following requirements areas been overlooked: func-2. \ntional, performance, interface, environment (development, manufacturing, test, transport, storage, operations), \nfacility (manufacturing, test, storage, operations), transportation (among areas for manufacturing, assembling, de-\nlivery points, within storage facilities, loading), training, personnel, operability, safety, security, appearance and \nphysical characteristics, and design.\nHave all assumptions been explicitly stated?3. \n\nCompliance\nAre all requirements at the correct level (e.g., system, segment, element, subsystem)? 1. \nAre requirements free of implementation specifics? (Requirements should state what is needed, not how to provide it.)2. \nAre requirements free of descriptions of operations? (Don\u2019t mix operation with requirements: update the ConOps 3. \ninstead.)\n\nConsistency\nAre the requirements stated consistently without contradicting themselves or the requirements of related sys-1. \ntems?\nIs the terminology consistent with the user and sponsor\u2019s terminology? With the project glossary?2. \nIs the terminology consistently used through out the document?3. \nAre the key terms included in the project\u2019s glossary?4. \n\nTraceability\nAre all requirements needed? Is each requirement necessary to meet the parent requirement? Is each requirement 1. \na needed function or characteristic? Distinguish between needs and wants. If it is not necessary, it is not a require-\nment. Ask, \u201cWhat is the worst that could happen if the requirement was not included?\u201d\nAre all requirements (functions, structures, and constraints) bidirectionally traceable to higher level requirements 2. \nor mission or system-of-interest scope (i.e., need(s), goals, objectives, constraints, or concept of operations)?\nIs each requirement stated in such a manner that it can be uniquely referenced (e.g., each requirement is uniquely 3. \nnumbered) in subordinate documents?\n\nCorrectness\nIs each requirement correct?1. \nIs each stated assumption correct? Assumptions must be confirmed before the document can be baselined.2. \nAre the requirements technically feasible?3. \n\n\n\nAppendix C: How to Write a Good Requirement\n\nNASA Systems Engineering Handbook ? 281\n\nFunctionality\nAre all described functions necessary and together sufficient to meet mission and system goals and objectives?1. \n\nPerformance\nAre all required performance specifications and margins listed (e.g., consider timing, throughput, storage size, la-1. \ntency, accuracy and precision)?\nIs each performance requirement realistic?2. \nAre the tolerances overly tight? Are the tolerances defendable and cost-effective? Ask, \u201cWhat is the worst thing that 3. \ncould happen if the tolerance was doubled or tripled?\u201d\n\nInterfaces\nAre all external interfaces clearly defined?1. \nAre all internal interfaces clearly defined?2. \nAre all interfaces necessary, sufficient, and consistent with each other?3. \n\nMaintainability\nHave the requirements for system maintainability been specified in a measurable, verifiable manner?1. \nAre requirements written so that ripple effects from changes are minimized (i.e., requirements are as weakly cou-2. \npled as possible)?\n\nReliability\nAre clearly defined, measurable, and verifiable reliability requirements specified?1. \nAre there error detection, reporting, handling, and recovery requirements?2. \nAre undesired events (e.g., single event upset, data loss or scrambling, operator error) considered and their re-3. \nquired responses specified?\nHave assumptions about the intended sequence of functions been stated? Are these sequences required?4. \nDo these requirements adequately address the survivability after a software or hardware fault of the system from the 5. \npoint of view of hardware, software, operations, personnel and procedures?\n\nVerifiability/Testability\nCan the system be tested, demonstrated, inspected, or analyzed to show that it satisfies requirements? Can this be 1. \ndone at the level of the system at which the requirement is stated? Does a means exist to measure the accomplish-\nment of the requirement and verify compliance? Can the criteria for verification be stated?\nAre the requirements stated precisely to facilitate specification of system test success criteria and requirements?2. \nAre the requirements free of unverifiable terms (e.g., flexible, easy, sufficient, safe, ad hoc, adequate, accommodate, 3. \nuser-friendly, usable, when required, if required, appropriate, fast, portable, light-weight, small, large, maximize, \nminimize, sufficient, robust, quickly, easily, clearly, other \u201cly\u201d words, other \u201cize\u201d words)?\n\nData Usage\nWhere applicable, are \u201cdon\u2019t care\u201d conditions truly \u201cdon\u2019t care\u201d? (\u201cDon\u2019t care\u201d values identify cases when the value 1. \nof a condition or flag is irrelevant, even though the value may be important for other cases.) Are \u201cdon\u2019t care\u201d condi-\ntions values explicitly stated? (Correct identification of \u201cdon\u2019t care\u201d values may improve a design\u2019s portability.)\n\n\n\n282 ? NASA Systems Engineering Handbook\n\nAppendix D: Requirements Verification Matrix\n\nWhen developing requirements, it is important to \nidentify an approach for verifying the requirements. \nThis appendix provides the matrix that defines how \nall the requirements are verified. Only \u201cshall\u201d require-\nments should be included in these matrices. The ma-\ntrix should identify each \u201cshall\u201d by unique identifier \nand be definitive as to the source, i.e., document from \n\nwhich the requirement is taken. This matrix could be \ndivided into multiple matrices (e.g., one per require-\nments document) to delineate sources of requirements \ndepending on the project. The example is shown to \nprovide suggested guidelines for the minimum infor-\nmation that should be included in the verification ma-\ntrix.\n\n\n\nRe\nqu\n\nir\ne?\n\n \nm\n\nen\nt N\n\no.\n a\n\nD\noc\n\num\nen\n\ntb\nPa\n\nra\ngr\n\nap\nhc\n\nSh\nal\n\nl \nSt\n\nat\nem\n\nen\ntd\n\nVe\nri\n\nfic\nat\n\nio\nn \n\nSu\nc?\n\nce\nss\n\n C\nri\n\nte\nri\n\nae\n\nVe\nri\n\nfi?\n \n\nca\nti\n\non\n \n\nM\net\n\nho\ndf\n\nFa\nci\n\nlit\ny \n\nor\n L\n\nab\ng\n\nPh\nas\n\neh\n\nA\ncc\n\nep\nta\n\nnc\ne \n\nRe\nqu\n\nir\ne?\n\n \nm\n\nen\nt?\n\ni\n\nPr\nefl\n\nig\nht\n\n \nA\n\ncc\nep\n\nt?\nan\n\nce\n?j\n\nPe\nrf\n\nor\nm\n\nin\ng \n\nO\nrg\n\nan\niz\n\na?\n \n\nti\non\n\nk\nRe\n\nsu\nlt\n\nsl\n\nP-\n1\n\nxx\nx\n\n3.\n2.\n\n1.\n1 \n\nCa\npa\n\nbi\nlit\n\ny:\n \n\nSu\npp\n\nor\nt \n\nU\npl\n\nin\nke\n\nd \nD\n\nat\na \n\n(L\nD\n\nR)\n\nSy\nst\n\nem\n X\n\n \nsh\n\nal\nl p\n\nro\n-\n\nvi\nde\n\n a\n m\n\nax\n. \n\ngr\nou\n\nnd\n-\n\nto\n-s\n\nta\ntio\n\nn \nup\n\nlin\nk \n\nof\n\u2026\n\n1.\n S\n\nys\nte\n\nm\n X\n\n lo\nck\n\ns \nto\n\n \nfo\n\nrw\nar\n\nd \nlin\n\nk \nat\n\n th\ne \n\nm\nin\n\n a\nnd\n\n m\nax\n\n d\nat\n\na \nra\n\nte\n to\n\nle\nra\n\nnc\nes\n\n2.\n S\n\nys\nte\n\nm\n X\n\n lo\nck\n\ns \nto\n\n th\ne \n\nfo\nrw\n\nar\nd \n\nlin\nk \n\nat\n th\n\ne \nm\n\nin\n a\n\nnd\n m\n\nax\n \n\nop\ner\n\nat\nin\n\ng \nfr\n\neq\nue\n\nnc\ny \n\nto\nle\n\nra\nnc\n\nes\n \n\nTe\nst\n\nxx\nx\n\n5\nxx\n\nx\nTP\n\nS \nxx\n\nxx\n\nP-\ni\n\nxx\nx\n\nO\nth\n\ner\n \n\npa\nra\n\ngr\nap\n\nhs\nO\n\nth\ner\n\n \n\u201cs\n\nha\nlls\n\n\u201d i\nn \n\nPT\nRS\n\nO\nth\n\ner\n c\n\nrit\ner\n\nia\nxx\n\nx\nxx\n\nx\nxx\n\nx\nxx\n\nx\nM\n\nem\no \n\nxx\nx\n\nS-\ni o\n\nr o\nth\n\ner\n \n\nun\niq\n\nue\n \n\nde\nsi\n\ngn\nat\n\nor\n\nxx\nxx\n\nx \n(o\n\nth\ner\n\n \nsp\n\nec\ns,\n\n IC\nD\n\ns,\n \n\net\nc.\n\n)\n\nO\nth\n\ner\n \n\npa\nra\n\ngr\nap\n\nhs\n \n\nO\nth\n\ner\n \n\n\u201cs\nha\n\nlls\n\u201d i\n\nn \nsp\n\nec\ns,\n\n IC\nD\n\ns,\n \n\net\nc.\n\nO\nth\n\ner\n c\n\nrit\ner\n\nia\nxx\n\nx\nxx\n\nx\nxx\n\nx\nxx\n\nx\nRe\n\npo\nrt\n\n x\nxx\n\na.\n \n\nU\nni\n\nqu\ne \n\nid\nen\n\ntifi\ner\n\n fo\nr e\n\nac\nh \n\nSy\nst\n\nem\n X\n\n re\nqu\n\nire\nm\n\nen\nt.\n\nb.\n \n\nD\noc\n\num\nen\n\nt n\num\n\nbe\nr t\n\nhe\n S\n\nys\nte\n\nm\n X\n\n re\nqu\n\nire\nm\n\nen\nt i\n\ns \nco\n\nnt\nai\n\nne\nd \n\nw\nith\n\nin\n.\n\nc.\n \n\nPa\nra\n\ngr\nap\n\nh \nnu\n\nm\nbe\n\nr o\nf t\n\nhe\n S\n\nys\nte\n\nm\n X\n\n re\nqu\n\nire\nm\n\nen\nt.\n\nd.\n \n\nTe\nxt\n\n (w\nith\n\nin\n re\n\nas\non\n\n) o\nf t\n\nhe\n S\n\nys\nte\n\nm\n X\n\n re\nqu\n\nire\nm\n\nen\nt, \n\ni.e\n., \n\nth\ne \n\n\u201cs\nha\n\nll.\u201d\n\ne.\n \n\nSu\ncc\n\nes\ns \n\ncr\nite\n\nria\n fo\n\nr t\nhe\n\n S\nys\n\nte\nm\n\n X\n re\n\nqu\nire\n\nm\nen\n\nt.\n\nf. \nVe\n\nrifi\nca\n\ntio\nn \n\nm\net\n\nho\nd \n\nfo\nr t\n\nhe\n S\n\nys\nte\n\nm\n X\n\n re\nqu\n\nire\nm\n\nen\nt (\n\nan\nal\n\nys\nis\n\n, i\nns\n\npe\nct\n\nio\nn,\n\n d\nem\n\non\nst\n\nra\ntio\n\nn,\n o\n\nr t\nes\n\nt)\n.\n\ng.\n \n\nFa\nci\n\nlit\ny \n\nor\n la\n\nbo\nra\n\nto\nry\n\n u\nse\n\nd \nto\n\n p\ner\n\nfo\nrm\n\n th\ne \n\nve\nrifi\n\nca\ntio\n\nn \nan\n\nd \nva\n\nlid\nat\n\nio\nn.\n\nh.\n \n\nPh\nas\n\ne \nin\n\n w\nhi\n\nch\n th\n\ne \nve\n\nrifi\nca\n\ntio\nn \n\nan\nd \n\nva\nlid\n\nat\nio\n\nn \nw\n\nill\n b\n\ne \npe\n\nrf\nor\n\nm\ned\n\n: (\n1)\n\n P\nre\n\n-D\nec\n\nla\nre\n\nd \nD\n\nev\nel\n\nop\nm\n\nen\nt, \n\n(2\n) F\n\nor\nm\n\nal\n B\n\nox\n-L\n\nev\nel\n\n F\nun\n\nct\nio\n\nna\nl, \n\n(3\n) F\n\nor\nm\n\nal\n B\n\nox\n-L\n\nev\nel\n\n E\nnv\n\niro\nnm\n\nen\nta\n\nl, \n(4\n\n) F\nor\n\nm\nal\n\n S\nys\n\nte\nm\n\n-\nLe\n\nve\nl E\n\nnv\niro\n\nnm\nen\n\nta\nl, \n\n(5\n) F\n\nor\nm\n\nal\n S\n\nys\nte\n\nm\n-L\n\nev\nel\n\n F\nun\n\nct\nio\n\nna\nl, \n\n(6\n) F\n\nor\nm\n\nal\n E\n\nnd\n-t\n\no-\nEn\n\nd \nFu\n\nnc\ntio\n\nna\nl, \n\n(7\n) I\n\nnt\neg\n\nra\nte\n\nd \nVe\n\nhi\ncl\n\ne \nFu\n\nnc\ntio\n\nna\nl, \n\n(8\n) O\n\nn-\nO\n\nrb\nit \n\nFu\nnc\n\ntio\nna\n\nl.\n\ni. \nIn\n\ndi\nca\n\nte\n w\n\nhe\nth\n\ner\n th\n\nis\n re\n\nqu\nire\n\nm\nen\n\nt i\ns \n\nal\nso\n\n v\ner\n\nifi\ned\n\n d\nur\n\nin\ng \n\nin\niti\n\nal\n a\n\ncc\nep\n\nta\nnc\n\ne \nte\n\nst\nin\n\ng \nof\n\n e\nac\n\nh \nun\n\nit.\n\nj. \nIn\n\ndi\nca\n\nte\n w\n\nhe\nth\n\ner\n th\n\nis\n re\n\nqu\nire\n\nm\nen\n\nt i\ns \n\nal\nso\n\n v\ner\n\nifi\ned\n\n d\nur\n\nin\ng \n\nan\ny \n\npr\ne-\n\nfli\ngh\n\nt o\nr r\n\nec\nur\n\nrin\ng \n\nac\nce\n\npt\nan\n\nce\n te\n\nst\nin\n\ng \nof\n\n e\nac\n\nh \nun\n\nit.\n\nk.\n \n\nO\nrg\n\nan\niz\n\nat\nio\n\nn \nre\n\nsp\non\n\nsi\nbl\n\ne \nfo\n\nr p\ner\n\nfo\nrm\n\nin\ng \n\nth\ne \n\nve\nrifi\n\nca\ntio\n\nn\n\nl. \nIn\n\ndi\nca\n\nte\n d\n\noc\num\n\nen\nts\n\n th\nat\n\n c\non\n\nta\nin\n\n th\ne \n\nob\nje\n\nct\niv\n\ne \nev\n\nid\nen\n\nce\n th\n\nat\n re\n\nqu\nire\n\nm\nen\n\nt w\nas\n\n s\nat\n\nis\nfie\n\nd\n\nTa\nbl\n\ne \nD\n\n?1\n R\n\neq\nui\n\nre\nm\n\nen\nts\n\n V\ner\n\nifi\nca\n\nti\non\n\n M\nat\n\nri\nx\n\n\n\n284 ? NASA Systems Engineering Handbook\n\nAppendix E: Creating the Validation Plan  \n(Including Validation Requirements Matrix)\n\nValidation \nProduct # Activity Objective\n\nValidation \nMethod\n\nFacility or \nLab Phase\n\nPerforming \nOrganization Results\n\nUnique \nidentifier \nfor \nvalidation \nproduct\n\nDescribe \nevaluation \nby the cus-\ntomer/spon-\nsor that will \nbe performed\n\nWhat is to be \naccomplished \nby the \ncustomer/ \nsponsor \nevaluation\n\nValidation \nmethod for \nthe System X \nrequirement \n(analysis, \ninspection, \ndemonstration, \nor test)\n\nFacility or \nlaboratory \nused to \nperform \nthe valida-\ntion\n\nPhase in \nwhich the \nverification/ \nvalidation \nwill be \nperformeda\n\nOrganization \nresponsible for \ncoordinating \nthe validation \nactivity\n\nIndicate \nthe \nobjective \nevidence \nthat \nvalidation \nactivity \noccurred\n\n1 Customer/\nsponsor will \nevaluate the \ncandidate \ndisplays\n\n1. Ensure \nlegibility is \nacceptable\n\n2. Ensure over-\nall appearance \nis acceptable\n\nTest xxx Phase A xxx\n\na. Example: (1) during product selection process, (2) prior to final product selection (if COTS) or prior to PDR, (3) prior to CDR, (4) during \nbox-level functional, (5) during system-level functional, (6) during end-to-end functional, (7) during integrated vehicle functional, \n(8) during on-orbit functional.\n\nWhen developing requirements, it is important to iden-\ntify a validation approach for how additional validation \nevaluation, testing, analysis or other demonstrations will \nbe performed to ensure customer/sponsor satisfaction. \n\nThis validation plan should include a validation ma-\ntrix with the elements in the example below. The final \ncolumn in the matrix below uses a display product as a \nspecific example.\n\nTable E?1 Validation Requirements Matrix\n\n\n\nNASA Systems Engineering Handbook ? 285\n\nAppendix F: Functional, Timing, and State Analysis \n\nFunctional Flow Block Diagrams\nFunctional analysis can be performed using various \nmethods, one of which is Functional Flow Block Dia-\ngrams (FFBDs). FFBDs define the system functions and \ndepict the time sequence of functional events. They iden-\ntify \u201cwhat\u201d must happen and do not assume a particular \nanswer to \u201chow\u201d a function will be performed. They are \nfunctionally oriented, not solution oriented. \n\nFFBDs are made up of functional blocks, each of which \nrepresents a definite, finite, discrete action to be accom-\nplished. The functional architecture is developed using a \nseries of leveled diagrams to show the functional decom-\nposition and display the functions in their logical, se-\nquential relationship. A consistent numbering scheme is \nused to label the blocks. The numbers establish identifica-\ntion and relationships that carry through all the diagrams \nand facilitate traceability from the lower levels to the top \nlevel. Each block in the first- (top-) level diagram can be \nexpanded to a series of functions in the second-level dia-\ngram, and so on. (See Figure F-1.) Lines connecting func-\ntions indicate function flow and not lapsed time or inter-\nmediate activity. Diagrams are laid out so that the flow \ndirection is generally from left to right. Arrows are often \nused to indicate functional flows. The diagrams show both \ninput (transfer to operational orbit) and output (transfer \nto STS orbit), thus facilitating the definition of interfaces \nand control process.\n\nEach diagram contains a reference to other functional \ndiagrams to facilitate movement between pages of the \ndiagrams. Gates are used: \u201cAND,\u201d \u201cOR,\u201d \u201cGo or No-Go,\u201d \nsometimes with enhanced functionality, including ex-\nclusive OR gate (XOR), iteration (IT), repetition (RP), or \nloop (LP). A circle is used to denote a summing gate and \nis used when AND/OR is present. AND is used to indi-\ncate parallel functions and all conditions must be satisfied \nto proceed (i.e., concurrency). OR is used to indicate that \nalternative paths can be satisfied to proceed (i.e., selec-\ntion). G and G\u2014are used to denote Go and No-Go condi-\ntions. These symbols are placed adjacent to lines leaving \na particular function to indicate alternative paths. For \nexamples of the above, see Figures F-2 and F-3.\n\nEnhanced Functional Flow Block Diagrams (EFFBDs) \nprovide data flow overlay to capture data dependencies. \nEFFBDs (shown in Figure F-4) represent: (1) functions, \n(2) control flows, and (3) data flows. An EFFBD specifi-\ncation of a system is complete enough that it is executable \nas a discrete event model, capable of dynamic, as well as \nstatic, validation. EFFBDs provide freedom to use either \ncontrol constructs or data triggers or both to specify ex-\necution conditions for the system functions. EFFBDs \ngraphically distinguish between triggering and nontrig-\ngering data inputs. Triggering data are required before \na function can begin execution. Triggers are actually \ndata items with control implications. In Figure F-4, the \ndata input shown with a green background and double-\nheaded arrows is a triggering data input. The nontrig-\ngering data inputs are shown with gray backgrounds and \nsingle-headed arrows. An EFFBD must be enabled by: \n(1) the completion of the function(s) preceding it in the \ncontrol construct and (2) triggered, if trigger data are \nidentified, before it can execute. For example, in Fig-\nure F-4, \u201c1. Serial Function\u201d must complete and \u201cData \n3\u201d must be present before \u201c3. Function in Concurrency\u201d \ncan execute. It should be noted that the \u201cExternal Input\u201d \ndata into \u201c1. Serial Function\u201d and the \u201cExternal Output\u201d \ndata from \u201c6. Output Function\u201d should not be confused \nwith the functional input and output for these functions, \nwhich are represented by the input and the output arrows \nrespectively. Data flows are represented as elongated ovals \nwhereas functions are represented as rectangular boxes.\n\nFunctional analysis looks across all life-cycle processes. \nFunctions required to deploy a system are very different \nfrom functions required to operate and ultimately dis-\npose of the system. Preparing FFBDs for each phase of \nthe life cycle as well as the transition into the phases \nthemselves is necessary to draw out all the requirements. \nThese diagrams are used both to develop requirements \nand to identify profitability. The functional analysis also \nincorporates alternative and contingency operations, \nwhich improve the probability of mission success. The \nflow diagrams provide an understanding of total opera-\ntion of the system, serve as a basis for development of \noperational and contingency procedures, and pinpoint \n\n\n\n286 ? NASA Systems Engineering Handbook\n\nAppendix F: Functional, Timing, and State Analysis\n\nTOP LEVEL\n\n1.0\n\nAscent Into\nOrbit Injection\n\n(6.0) Ref.\nTransfer to\nSTS Orbit\n\n(3.0) Ref.\nTransfer to\nOPS Orbit \n\nAND\n\n (4.10) Ref.\nTransmit Pay-\n\nload & Sub-\nsystem Data \n\n(4.7) Ref.\nStore/Process\n\nCommand\n\nOR\n\nTHIRD LEVEL\n\nSECOND LEVEL\n\n2.0\n\nCheck Out\nand Deploy\n\n3.0\n\nTransfer to\nOPS Orbit\n\n5.0\n\nContingency\nOperations\n\n6.0\n\nTransfer to\nSTS Orbit\n\n7.0\n\nRetrieve\nSpacecraft\n\n8.0\n\nReenter and\nLand\n\n4.1\n\nProvide\nElectric Power\n\n4.2\nProvide\nAttitude\n\nStabilization\n\n4.3\nProvide\nThermal\nControl\n\n4.4\n\nProvide Orbit\nMain\n\n4.5\n\nReceive\nCommand\n\n4.6\n\nReceive Com-\nmand (Omni)\n\n4.7\n\nStore/Process\nCommand\n\n4.8\n\nAcquire\nPayload Data\n\n4.0\nPerform\nMission\n\nOperations\n\n4.10\nTransmit Pay-\n\nload & Sub-\nsystem Data\n\nOR\n\n4.9\nAcquire\n\nSubsystem\nStatus Data\n\n4.11\nTransmit\n\nSubsystem\nData\n\n4.8.1\nCompute TDRS\n\nPointing\nVector\n\n4.8.2\nSlew to\n\nand Track\nTDRS\n\n4.8.3\n\nRadar to\nStandby\n\n4.8.4\nCompute LOS\n\nPointing\nVector\n\n4.8.5\nSlew S/C\nto LOS\nVector\n\n4.8.6\nCommand \n\nERP PW\nRadar On\n\n4.8.7\nProcess Re-\n\nceiving Signal\nand Format\n\n4.8.8\n\nRadar to\nStandby\n\n4.8.9\n\nRadar O?\n\nOR OR\n\nOR\n\nFigure F?1 FFBD flowdown\n\nareas where changes in operational procedures could \nsimplify the overall system operation. This organiza-\ntion will eventually feed into the WBS structure and ul-\ntimately drive the overall mission organization and cost. \nIn certain cases, alternative FFBDs may be used to rep-\nresent various means of satisfying a particular function \nuntil data are acquired, which permits selection among \nthe alternatives. For more information on FFBDs and \nEFFBDs, see Jim Long\u2019s Relationships between Common \nGraphical Representations in Systems Engineering.\n\nRequirements Allocation Sheets\nRequirements allocation sheets document the connection \nbetween allocated functions, allocated performance, and \nthe physical system. They provide traceability between \nTechnical Requirements Definition functional analysis \nactivities and Logical Decomposition and Design Solu-\ntion Definition activities and maintain consistency be-\ntween them, as well as show disconnects. Figure F-5 pro-\nvides an example of a requirements allocation sheet. The \n\n\n\nAppendix F: Functional, Timing, and State Analysis\n\nNASA Systems Engineering Handbook ? 287\n\nFigure F?2 FFBD: example 1\n\nRef 9.2, Provide Guidance\n\n9.2.1\nAND\n\n9.2.4\n\n9.2.3\n\n9.2.2\n\nAND3.5 Ref\n\nOR\n\nSystem\nMalfunction \n\nParallel  Functions \n\nAlternate  Functions \n\nFunctional\nDescription\n\nFunction\nNumber\n\nSumming\nGate\n\nLeader\nNote\n\nGo Flow\n\nNo Go Flow\n1.1.2 Ref\n\nAND\n\nOR\n\nRef 11.3.1\n\nSee Detail Diagram\n\nSee Detail\nDiagram\n\nSee Detail Diagram\n\nOR\nG\n\nG \n_\n\nOR\n\nTentative\nFunction\n\nScope Note:  _________________\n____________________________\n____________________________ Functional Flow Block\n\nDiagram FormatTitle Block and Standard Drawing Number\n\n2nd Level\nFlow-Level Designator\n\nInterface Reference\nBlock (used on first- \nand lower level\nfunctional diagrams \nonly)\n\nFigure F?3 FFBD showing additional control constructs: example 2\n\nMultiple Exit Function Iterate\n\nLoop\n\nConcurrency Select\n\nCompletion Criterion\n\nBranch Annotation\n\nReplicate\n\n1\n\nFunction in a\nConcurrency\n\n2\nSecond\n\nFunction in a\nConcurrency\n\n4\n\nMulti-Exit\nFunction\n\n5\n\nFunction on\nExit Branch\n\n7\n\nFunction in a\nReplicate\n\n6\nFunction on\na Coordinate\n\nBranch\n\nAND LP\n\nRPRP\n\nDomain set for iterate\n\nLoop annotation\n\nDomain set for\nreplicate with\ncoordination\n\nAnnotation\n\nBranch #2\n\nBranch #3\n\nCC #2\n\nCC #1\n\nRef RefAND OROR LP\n\n3\n\nFunction in \nan Iterate\n\nIT IT\n\nOR\n\n\n\n288 ? NASA Systems Engineering Handbook\n\nAppendix F: Functional, Timing, and State Analysis\n\nFigure F?4 Enhanced FFBD: example 3\n\n2\n\nMulti-Exit\nFunction\n\n3\n\nFunction in\nConcurrency\n\n4\nFunction in\nMulti-Exit\nConstruct\n\nAND\n\n3 times\n\nAND\n\nOR\n\n5\n\nFunction in\nIterate\n\nIT IT\n\nData 1 Data 2\n\nData 3\n\nData 4\n\nData 5\n\nExternal\nOutput\n\n6\n\nOutput\nFunction\n\nExternal\nInput\n\n1\n\nSerial\nFunction\n\nIT\nCC #2\n\nCC #1\n\nreference column to the far right indicates the function \nnumbers from the FFBDs. Fill in the requirements allo-\ncation sheet by performing the following: \n\nInclude the functions and function numbers from 1. \nthe FFBDs.\nAllocate functional performance requirements and 2. \ndesign requirements to the appropriate function(s) \n(many requirements may be allocated to one func-\ntion, or one requirement may be allocated to many \nfunctions).\nAll system-level requirements must be allocated to 3. \na function to ensure the system meets all system re-\nquirements (functions without allocated requirements \nshould be eliminated as unnecessary activities).\nAllocate all derived requirements to the function 4. \nthat spawned the requirement.\nIdentify the physical equipment, configuration item, 5. \nfacilities, and specifications that will be used to meet \nthe requirements.\n\n(For a reference on requirements allocation sheets, see \nDOD\u2019s Systems Engineering Fundamentals Guide.)\n\nN2 Diagrams\nAn N-squared (N2) diagram is a matrix representation \nof functional and/or physical interfaces between ele-\nments of a system at a particular hierarchical level. The \nN2 diagram has been used extensively to develop data in-\nterfaces, primarily in the software areas. However, it can \nalso be used to develop hardware interfaces as shown in \nFigure F-6. The system components are placed on the di-\nagonal. The remainder of the squares in the NxN matrix \nrepresent the interfaces. The square at the intersection of \na row and a column contains a description of the inter-\nface between the two components represented on that \nrow and that column. For example, the solar arrays have \na mechanical interface with the structure and an elec-\ntrical interface and supplied service interface with the \nvoltage converters. Where a blank appears, there is no \ninterface between the respective components. \n\nThe N2 diagram can be taken down into successively \nlower levels to the hardware and software component \nfunctional levels. In addition to defining the data that \nmust be supplied across the interface, by showing the \n\n\n\nAppendix F: Functional, Timing, and State Analysis\n\nNASA Systems Engineering Handbook ? 289\n\nID DESCRIPTION REQUIREMENT\nTRACED \n\nFROM\nPERFORMANCE MARGIN COMMENTS REF\n\nM1 Mission   Orbit 575 +/-15 km Sun-synchronous dawn-dusk orbit S3, S11, P3 Complies NA Pegasus XL with HAPS \nprovides required launch \ninjection dispersion accuracy\n\nF.2.c\n\nM2 Launch Vehicle  Pegasus XL with HAPS P2, P4 Complies NA F.2.c\nM3 Observatory Mass The  observatory total mass shall not exceed\n\n 241 kg \nM1, M2 192.5 kg 25.20% F.5.b\n\nM4 Data Acquisition\nQuality \n\nThe  mission shall deliver 95% data with \nbetter than 1 in 100,000 BER\n\nP1 Complies NA Standard margins and\nsystems baselined; formal \nsystem analysis to be \ncompleted by PDR  \n\nF.7\n\nM5 Communication\nBand \n\nThe mission shall use S-band SQPSK at 5 Mbps for \nspacecraft downlink and 2 kbps uplink \n\nS12, P4 Complies NA See SC27, SC28, and G1, G2 F.3.f,\nF.7\n\nM7 Tracking MOC shall use NORAD two-line elements for \nobservatory tracking \n\nP4 Complies NA F.7\n\nM8 Data Latency Data latency shall be less than 72 hours P12 Complies NA F.7\nM9 Daily Data Volume Accommodate average daily raw science data \n\nvolume of 10.8 Gbits \nP1, S12 Complies 12% Margin based on funded \n\nground contacts \nF.3.e,\n\nF.7\nM10 Ground Station The mission shall be compatible with the \n\nRutherford Appleton Laboratory Ground Station \nand  the Poker Flat Ground Station \n\nP1 Complies NA F.7\n\nM11 Orbital Debris\n(Casualty Area) \n\nDesign  observatory for demise upon \nreentry with <1/10,000 probability of injury\n\nP3 1/51,000 400% See Orbital Debris Analysis in \nAppendix M-6\n\nF.2.e,\nApp.6\n\nM12 Orbital Debris\n(Lifetime)\n\nDesign  observatory for reentry <25 years \nafter end of mission \n\nP3 <10 years 15 years See Orbital Debris Analysis in \nAppendix M-6\n\nF.2.e,\nApp.6\n\nFigure F?5 Requirements allocation sheet\n\nFigure F?6 N2 diagram for orbital equipment \nNote: From NASA Reference Publication 1370, Training Manual for Elements of Interface Definition and Control.\n\nStructure\n\nFuel\nPods\n\nThrusters\n\nSolar\nArrays\n\nHeat\nConverters\n\nVoltage\nConverters\n\nAntenna\nA\n\nAntenna\nB\n\nExperiment\n1\n\nExperiment\n2\n\nExperiment\n3\n\nGyros\n\nMechanical/Physical Interface\n\nElectrical/Functional Interface\n\nSupplied Services\n\nLegend:\n\n\n\n290 ? NASA Systems Engineering Handbook\n\nAppendix F: Functional, Timing, and State Analysis\n\nU\nse\n\nr\n\nWait Access\n\nWait Card\n\nIdle\n\nA\ncc\n\nes\ns \n\nCa\nrd\n\nSy\nst\n\nem\n\nNo Card\n\nHas Card\n\nCode\n\nStart OK {t\u2026t+3}\n\nIdle\n\nU\nse\n\nr A\ncc\n\nep\nte\n\nd\n\n[d\u20263*d]\n\n0\u202613\n\nIdle Wait Card Wait Access\n\n0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190\n\n[d\u20263*d]\n\nFigure F?7 Timing diagram example\n\ndata flows the N2 chart pinpoints areas where conflicts \ncould arise in interfaces, and highlights input and output \ndependency assumptions and requirements. \n\nTiming Analysis\nThere are several methods for visualizing the complex \ntiming relationships in a system. Two of the more im-\nportant ones are the timing diagram and the state tran-\nsition diagram. The timing diagram (see Figure F-7) de-\nfines the behavior of different objects within a timescale. \nIt provides a visual representation of objects changing \nstate and interacting over time. Timing diagrams can be \nused for defining the behavior of hardware-driven and/\nor software-driven components. While a simple timeline \nanalysis is very useful in understanding relationships \nsuch as concurrency, overlap, and sequencing, state dia-\ngrams (see Figure F-8) allow for even greater flexibility \nin that they can depict events such as loops and deci-\nsion processes that may have largely varying timelines. \nTiming information can be added to an FFBD to create \na timeline analysis. This is very useful for allocating re-\nsources and generating specific time-related design re-\nquirements. It also elucidates performance characteris-\ntics and design constraints. However, it is not complete. \n\nState diagrams are needed to show the flow of the system \nin response to varying inputs. \n\nThe tools of timing analysis are rather straightforward. \nWhile some Commercial-Off-the-Shelf (COTs) tools are \navailable, any graphics tool and a good spreadsheet will \ndo. The important thing to remember is that timeline \nanalysis is better for linear flows while circular, looping, \nmulti-path, and combinations of these are best described \nwith state diagrams. Complexity should be kept layered \nand track the FFBDs. The ultimate goal of using all these \ntechniques is simply to force the thought process enough \ninto the details of the system that most of the big sur-\nprises can be avoided.\n\nState Analysis\nState diagramming is another graphical tool that is most \nhelpful for understanding and displaying the complex \ntiming relationships in a system. Timing diagrams do \nnot give the complete picture of the system. State dia-\ngrams are needed to show the flow of the system in re-\nsponse to varying inputs. State diagrams provide a sort of \nsimplification of understanding on a system by breaking \ncomplex reactions into smaller and smaller known re-\n\n\n\nAppendix F: Functional, Timing, and State Analysis\n\nNASA Systems Engineering Handbook ? 291\n\nFigure F?8 Slew command status state diagram\n\nReady\n\nSlewing\n\nComplete Settled\n\nRejected\n\nGood\nSlew \n\nCommand\n\n\u201cBad\u201d\nSlew \n\nCommand\n\nSlew \nCommand\n\nTimer Expire\n\nInitialize\n\nEnd of \nSlew\n\nEnd of \nSettling\n\nSlew \nCommand\n\nTimer Expire\n\nSlew \nCommand\n\nTimer Expire\n\nSlew \nCommand\n\nTi mer \nExpired?\n\nNo\n\nYes\n\nMinor \nCycle\n\nSlew \nCommand\n\nTimer Expire\n\n\u201cAny\u201d\nSlew \n\nCommand\n\n\u201cAny\u201d\nSlew \n\nCommand\n\u201cAny\u201d\nSlew \n\nCommand\n\nsponses. This allows detailed requirements to be devel-\noped and verified with their timing performance.\n\nFigure F-8 shows a slew command status state diagram \nfrom the James Webb Space Telescope. Ovals represent \nthe system states. Arcs represent the event that triggers the \nstate change as well as the action or output taken by the \nsystem in response to the event. \n\nSelf-loops are permitted. In the example in Fig-\nure F-8 the slew states can loop until they arrive at \n\nthe correct location, and then they can loop while \nthey settle. \n\nWhen it is used to represent the behavior of a sequential \nfinite-state machine, the state diagram is called a state \ntransition diagram. A sequential finite-state machine is \none that has no memory, which means that the current \noutput only depends on the current input. The state tran-\nsition diagram models the event-based, time-dependent \nbehavior of such a system.\n\n\n\n292 ? NASA Systems Engineering Handbook\n\nAppendix F: Functional, Timing, and State Analysis\n\nContext Diagrams\n\nWhen presented with a system design problem, the systems engineer\u2019s first task \nis to truly understand the problem. That means understanding the context in \nwhich the problem is set. A context diagram is a useful tool for grasping the sys-\ntem to be built and the external domains that are relevant to that system and \nwhich have interfaces to the system. The diagram shows the general structure \nof a context diagram. The system is shown surrounded by the external systems \nwhich have interfaces to the system. These systems are not part of the system, \nbut they interact with the system via the system\u2019s external interfaces. The exter-\nnal systems can impact the system, and the system does impact the external \nsystems. They play a major role in establishing the requirements for the system. \nEntities further removed are those in the system\u2019s context that can impact the system but cannot be impacted by the \nsystem. These entities in the system\u2019s context are responsible for some of the system\u2019s requirements.\n\nDefining the boundaries of a system is a critical but often neglected task. Using an example from a satellite project, one \nof the external systems that is impacted by the satellite would be the Tracking and Data Relay Satellite System (TDRSS). \nThe TDRSS is not part of the satellite system, but it defines requirements on the satellite and is impacted by the satel-\nlite since it must schedule contacts, receive and transmit data and commands, and downlink the satellite data to the \nground. An example of an entity in the context of the satellite system that is not impacted by the satellite system is the \nGlobal Positioning Satellite (GPS) system. The GPS is not impacted in any way by the satellite, but it will levy some re-\nquirements on the satellite if the satellite is to use the GPS signals for navigation.\n\nReference: Diagram is from Buede, The Engineering Design of Systems, p. 38.\n\nSystem\n\nExternal Systems\n\nContext\n\nAre impacted by the system \n\nImpacts, but not impacted by, the system \n\n\n\nNASA Systems Engineering Handbook ? 293\n\nAppendix G: Technology Assessment/Insertion\n\nIntroduction, Purpose, and Scope\nThe Agency\u2019s programs and projects, by their very na-\nture, frequently require the development and infusion \nof new technological advances to meet mission goals, \nobjectives, and resulting requirements. Sometimes the \nnew technological advancement being infused is actu-\nally a heritage system that is being incorporated into a \ndifferent architecture and operated in different environ-\nment from that for which it was originally designed. In \nthis latter case, it is often not recognized that adaptation \nof heritage systems frequently requires technological ad-\nvancement and as a result, key steps in the development \nprocess are given short shrift\u2014often to the detriment of \nthe program/project. In both contexts of technological \nadvancement (new and adapted heritage), infusion is a \nvery complex process that has been dealt with over the \nyears in an ad hoc manner differing greatly from project \nto project with varying degrees of success. \n\nFrequently, technology infusion has resulted in schedule \nslips, cost overruns, and occasionally even to cancella-\ntions or failures. In post mortem, the root cause of such \nevents has often been attributed to \u201cinadequate defini-\ntion of requirements.\u201d If such were indeed the root cause, \nthen correcting the situation would simply be a matter of \nrequiring better requirements definition, but since his-\ntory seems frequently to repeat itself, this must not be \nthe case\u2014at least not in total. \n\nIn fact there are many contributors to schedule slip, cost \noverrun, and project cancellation and failure\u2014among \nthem lack of adequate requirements definition. The \ncase can be made that most of these contributors are \nrelated to the degree of uncertainty at the outset of the \nproject and that a dominant factor in the degree of un-\ncertainty is the lack of understanding of the maturity of \nthe technology required to bring the project to fruition \nand a concomitant lack of understanding of the cost and \nschedule reserves required to advance the technology \nfrom its present state to a point where it can be quali-\nfied and successfully infused with a high degree of con-\nfidence. Although this uncertainty cannot be eliminated, \nit can be substantially reduced through the early applica-\n\ntion of good systems engineering practices focused on \nunderstanding the technological requirements; the ma-\nturity of the required technology; and the technological \nadvancement required to meet program/project goals, \nobjectives, and requirements.\n\nA number of processes can be used to develop the ap-\npropriate level of understanding required for successful \ntechnology insertion. The intent of this appendix is to \ndescribe a systematic process that can be used as an ex-\nample of how to apply standard systems engineering \npractices to perform a comprehensive Technology As-\nsessment (TA). The TA comprises two parts, a Tech-\nnology Maturity Assessment (TMA) and an Advance-\nment Degree of Difficulty Assessment (AD2). The \nprocess begins with the TMA which is used to deter-\nmine technological maturity via NASA\u2019s Technology \nReadiness Level (TRL) scale. It then proceeds to de-\nvelop an understanding of what is required to advance \nthe level of maturity through AD2. It is necessary to \nconduct TAs at various stages throughout a program/\nproject to provide the Key Decision Point (KDP) prod-\nucts required for transition between phases. (See Ta-\nble G-1.)\n\nThe initial TMA provides the baseline maturity of \nthe system\u2019s required technologies at program/project \noutset and allows monitoring progress throughout de-\nvelopment. The final TMA is performed just prior to \nthe Preliminary Design Review. It forms the basis for \nthe Technology Readiness Assessment Report (TRAR), \nwhich documents the maturity of the technological ad-\nvancement required by the systems, subsystems, and \ncomponents demonstrated through test and analysis. \nThe initial AD2 assessment provides the material neces-\nsary to develop preliminary cost and to schedule plans \nand preliminary risk assessments. In subsequent assess-\nment, the information is used to build the technology \ndevelopment plan in the process identifying alternative \npaths, fallback positions, and performance descope op-\ntions. The information is also vital to preparing mile-\nstones and metrics for subsequent Earned Value Man-\nagement (EVM). \n\n\n\n294 ? NASA Systems Engineering Handbook\n\nAppendix G: Technology Assessment/Insertion\n\n.1: Integ MPS\n\n.2: LH System\n\n.3: O2 Fluid Sys.\n\n.4: Pressure &\nPneumatic Sys.\n\n.5: Umbilicals & \nDisconnect \n\nCrew Launch\nVehicle\n\n1.3\n\nLaunch\nVehicle\n\n1.3.8\n\nFirst Stage\n1.3.8.1\n\nUpper Stage\n1.3.8.2\n\nUpper Stage\nEngine\n\n1.3.8.3\n...\n\nMPS\n1.3.8.2.4\n\n...\n\nUS RCS\n1.3.8.2.5\n\nFS RCS\n1.3.8.2.6\n\nTVCS\n1.3.8.2.7\n\nAvionics\n1.3.8.2.8\n\nSoftware\n1.3.8.2.9\n\n...\nIntegrated\nTest H/W\n\n1.3.8.2.10\n\n...\n\n.1: Integ RCS\n\n.2: Integ Energy\nSupport\n\n.1: Integ RCS .1: Integ TVCS\n\n.2: Actuator\n\n.3: Hydraulic\nPower\n\n.4: APU\n\n.1: Integ Avionics\n\n.2: C&DH System\n\n.3: GN&C H/W\n\n.4: Radio Frequency\nSystem\n\n.5: EPS\n\n.6: Electrical Integration\n\n.7: Develop Flight Instrument\n\n.8: Sensor & Instrument System\n\n.9: EGSE\n\n.10: Integ CLV Avionics System\nElement Testing\n\n.11: Flight Safety System\n\n.1: Integ S/W\nSystem\n\n.2: Flight S/W\n\n.1: MPTA\n\n.2: GVT\n\n.3: STA\n\n.4: US for DTF-1\n\n.5: US for VTF-2\n\n.6: US for RRF-3\n\n.7: Struc. Thermal\nComponent Test\n\nFigure G?1 PBS example\n\nTable G?1 Products Provided by the TA as a Function of Program/Project Phase\n\nGate Product\n\nKDP A\u2014Transition from \nPre-Phase A to Phase A\n\nRequires an assessment of potential technology needs versus current and planned technology \nreadiness levels, as well as potential opportunities to use commercial, academic, and other \ngovernment agency sources of technology. Included as part of the draft integrated baseline.\n\nKDP B\u2014Transition from \nPhase A to Phase B\n\nRequires a technology development plan identifying technologies to be developed, heritage \nsystems to be modified, alternative paths to be pursued, fall-back positions and corresponding \nperformance descopes, milestones, metrics, and key decision points. Incorporated in the \npreliminary project plan.\n\nKDP C\u2014Transition from \nPhase B to Phase C/D\n\nRequires a TRAR demonstrating that all systems, subsystems, and components have achieved \na level of technological maturity with demonstrated evidence of qualification in a relevant \nenvironment.\n\nSource: NPR 7120.5.\n\nThe TMA is performed against the hierarchical break-\ndown of the hardware and software products of the pro-\ngram/project PBS to achieve a systematic, overall un-\n\nderstanding at the system, subsystem, and component \nlevels. (See Figure G-1.)\n\n\n\nAppendix G: Technology Assessment/Insertion\n\nNASA Systems Engineering Handbook ? 295\n\nFigure G?2 Technology assessment process\n\nIdentify systems, sub-\nsystems, and components\n\nper hierarchical product\nbreakdown of the WBS \n\nAssign TRL to subsystems\nbased on lowest TRL of\ncomponents and TRL \n\nstate of integration \n\nAssign TRL to all\ncomponents based on\nassessment of maturity\n\nAssign TRL to systems\nbased on lowest TRL of\n\nsubsystems and TRL \nstate of integration \n\nIdentify all components,\nsubsystems, and systems\n\nthat are at lower TRLs\nthan required by program\n\nBaseline technology\nmaturity assessment\n\nPerform AD2 on all\ncomponents, subsystems,\nand systems that are below\n\nrequisite maturity level\n\nTechnology Development Plan\nCost Plan\n\nSchedule Plan\nRisk Assessment\n\nInputs/Entry Criteria\nIt is extremely important that a TA process be defined \nat the beginning of the program/project and that it be \nperformed at the earliest possible stage (concept devel-\nopment) and throughout the program/project through \nPDR. Inputs to the process will vary in level of detail ac-\ncording to the phase of the program/project, and even \nthough there is a lack of detail in Pre-Phase A, the TA will \ndrive out the major critical technological advancements \nrequired. Therefore, at the beginning of Pre-Phase A, the \nfollowing should be provided:\n\nRefinement of TRL definitions. ?\nDefinition of AD ? 2.\nDefinition of terms to be used in the assessment process. ?\nEstablishment of meaningful evaluation criteria and  ?\nmetrics that will allow for clear identification of gaps \nand shortfalls in performance.\nEstablishment of the TA team. ?\nEstablishment of an independent TA review team. ?\n\nHow to Do Technology Assessment\nThe technology assessment process makes use of basic \nsystems engineering principles and processes. As men-\ntioned previously, it is struc-\ntured to occur within the \nframework of the Product \nBreakdown Structure (PBS) \nto facilitate incorporation of \nthe results. Using the PBS \nas a framework has a two-\nfold benefit\u2014it breaks the \n\u201cproblem\u201d down into sys-\ntems, subsystems, and com-\nponents that can be more \naccurately assessed; and it \nprovides the results of the \nassessment in a format that \ncan readily be used in the \ngeneration of program costs \nand schedules. It can also be \nhighly beneficial in providing \nmilestones and metrics for \nprogress tracking using \nEVM. As discussed above, \nit is a two-step process com-\nprised of (1) the determina-\n\ntion of the current technological maturity in terms of \nTRLs and (2) the determination of the difficulty asso-\nciated with moving a technology from one TRL to the \nnext through the use of the AD2. The overall process \nis iterative, starting at the conceptual level during pro-\ngram Formulation, establishing the initial identifica-\ntion of critical technologies and the preliminary cost, \nschedule, and risk mitigation plans. Continuing on into \nPhase A, it is used to establish the baseline maturity, the \ntechnology development plan and associated costs and \nschedule. The final TA consists only of the TMA and \nis used to develop the TRAR which validates that all \nelements are at the requisite maturity level. (See Fig-\nure G-2.) \n\nEven at the conceptual level, it is important to use the for-\nmalism of a PBS to avoid having important technologies \nslip through the crack. Because of the preliminary nature \nof the concept, the systems, subsystems, and components \nwill be defined at a level that will not permit detailed as-\nsessments to be made. The process of performing the as-\nsessment, however, is the same as that used for subsequent, \nmore detailed steps that occur later in the program/project \nwhere systems are defined in greater detail. \n\nOnce the concept has been formulated and the initial \nidentification of critical technologies made, it is nec-\n\n\n\n296 ? NASA Systems Engineering Handbook\n\nAppendix G: Technology Assessment/Insertion\n\nFigure G?4 Technology readiness levels\n\nTRL 1          Basic principles observed and reported\n\nTRL 2         Technology concept and/or application formulated\n  __\n\nTRL 3 Analytical and experimental critical function and/or\n __ characteristic proof-of-concept\n\nTRL 4 Component and/or breadboard validation in laboratory \n  __ environment \n\nTRL 5 Component and/or breadboard validation in relevant\n  __ environment\n\nTRL 6 System/subsystem model or prototype demonstration\n  __  in a relevant environment (ground or space)\n\nTRL 7 System prototype demonstration in a \n target/space environment\n  __\n\nTRL 8 Actual system completed and \u201c?ight quali?ed\u201d through\n  __ test and demonstration (ground or ?ight)\n\nTRL 9 Actual system \u201c?ight proven\u201d through successful\n  __ mission operations\n\nSystem test, launch, \nand operations\n\nSystem/subsystem\ndevelopment\n\nTechnology\ndemonstration\n\nTechnology\ndevelopment\n\nResearch to prove\nfeasibility\n\nBasic technology\nresearch\n\nessary to perform detailed architecture studies with \nthe Technology Assessment Process intimately inter-\n\nEstablishing TRLs\nTRL is, at its most basic, a description of the perfor-\nmance history of a given system, subsystem, or com-\nponent relative to a set of levels first described at NASA \nHQ in the 1980s. The TRL essentially describes the state \nof the art of a given technology and provides a baseline \nfrom which maturity is gauged and advancement de-\nfined. (See Figure G-4.) Even though the concept of TRL \nhas been around for almost 20 years, it is not well un-\nderstood and frequently misinterpreted. As a result, we \noften undertake programs without fully understanding \neither the maturity of key technologies or what is needed \nto develop them to the required level. It is impossible \nto understand the magnitude and scope of a develop-\nment program without having a clear understanding of \nthe baseline technological maturity of all elements of the \nsystem. Establishing the TRL is a vital first step on the \n\nFigure G?3 Architectural studies and technology \ndevelopment\n\nRequire-\nments\n\nTRL/AD2 Assessment\n\nArchitectural\nStudies\n\nTechnology Maturation \n\nConcepts\nSystem\nDesign\n\nwoven. (See Figure G-3.) The purpose of the architec-\nture studies is to refine end-item system design to meet \nthe overall scientific requirements of the mission. It is \nimperative that there be a continuous relationship be-\ntween architectural studies \nand maturing technology \nadvances. The architectural \nstudies must incorporate \nthe results of the technology \nmaturation, planning for \nalternative paths and iden-\ntifying new areas required \nfor development as the ar-\nchitecture is refined. Simi-\nlarly, it is incumbent upon \nthe technology maturation \nprocess to identify require-\nments that are not feasible \nand development routes \nthat are not fruitful and to \ntransmit that information to \nthe architecture studies in a \ntimely manner. Similarly, it \nis incumbent upon the ar-\nchitecture studies to provide \nfeedback to the technology \ndevelopment process rela-\ntive to changes in require-\nments. Particular attention \nmust be given to \u201cheritage\u201d \nsystems in that they are \noften used in architectures \nand environments different \nfrom those in which they \nwere designed to operate. \n\n\n\nAppendix G: Technology Assessment/Insertion\n\nNASA Systems Engineering Handbook ? 297\n\nway to a successful program. A frequent misconception \nis that in practice it is too difficult to determine TRLs and \nthat when you do it is not meaningful. On the contrary, \nidentifying TRLs can be a straightforward systems engi-\nneering process of determining what was demonstrated \nand under what conditions was it demonstrated. \n\nAt first blush, the TRL descriptions in Figure G-4 ap-\npear to be straightforward. It is in the process of trying \nto assign levels that problems arise. A primary cause of \ndifficulty is in terminology\u2014everyone knows what a \nbreadboard is, but not everyone has the same definition. \nAlso, what is a \u201crelevant environment\u201d? What is relevant \nto one application may or may not be relevant to another. \nMany of these terms originated in various branches of en-\ngineering and had, at the time, very specific meanings to \nthat particular field. They have since become commonly \nused throughout the engineering field and often take dif-\nferences in meaning from discipline to discipline, some \nsubtle, some not so subtle. \u201cBreadboard,\u201d for example, \ncomes from electrical engineering where the original use \nreferred to checking out the functional design of an elec-\ntrical circuit by populating a \u201cbreadboard\u201d with compo-\nnents to verify that the design operated as anticipated. \nOther terms come from mechanical engineering, refer-\nring primarily to units that are subjected to different \nlevels of stress under testing, i.e., qualification, pro-\ntoflight, and flight units. The first step in developing a \nuniform TRL assessment (see Figure G-5) is to define \nthe terms used. It is extremely important to develop and \nuse a consistent set of definitions over the course of the \nprogram/project. \n\nHaving established a common set of terminology, it \nis necessary to proceed to the next step\u2014quantifying \n\u201cjudgment calls\u201d on the basis of past experience. Even \nwith clear definitions there will be the need for judg-\nment calls when it comes time to assess just how similar \na given element is relative to what is needed (i.e., is it \nclose enough to a prototype to be considered a proto-\ntype, or is it more like an engineering breadboard?). De-\nscribing what has been done in terms of form, fit, and \nfunction provides a means of quantifying an element \nbased on its design intent and subsequent performance. \nThe current definitions for software TRLs are contained \nin NPR 7120.8, NASA Research and Technology Program \nand Project Management Requirements.\n\nA third critical element of any assessment relates to the \nquestion of who is in the best position to make judgment \n\ncalls relative to the status of the technology in question. \nFor this step, it is extremely important to have a well-\nbalanced, experienced assessment team. Team members \ndo not necessarily have to be discipline experts. The pri-\nmary expertise required for a TRL assessment is that the \n\nFigure G?5 The TMA thought process\n\nHas an identical unit been successfully\noperated/launched in identical\n\nconfiguration/environment?\n\nHas an identical unit  in a different configuration/\nsystem architecture been successfully operated\nin space or the target environment or launched?\n\nIf so, then this initially drops to TRL 5 until\ndifferences are evaluated.\n\nHas an identical unit been flight qualified but\nnot yet operated in space or the target\n\nenvironment or launched?\n\nHas a prototype unit (or one similar enough to be\nconsidered a prototype) been successfully operated\nin space or the target environment or launched?\n\nHas a prototype unit (or one similar enough \nto be considered a prototype) been \n\ndemonstrated in a relevant environment?\n\nHas a breadboard unit been demonstrated in \na relevant environment?\n\nHas a breadboard unit been demonstrated in \na laboratory environment?\n\nHas analytical and experimental\nproof-of-concept been demonstrated?\n\nHas concept or application\nbeen formulated?\n\nHave basic principles been observed\nand reported?\n\nRETHINK POSITION REGARDING\nTHIS TECHNOLOGY!\n\nNO\n\nNO\n\nNO\n\nNO\n\nNO\n\nNO\n\nNO\n\nNO\n\nNO\n\nNO\n\nTRL 9YES\n\nTRL 5YES\n\nTRL 2YES\n\nTRL 1YES\n\nTRL 3YES\n\nTRL 4YES\n\nTRL 5YES\n\nTRL 6YES\n\nTRL 7YES\n\nTRL 8YES\n\n\n\n298 ? NASA Systems Engineering Handbook\n\nAppendix G: Technology Assessment/Insertion\n\nFigure G?6 TRL assessment matrix\n\nTRL ASSESSMENT\n\nD\nev\n\nel\nop\n\nm\nen\n\nta\nl M\n\nod\nel\n\nBr\nea\n\ndb\noa\n\nrd\n\nBr\nas\n\nsb\noa\n\nrd\n\nPr\not\n\not\nyp\n\ne\n\nCo\nnc\n\nep\nt\n\nDemonstration Units Environment\n\nFl\nig\n\nht\n Q\n\nua\nli?\n\ned\n\nRe\nle\n\nva\nnt\n\n E\nnv\n\niro\nnm\n\nen\nt\n\nSp\nac\n\ne \nEn\n\nvi\nro\n\nnm\nen\n\nt\n\nSp\nac\n\ne/\nLa\n\nun\nch\n\n O\npe\n\nra\ntio\n\nn\n\nLa\nbo\n\nra\nto\n\nry\n E\n\nnv\niro\n\nnm\nen\n\nt\n\nO\nve\n\nra\nll \n\nTR\nL\n\nUnit Description\n\nFo\nrm\n\nFi\nt\n\nFu\nnc\n\ntio\nn\n\nA\npp\n\nro\npr\n\nia\nte\n\n S\nca\n\nle\n\nRed = Below TRL 3\nYellow = TRL 3,4 & 5\nGreen = TRL 6 and above\nWhite = Unknown\n\nX Exists\n\n1.0 System\n   1.1 Subsystem X\n       1.1.1   Mechanical Components\n       1.1.2   Mechanical Systems\n       1.1.3   Electrical Components X X X X X\n       1.1.4   Electrical Systems\n       1.1.5   Control Systems\n       1.1.6   Thermal Systems X X X\n       1.1.7   Fluid Systems X\n       1.1.8   Optical Systems\n       1.1.9   Electro-Optical Systems\n       1.1.10  Software Systems\n       1.1.11  Mechanisms X\n       1.1.12  Integration \n  1.2 Subsystem Y\n     1.2.1 Mechanical Components\n\nD\nev\n\nel\nop\n\nm\nen\n\nta\nl M\n\nod\nel\n\nBr\nea\n\ndb\noa\n\nrd\n\nBr\nas\n\nsb\noa\n\nrd\n\nPr\not\n\not\nyp\n\ne\n\nCo\nnc\n\nep\nt\n\nFl\nig\n\nht\n Q\n\nua\nli?\n\ned\n\nRe\nle\n\nva\nnt\n\n E\nnv\n\niro\nnm\n\nen\nt\n\nSp\nac\n\ne \nEn\n\nvi\nro\n\nnm\nen\n\nt\n\nSp\nac\n\ne/\nLa\n\nun\nch\n\n O\npe\n\nra\ntio\n\nn\n\nLa\nbo\n\nra\nto\n\nry\n E\n\nnv\niro\n\nnm\nen\n\nt\n\nO\nve\n\nra\nll \n\nTR\nL\n\nFo\nrm\n\nFi\nt\n\nFu\nnc\n\ntio\nn\n\nA\npp\n\nro\npr\n\nia\nte\n\n S\nca\n\nle\n\nsystems engineer/user understands the current state of the \nart in applications. Having established a set of definitions, \ndefined a process for quantifying judgment calls, and as-\nsembled an expert assessment team, the process primarily \nconsists of asking the right questions. The flowchart de-\npicted in Figure G-5 demonstrates the questions to ask to \ndetermine TRL at any level in the assessment. \n\nNote the second box particularly refers to heritage sys-\ntems. If the architecture and the environment have \nchanged, then the TRL drops to TRL 5\u2014at least intially. \nAdditional testing may need to be done for heritage sys-\ntems for the new use or new environment. If in subse-\nquent analysis the new environment is sufficiently close to \nthe old environment, or the new architecture sufficiently \nclose to the old architecture then the resulting evaluation \ncould be then TRL 6 or 7, but the most important thing \nto realize is that it is no longer at a TRL 9. Applying this \nprocess at the system level and then proceeding to lower \nlevels of subsystem and component identifies those ele-\n\nments that require development and sets the stage for the \nsubsequent phase, determining the AD2. \n\nA method for formalizing this process is shown in Fig-\nure G-6. Here, the process has been set up as a table: the \nrows identify the systems, subsystems, and components \nthat are under assessment. The columns identify the cate-\ngories that will be used to determine the TRL\u2014i.e., what \nunits have been built, to what scale, and in what environ-\nment have they been tested. Answers to these questions de-\ntermine the TRL of an item under consideration. The TRL \nof the system is determined by the lowest TRL present in \nthe system; i.e., a system is at TRL 2 if any single element in \nthe system is at TRL 2. The problem of multiple elements \nbeing at low TRLs is dealt with in the AD2 process. Note \nthat the issue of integration affects the TRL of every system, \nsubsystem, and component. All of the elements can be at a \nhigher TRL, but if they have never been integrated as a unit, \nthe TRL will be lower for the unit. How much lower de-\npends on the complexity of the integration. \n\n\n\nNASA Systems Engineering Handbook ? 299\n\nAppendix H: Integration Plan Outline\n\nPurpose\nThe integration plan defines the integration and verifica-\ntion strategies for a project interface with the system de-\nsign and decomposition into the lower level elements.1 \nThe integration plan is structured to bring the elements \ntogether to assemble each subsystem and to bring all of \nthe subsystems together to assemble the system/product. \nThe primary purposes of the integration plan are: (1) to \ndescribe this coordinated integration effort that supports \nthe implementation strategy, (2) to describe for the par-\nticipants what needs to be done in each integration step, \nand (3) to identify the required resources and when and \nwhere they will be needed.\n\nQuestions/Checklist\nDoes the integration plan include and cover integra- ?\ntion of all of the components and subsystems of the \nproject, either developed or purchased?\nDoes the integration plan account for all external sys- ?\ntems to be integrated with the system (for example, \ncommunications networks, field equipment, other \n\n1The material in this appendix is adapted from Federal \nHighway Administration and CalTrans, Systems Engineering \nGuidebook for ITS, Version 2.0. \n\n complete systems owned by the government or owned \nby other government agencies)?\nDoes the integration plan fully support the imple- ?\nmentation strategy, for example, when and where the \nsubsystems and system are to be used?\nDoes the integration plan mesh with the verification  ?\nplan?\nFor each integration step, does the integration plan  ?\ndefine what components and subsystems are to be in-\ntegrated?\nFor each integration step, does the integration plan  ?\nidentify all the needed participants and define what \ntheir roles and responsibilities are?\nDoes the integration plan establish the sequence and  ?\nschedule for every integration step?\nDoes the integration plan spell out how integration  ?\nproblems are to be documented and resolved?\n\nIntegration Plan Contents\nTable H-1 outlines the content of the integration plan by \nsection.\n\n\n\n300 ? NASA Systems Engineering Handbook\n\nAppendix H: Integration Plan Outline\n\nSection Contents\n\nTitle page The title page should follow the NASA procedures or style guide. At a minimum, it should contain \nthe following information:\n\nINTEGRATION PLAN FOR THE [insert name of project] AND [insert name of organization] ?\n\n Contract number ?\n\n Date that the document was formally approved ?\n\n The organization responsible for preparing the document ?\n\n Internal document control number, if available ?\n\n Revision version and date issued ?\n\n1.0 Purpose of Docu-\nment\n\nA brief statement of the purpose of this document. It is the plan for integrating the components \nand subsystems of the project prior to verification.\n\n2.0 Scope of Project This section gives a brief description of the planned project and the purpose of the system to be \nbuilt. Special emphasis is placed on the project\u2019s deployment complexities and challenges.\n\n3.0 Integration \nStrategy\n\nThis section informs the reader what the high-level plan is for integration and, most importantly, \nwhy the integration plan is structured the way it is. The integration plan is subject to several, \nsometimes conflicting, constraints. Also, it is one part of the larger process of build, integrate, verify, \nand deploy, all of which must be synchronized to support the same project strategy. So, for even \na moderately complex project, the integration strategy, based on a clear and concise statement \nof the project\u2019s goals and objectives, is described here at a high, but all-inclusive, level. It may also \nbe necessary to describe the analysis of alternative strategies to make it clear why this particular \nstrategy was selected. \n\nThe same strategy is the basis for the build plan, the verification plan, and the deployment plan. \n\nThis section covers and describes each step in the integration process. It describes what compo-\nnents are integrated at each step and gives a general idea of what threads of the operational capa-\nbilities (requirements) are covered. It ties the plan to the previously identified goals and objectives \nso the stakeholders can understand the rationale for each integration step. This summary-level \ndescription also defines the schedule for all the integration efforts.\n\n4.0 Phase 1 Integra-\ntion\n\nThis, and the following sections, define and explain each step in the integration process. The intent \nhere is to identify all the needed participants and to describe to them what they have to do.\n\nIn general, the description of each integration step should identify:\n\nThe location of the activities. ?\n\nThe project-developed equipment and software products to be integrated . Initially this is just a  ?\nhigh-level list, but eventually the list must be exact and complete, showing part numbers and \nquantity. \n\nAny support equipment (special software, test hardware, software stubs, and drivers to simulate  ?\nyet-to-be-integrated software components, external systems) needed for this integration step. \nThe same support equipment is most likely needed for the subsequent verification step.\n\nAll integration activities that need to be performed after installation, including integration with  ?\non-site systems and external systems at other sites.\n\nA description of the verification activities, as defined in the applicable verification plan, that  ?\noccur after this integration step.\n\nThe responsible parties for each activity in the integration step. ?\n\nThe schedule for each activity. ?\n\n5.0 Multiple Phase \nIntegration Steps (1 \nor N steps)\n\nThis, and any needed additional sections, follow the format for Section 3.0. Each covers each step in \na multiple step integration effort.\n\nTable H?1 Integration Plan Contents\n\n\n\nNASA Systems Engineering Handbook ? 301\n\nAppendix I: Verification and Validation Plan  \nSample Outline\n\n1. Introduction\n 1.1 Purpose and Scope\n 1.2 Responsibility and Change Authority\n 1.3 Definitions\n\n2. Applicable and Reference Documents\n 2.1 Applicable Documents\n 2.2 Reference Documents\n 2.3 Order of Precedence\n\n3. System X Description\n 3.1 System X Requirements Flow Down\n 3.2 System X Architecture\n 3.3 End Item Architectures\n  3.3.1 System X End Item A\n  3.3.n System X End Item n\n 3.4 System X Ground Support Equipment\n 3.5 Other Architecture Descriptions\n\n4. Verification and Validation Process\n 4.1 Verification and Validation Management Responsibilities\n 4.2 Verification Methods\n  4.2.1 Analysis\n  4.2.2 Inspection\n  4.2.3 Demonstration \n  4.2.4 Test\n   4.2.4.1 Qualification Testing\n    4.2.4.2 Other Testing\n 4.3 Validation Methods\n 4.4 Certification Process\n 4.5 Acceptance Testing\n\n5. Verification and Validation Implementation\n 5.1 System X Design and Verification and Validation Flow\n 5.2 Test Articles\n 5.3 Support Equipment\n 5.4 Facilities\n\n6. System X End Item Verification and Validation\n 6.1 End Item A\n  6.1.1 Developmental/Engineering Unit Evaluations\n  6.1.2 Verification Activities \n   6.1.2.1 Verification Testing\n    6.1.2.1.1 Qualification Testing\n    6.1.2.1.2 Other Testing\n\n\n\n302 ? NASA Systems Engineering Handbook\n\nAppendix I: Verification and Validation Plan Sample Outline\n\n   6.1.2.2 Verification Analysis\n    6.1.2.2.1 Thermal Analysis\n    6.1.2.2.2 Stress Analysis\n    6.1.2.2.3 Analysis of Fracture Control\n    6.1.2.2.4 Materials Analysis\n    6.1.2.2.5 EEE Parts Analysis\n   6.1.2.3 Verification Inspection\n   6.1.2.4 Verification Demonstration\n  6.1.3 Validation Activities\n  6.1.4 Acceptance Testing\n 6.n End Item n\n\n7. System X Verification and Validation\n 7.1 End-Item-to-End-Item Integration\n  7.1.1 Developmental/Engineering Unit Evaluations\n  7.1.2 Verification Activities\n    7.1.2.1 Verification Testing\n    7.1.2.2 Verification Analysis\n    7.1.2.3 Verification Inspection\n    7.1.2.4 Verification Demonstration\n  7.1.3 Validation Activities\n 7.2 Complete System Integration\n  7.2.1 Developmental/Engineering Unit Evaluations\n  7.2.2 Verification Activities\n    7.2.2.1 Verification Testing\n    7.2.2.2 Verification Analysis\n    7.2.2.3 Verification Inspection\n    7.2.2.4 Verification Demonstration\n  7.2.3 Validation Activities\n\n8. System X Program Verification and Validation\n 8.1 Vehicle Integration\n 8.2 End-to-End Integration\n 8.3 On-Orbit V&V Activities\n\n9. System X Certification Products\n\nAppendix A: Acronyms and Abbreviations\nAppendix B: Definition of Terms\nAppendix C: Requirement Verification Matrix\nAppendix D: System X Validation Matrix\n\n\n\nNASA Systems Engineering Handbook ? 303\n\nAppendix J: SEMP Content Outline\n\nSEMP Content\nThe SEMP is the foundation document for the techni-\ncal and engineering activities conducted during the \nproject. The SEMP conveys information on the technical \nintegration methodologies and activities for the project \nwithin the scope of the project plan to all of the person-\nnel. Because the SEMP provides the specific technical \nand management information to understand the tech-\nnical integration and interfaces, its documentation and \napproval serves as an agreement within the project of \nhow the technical work will be conducted. The technical \nteam, working under the overall program/project plan, \ndevelops and updates the SEMP as necessary. The tech-\nnical team works with the project manager to review the \ncontent and obtain concurrence. The SEMP includes the \nfollowing three general sections:\n\nTechnical program planning and control, which de- ?\nscribes the processes for planning and control of the \nengineering efforts for the design, development, test, \nand evaluation of the system.\nSystems engineering processes, which includes spe- ?\ncific tailoring of the systems engineering process as \ndescribed in the NPR, implementation procedures, \ntrade study methodologies, tools, and models to be \nused.\nEngineering specialty integration describes the in- ?\ntegration of the technical disciplines\u2019 efforts into the \nsystems engineering process and summarizes each \ntechnical discipline effort and cross references each of \nthe specific and relevant plans.\n\nPurpose and Scope\nThis section provides a brief description of the purpose, \nscope, and content of the SEMP. The scope encompasses \nthe SE technical effort required to generate the work \nproducts necessary to meet the success criteria for the \nproduct-line life-cycle phases. The SEMP is a plan for \ndoing the project technical effort by a technical team for \na given WBS model in the system structure and to help \nmeet life-cycle phase success criteria.\n\nApplicable Documents\nThis section of the SEMP lists the documents applicable \nto this specific project and its SEMP implementation and \ndescribes major standards and procedures that this tech-\nnical effort for this specific project needs to follow. Spe-\ncific implementation of standardization tasking is incor-\nporated into pertinent sections of the SEMP.\n\nProvide the engineering standards and procedures to be \nused in the project. Examples of specific procedures could \ninclude any hazardous material handling, crew training \nfor control room operations, special instrumentation \ntechniques, special interface documentation for vehicles, \nand maintenance procedures specific to the project. \n\nTechnical Summary\nThis section contains an executive summary describing \nthe problem to be solved by this technical effort and the \npurpose, context, and products of the WBS model to be \ndeveloped and integrated with other interfacing systems \nidentified.\n\nSystem Description\nThis section contains a definition of the purpose/mis-\nsion/objective of the system being developed, a brief \ndescription of the purpose of the products of the WBS \nmodels of the system structure for which this SEMP ap-\nplies, and the expected scenarios for the system. Each \nWBS model includes the system end products and their \nsubsystems and the supporting or enabling products and \nany other work products (plans, baselines) required for \nthe development of the system. The description should \ninclude any interfacing systems and system products, \nincluding humans, with which the WBS model system \nproducts will interact physically, functionally, or elec-\ntronically. \n\nIdentify and document system constraints, including \ncost, schedule, and technical (for example, environ-\nmental, design).\n\n\n\n304 ? NASA Systems Engineering Handbook\n\nAppendix J: SEMP Content Outline\n\nSystem Structure\nThis section contains an explanation of how the WBS \nmodels will be developed, how the resulting WBS model \nwill be integrated into the project WBS, and how the \noverall system structure will be developed. This section \ncontains a description of the relationship of the specifi-\ncation tree and the drawing tree with the products of the \nsystem structure and how the relationship and interfaces \nof the system end products and their life-cycle-enabling \nproducts will be managed throughout the planned tech-\nnical effort.\n\nProduct Integration\nThis subsection contains an explanation of how the \nproduct will be integrated and will describe clear organi-\nzational responsibilities and interdependencies whether \nthe organizations are geographically dispersed or man-\naged across Centers. This includes identifying organiza-\ntions\u2014intra- and inter-NASA, other Government agen-\ncies, contractors, or other partners\u2014and delineating \ntheir roles and responsibilities.\n\nWhen components or elements will be available for inte-\ngration needs to be clearly understood and identified on \nthe schedule to establish critical schedule issues.\n\nPlanning Context\nThis subsection contains the product-line life-cycle \nmodel constraints (e.g., NPR 7120.5) that affect the plan-\nning and implementation of the common technical pro-\ncesses to be applied in performing the technical effort. \nThe constraints provide a linkage of the technical effort \nwith the applicable product-line life-cycle phases cov-\nered by the SEMP including, as applicable, milestone de-\ncision gates, major technical reviews, key intermediate \nevents leading to project completion, life-cycle phase, \nevent entry and success criteria, and major baseline and \nother work products to be delivered to the sponsor or \ncustomer of the technical effort.\n\nBoundary of Technical Effort\nThis subsection contains a description of the boundary \nof the general problem to be solved by the technical ef-\nfort. Specifically, it identifies what can be controlled by \nthe technical team (inside the boundary) and what influ-\nences the technical effort and is influenced by the tech-\nnical effort but not controlled by the technical team (out-\nside the boundary). Specific attention should be given to \n\nphysical, functional, and electronic interfaces across the \nboundary.\n\nDefine the system to be addressed. A description of the \nboundary of the system can include the following: defi-\nnition of internal and external elements/items involved \nin realizing the system purpose as well as the system \nboundaries in terms of space, time, physical, and oper-\national. Also, identification of what initiates the transi-\ntions of the system to operational status and what initi-\nates its disposal is important. The following is a general \nlisting of other items to include, as appropriate:\n\nGeneral and functional descriptions of the subsys- ?\ntems,\nDocument current and established subsystem perfor- ?\nmance characteristics,\nIdentify and document current interfaces and charac- ?\nteristics,\nDevelop functional interface descriptions and func- ?\ntional flow diagrams,\nIdentify key performance interface characteristics, and ?\nIdentify current integration strategies and architecture. ?\n\nCross References\nThis subsection contains cross references to appropriate \nnontechnical plans and critical reference material that \ninterface with the technical effort. It contains a summary \ndescription of how the technical activities covered in \nother plans are accomplished as fully integrated parts of \nthe technical effort.\n\nTechnical Effort Integration\nThis section contains a description of how the various \ninputs to the technical effort will be integrated into a co-\nordinated effort that meets cost, schedule, and perfor-\nmance objectives.\n\nThe section should describe the integration and coordi-\nnation of the specialty engineering disciplines into the \nsystems engineering process during each iteration of the \nprocesses. Where there is potential for overlap of special-\nty efforts, the SEMP should define the relative responsi-\nbilities and authorities of each. This section should con-\ntain, as needed, the project\u2019s approach to the following:\n\nConcurrent engineering, ?\nThe activity phasing of specialty engineering, ?\nThe participation of specialty disciplines, ?\nThe involvement of specialty disciplines, ?\n\n\n\nAppendix J: SEMP Content Outline\n\nNASA Systems Engineering Handbook ? 305\n\nThe role and responsibility of specialty disciplines, ?\nThe participation of specialty disciplines in system  ?\ndecomposition and definition,\nThe role of specialty disciplines in verification and  ?\nvalidation,\nReliability, ?\nMaintainability, ?\nQuality assurance, ?\nIntegrated logistics, ?\nHuman engineering, ?\nSafety, ?\nProducibility, ?\nSurvivability/vulnerability, ?\nNational Environmental Policy Act compliance, and ?\nLaunch approval/flight readiness. ?\n\nProvide the approach for coordination of diverse technical \ndisciplines and integration of the development tasks. For \nexample, this can include the use of integrated teaming \napproaches. Ensure that the specialty engineering disci-\nplines are properly represented on all technical teams and \nduring all life-cycle phases of the project. Define the scope \nand timing of the specialty engineering tasks.\n\nResponsibility and Authority\nThis subsection contains a description of the organizing \nstructure for the technical teams assigned to this technical \neffort and includes how the teams will be staffed and man-\naged, including (1) what organization/panel will serve as \nthe designated governing authority for this project and, \ntherefore, will have final signature authority for this SEMP; \n(2) how multidisciplinary teamwork will be achieved; \n(3) identification and definition of roles, responsibilities, \nand authorities required to perform the activities of each \nplanned common technical process; (4) planned technical \nstaffing by discipline and expertise level, with human re-\nsource loading; (5) required technical staff training; and \n(6) assignment of roles, responsibilities, and authorities to \nappropriate project stakeholders or technical teams to en-\nsure planned activities are accomplished.\n\nProvide an organization chart and denote who on the \nteam is responsible for each activity. Indicate the lines \nof authority and responsibility. Define the resolution au-\nthority to make decisions/decision process. Show how \nthe engineers/engineering disciplines relate.\n\nThe systems engineering roles and responsibilities need \nto be addressed for the following: project office, user, \nContracting Office Technical Representative (COTR), \n\nsystems engineering, design engineering, specialty engi-\nneering, and contractor.\n\nContractor Integration\nThis subsection contains a description of how the tech-\nnical effort of in-house and external contractors is to be \nintegrated with the NASA technical team efforts. This \nincludes establishing technical agreements, monitoring \ncontractor progress against the agreement, handling \ntechnical work or product requirements change requests, \nand acceptance of deliverables. The subsection will spe-\ncifically address how interfaces between the NASA tech-\nnical team and the contractor will be implemented for \neach of the 17 common technical processes. For ex-\nample, it addresses how the NASA technical team will be \ninvolved with reviewing or controlling contractor-gen-\nerated design solution definition documentation or how \nthe technical team will be involved with product verifica-\ntion and product validation activities.\n\nKey deliverables for the contractor to complete their \nsystems and those required of the contractor for other \nproject participants need to be identified and established \non the schedule.\n\nSupport Integration\nThis subsection contains a description of the methods \n(such as integrated computer-aided tool sets, integrated \nwork product databases, and technical management in-\nformation systems) that will be used to support technical \neffort integration.\n\nCommon Technical Processes \nImplementation\nEach of the 17 common technical processes will have a \nseparate subsection that contains a plan for performing \nthe required process activities as appropriately tailored. \n(See NPR 7123.1 for the process activities required and \ntailoring.) Implementation of the 17 common technical \nprocesses includes (1) the generation of the outcomes \nneeded to satisfy the entry and success criteria of the \napplicable product-line life-cycle phase or phases iden-\ntified in D.4.4.4 and (2) the necessary inputs for other \ntechnical processes. These sections contain a description \nof the approach, methods, and tools for: \n\nIdentifying and obtaining adequate human and non- ?\nhuman resources for performing the planned process, \n\n\n\n306 ? NASA Systems Engineering Handbook\n\nAppendix J: SEMP Content Outline\n\ndeveloping the work products, and providing the ser-\nvices of the process.\nAssigning responsibility and authority for performing  ?\nthe planned process, developing the work products, \nand providing the services of the process.\nTraining the technical staff performing or supporting  ?\nthe process, where training is identified as needed.\nDesignating and placing designated work products of  ?\nthe process under appropriate levels of configuration \nmanagement.\nIdentifying and involving stakeholders of the process. ?\nMonitoring and controlling the process. ?\nIdentifying, defining, and tracking metrics and suc- ?\ncess. \nObjectively evaluating adherence of the process and  ?\nthe work products and services of the process to the \napplicable requirements, objectives, and standards \nand addressing noncompliance.\nReviewing activities, status, and results of the process  ?\nwith appropriate levels of management and resolving \nissues.\n\nThis section should also include the project-specific \ndescription of each of the 17 processes to be used, in-\ncluding the specific tailoring of the requirements to the \nsystem and the project; the procedures to be used in \nimplementing the processes; in-house documentation; \ntrade study methodology; types of mathematical and/or \nsimulation models to be used; and generation of speci-\nfications.\n\nTechnology Insertion\nThis section contains a description of the approach and \nmethods for identifying key technologies and their as-\nsociated risks and criteria for assessing and inserting \ntechnologies, including those for inserting critical \ntechnologies from technology development projects. \nAn approach should be developed for appropriate level \nand timing of technology insertion. This could include \nalternative approaches to take advantage of new tech-\nnologies to meet systems needs as well as alternative \noptions if the technologies do not prove appropriate in \nresult or timing. The strategy for an initial technology \nassessment within the scope of the project require-\nments should be provided to identify technology con-\nstraints for the system. \n\nAdditional SE Functions and \nActivities\nThis section contains a description of other areas not \nspecifically included in previous sections but that are es-\nsential for proper planning and conduct of the overall \ntechnical effort.\n\nSystem Safety\nThis subsection contains a description of the approach \nand methods for conducting safety analysis and assessing \nthe risk to operators, the system, the environment, or the \npublic.\n\nEngineering Methods and Tools\nThis subsection contains a description of the methods \nand tools not included in the technology insertion sec-\ntion that are needed to support the overall technical ef-\nfort and identifies those tools to be acquired and tool \ntraining requirements.\n\nDefine the development environment for the project, in-\ncluding automation and software tools. If required, de-\nvelop and/or acquire the tools and facilities for all disci-\nplines on the project. Standardize when possible across \nthe project, or enable a common output format of the \ntools that can be used as input by a broad range of tools \nused on the project. Define the requirements for infor-\nmation management systems and for using existing ele-\nments. Define and plan for the training required to use \nthe tools and technology across the project.\n\nSpecialty Engineering\nThis subsection contains a description of engineering \ndiscipline and specialty requirements that apply across \nprojects and the WBS models of the system structure. \nExamples of these requirement areas would include \nplanning for safety, reliability, human factors, logistics, \nmaintainability, quality, operability, and supportability. \nEstimate staffing levels for these disciplines and incorpo-\nrate with the project requirements. \n\nIntegration with the Project Plan and \nTechnical Resource Allocation\nThis section contains how the technical effort will inte-\ngrate with project management and defines roles and re-\nsponsibilities. It addresses how technical requirements \n\n\n\nAppendix J: SEMP Content Outline\n\nNASA Systems Engineering Handbook ? 307\n\nwill be integrated with the project plan to determinate \nthe allocation of resources, including cost, schedule, and \npersonnel, and how changes to the allocations will be co-\nordinated. \n\nThis section describes the interface between all of the \ntechnical aspects of the project and the overall project \nmanagement process during the systems engineering \nplanning activities and updates. All activities to coor-\ndinate technical efforts with the overall project are in-\ncluded, such as technical interactions with the external \nstakeholders, users, and contractors. \n\nWaivers\nThis section contains all approved waivers to the Center \nDirector\u2019s Implementation Plan for SE NPR 7123.1 re-\nquirements for the SEMP. This section also contains a \nseparate subsection that includes any tailored SE NPR \nrequirements that are not related and able to be docu-\nmented in a specific SEMP section or subsection.\n\nAppendices\nAppendices are included, as necessary, to provide a glos-\nsary, acronyms and abbreviations, and information pub-\nlished separately for convenience in document main-\n\ntenance. Included would be: (1) information that may \nbe pertinent to multiple topic areas (e.g., description of \nmethods or procedures); (2) charts and proprietary data \napplicable to the technical efforts required in the SEMP; \nand (3) a summary of technical plans associated with the \nproject. Each appendix should be referenced in one of \nthe sections of the engineering plan where data would \nnormally have been provided.\n\nTemplates\nAny templates for forms, plans, or reports the technical \nteam will need to fill out, like the format for the verifica-\ntion and validation plan, should be included in the ap-\npendices.\n\nReferences\nThis section contains all documents referenced in the \ntext of the SEMP.\n\nSEMP Preparation Checklist\nThe SEMP, as the key reference document capturing the \ntechnical planning, needs to address some basic topics. \nFor a generic SEMP preparation checklist, refer to Sys-\ntems Engineering Guidebook by James Martin.\n\n\n\n308 ? NASA Systems Engineering Handbook\n\nAppendix K: Plans\n\nActivity Plan ..........................................................................................................................   187\nBaseline Plan .........................................................................................................................   111\nBuild Plan ..............................................................................................................................   300\nClosure Plan ..........................................................................................................................   178\nConfiguration Management Plan ..............................................................................  176, 311\nCost Account Plan ...............................................................................................................   121\nData Management Plan.......................................................................................................   158\nDeployment Plan .................................................................................................................   300\nEarned Value Management Plan .......................................................................................   166\nEngineering Plan ..................................................................................................................   307\nImplementation Plan ...........................................................................................................   148\nInstallation Plan ...................................................................................................................   230\nIntegration Plan ....................................................................................................................   299\nInterface Control Plan ...........................................................................................................   81\nLaunch and Early Orbit Plan ...............................................................................................   35\nLife-Cycle Cost Management Plan....................................................................................   129\nLogistics Support Plan...........................................................................................................   26\nMission Operations Plan ......................................................................................................   26\nOperations Plan ......................................................................................................................   35\nProduction Plan .....................................................................................................................   25\nProgram Plan ..........................................................................................................................   19\nProject Plan ...........................................................................................................................   112\nProject Protection Plan ...............................................................................................  260, 321\nQuality Control Plan .............................................................................................................   24\nRequirements Management Plan ......................................................................................   134\nReview Plan ...........................................................................................................................   169\nRisk Control Plan .................................................................................................................   142\nRisk Management Plan .......................................................................................................   140\nRisk Mitigation Plan ............................................................................................................   295\nSoftware Development Plan ...............................................................................................   104\nSoftware IV&V Plan ............................................................................................................   105\nSource Evaluation Plan .......................................................................................................   219\nStrategic Plan ........................................................................................................................   152\nSurveillance Plan ..................................................................................................................   225\nSystem and Subsystem Test Plans .......................................................................................   42\nSystems Decommissioning/Disposal Plan...........................................................................   7\nSystems Engineering Management Plan (SEMP) ...................................................  113, 303\nTechnology Development Plan ..........................................................................................   277\nTest Plan ................................................................................................................................   230\nTime-Phased Resource Plan ..............................................................................................   190\nTransition Plan .....................................................................................................................   230\nTransportation Plan .............................................................................................................   187\nValidation Plan .............................................................................................................  100, 284\nVerification Plan .....................................................................................................................   83\n\n\n\nNASA Systems Engineering Handbook ? 309\n\nAppendix L: Interface Requirements  \nDocument Outline\n\n1.0 Introduction \n 1.1 Purpose and Scope. State the purpose of this document and briefly identify the interface to be defined. (For \n\nexample, \u201cThis IRD defines and controls the interface(s) requirements between ______ and _____.\u201d) \n 1.2 Precedence. Define the relationship of this document to other program documents and specify which is \n\ncontrolling in the event of a conflict. \n 1.3 Responsibility and Change Authority. State the responsibilities of the interfacing organizations for devel-\n\nopment of this document and its contents. Define document approval authority (including change approval \nauthority). \n\n2.0 Documents\n 2.1 Applicable Documents. List binding documents that are invoked to the extent specified in this IRD. The \n\nlatest revision or most recent version should be listed. Documents and requirements imposed by higher\u2013level \ndocuments (higher order of precedence) should not be repeated. \n\n2.2 Reference Documents. List any document that is referenced in the text in this subsection. \n3.0 Interfaces \n\n3.1 General. In the subsections that follow, provide the detailed description, responsibilities, coordinate systems, \nand numerical requirements as they relate to the interface plane. \n3.1.1 Interface Description. Describe the interface as defined in the system specification. Use tables, fig-\n\nures, or drawings as appropriate.\n3.1.2 Interface Responsibilities. Define interface hardware and interface boundary responsibilities to de-\n\npict the interface plane. Use tables, figures, or drawings as appropriate. \n3.1.3 Coordinate Systems. Define the coordinate system used for interface requirements on each side of the \n\ninterface. Use tables, figures, or drawings as appropriate.\n3.1.4 Engineering Units, Tolerances, and Conversions. Define the measurement units along with toler-\n\nances. If required, define the conversion between measurement systems.\n3.2 Interface Requirements. In the subsections that follow, define structural limiting values at the interface, such \n\nas interface loads, forcing functions, and dynamic conditions.\n3.2.1 Interface Plane. Define the interface requirements on each side of the interface plane. \n\n3.2.1.1 Envelope \n3.2.1.2 Mass Properties. Define the derived interface requirements based on the allocated require-\n\nments contained in the applicable specification pertaining to that side of the interface. For ex-\nample, this subsection should cover the mass of the element.\n\n3.2.1.3 Structural/Mechanical. Define the derived interface requirements based on the allocated re-\nquirements contained in the applicable specification pertaining to that side of the interface. \nFor example, this subsection should cover attachment, stiffness, latching, and mechanisms.\n\n3.2.1.4 Fluid. Define the derived interface requirements based on the allocated requirements con-\ntained in the applicable specification pertaining to that side of the interface. For example, this \nsubsection should cover fluid areas such as thermal control, O2 and N2, potable and waste \nwater, fuel cell water, and atmospheric sampling.\n\n\n\n310 ? NASA Systems Engineering Handbook\n\nAppendix L: Interface Requirements Document Outline\n\n3.2.1.5 Electrical (Power). Define the derived interface requirements based on the allocated require-\nments contained in the applicable specification pertaining to that side of the interface. For ex-\nample, this subsection should cover various electric current, voltage, wattage, and resistance \nlevels.\n\n3.2.1.6 Electronic (Signal). Define the derived interface requirements based on the allocated re-\nquirements contained in the applicable specification pertaining to that side of the interface. \nFor example, this subsection should cover various signal types such as audio, video, command \ndata handling, and navigation. \n\n3.2.1.7 Software and Data. Define the derived interface requirements based on the allocated require-\nments contained in the applicable specification pertaining to that side of the interface. For ex-\nample, this subsection should cover various data standards, message timing, protocols, error \ndetection/correction, functions, initialization, and status.\n\n3.2.1.8 Environments. Define the derived interface requirements based on the allocated require-\nments contained in the applicable specification pertaining to that side of the interface. For \nexample, cover the dynamic envelope measures of the element in English units or the metric \nequivalent on this side of the interface. \n\n 3.2.1.8.1 Electromagnetic Effects\n3.2.1.8.1.a Electromagnetic Compatibility. Define the appropriate electro-\n\nmagnetic compatibility requirements. For example, end-item-1-to-\nend-item-2 interface shall meet the requirements [to be determined] \nof systems requirements for electromagnetic compatibility.\n\n3.2.1.8.1.b Electromagnetic Interference. Define the appropriate electromag-\nnetic interference requirements. For example, end-item-1-to-end-\nitem-2 interface shall meet the requirements [to be determined] of \nelectromagnetic emission and susceptibility requirements for elec-\ntromagnetic compatibility.\n\n3.2.1.8.1.c Grounding.  Define the appropriate grounding requirements. For \nexample, end-item-1-to-end-item-2 interface shall meet the re-\nquirements [to be determined] of grounding requirements.\n\n3.2.1.8.1.d Bonding. Define the appropriate bonding requirements. For ex-\nample, end-item-1-to-end-item-2 structural/mechanical interface \nshall meet the requirements [to be determined] of electrical bonding \nrequirements.\n\n3.2.1.8.1.e Cable and Wire Design. Define the appropriate cable and wire de-\nsign requirements. For example, end-item-1-to-end-item-2 cable \nand wire interface shall meet the requirements [to be determined] \nof cable/wire design and control requirements for electromagnetic \ncompatibility. \n\n3.2.1.8.2 Acoustic. Define the appropriate acoustics requirements. Define the acoustic noise \nlevels on each side of the interface in accordance with program or project require-\nments. \n\n3.2.1.8.3 Structural Loads. Define the appropriate structural loads requirements. Define \nthe mated loads that each end item must accommodate. \n\n3.2.1.8.4 Vibroacoustics. Define the appropriate vibroacoustics requirements. Define the \nvibroacoustic loads that each end item must accommodate. \n\n3.2.1.9 Other Types of Interface Requirements. Define other types of unique interface requirements \nthat may be applicable. \n\n\n\nNASA Systems Engineering Handbook ? 311\n\nAppendix M: CM Plan Outline\n\nSection Description\n\n1.0 Introduction This section includes:\n\nThe purpose and scope of the CM plan and the program phases to which it applies ?\n\nBrief description of the system or top-level configuration items ?\n\n2.0 Applicable and Reference \nDocuments\n\nThis section includes a list of the specifications, standards, manuals, and other docu-\nments, referenced in the plan by title, document number, issuing authority, revision, \nand as applicable, change notice, amendment, and issue date.\n\n3.0 CM Concepts and Organization This section includes:\n\nCM objectives ?\n\nInformation needed to support the achievement of objectives in the current and  ?\nfuture phases\n\nDescription and graphic portraying the project\u2019s planned organization with  ?\nemphasis on the CM activities\n\n4.0 CM Process\n\nCM Management and Planning ?\n\nConfiguration Identification ?\n\nConfiguration Control ?\n\nConfiguration Status Accounting ?\n\nConfiguration Audits ?\n\nThis section includes a description of the project\u2019s CM process for accomplishing the \nfive CM activities, which includes but is not limited to: \n\nCM activities for the current and future phases ?\n\nBaselines ?\n\nConfiguration items ?\n\nEstablishment and membership of configuration control boards ?\n\nNomenclature and numbering ?\n\nHardware/software identification ?\n\nFunctional configuration audits and physical configuration audits ?\n\n5.0 Management of Configuration \nData\n\nThis section describes the methods for meeting the CM technical data requirements.\n\n6.0 Interface Management This section includes a description on how CM will maintain and control interface \ndocumentation.\n\n7.0 CM Phasing and Schedule This section describes milestones for implementing CM commensurate with major \nprogram milestones.\n\n8.0 Subcontractor/Vendor Control This section describes methods used to ensure subcontractor/vendors comply with \nCM requirements.\n\nA typical CM plan should include the following:\n\nTable M?1 CM Plan Outline\n\n\n\n312 ? NASA Systems Engineering Handbook\n\nAppendix N: Guidance on Technical Peer  \nReviews/Inspections \n\nIntroduction\nThe objective of technical peer reviews/inspections is to \nremove defects as early as possible in the development \nprocess. Peer reviews/inspections are a well defined re-\nview process for finding and fixing defects, conducted by \na team of peers with assigned roles, each having a vested \ninterest in the work product under review. Peer re-\nviews/inspections are held within development phases, \nbetween milestone reviews, on completed products or \ncompleted portions of products. The results of peer re-\nviews/inspections can be reported at milestone reviews. \nChecklists are heavily utilized in peer reviews/inspec-\ntions to improve the quality of the review. \n\nTechnical peer reviews/inspections have proven over \ntime to be one of the most effective practices available for \nensuring quality products and on-time deliveries. Many \nstudies have demonstrated their benefits, both within \nNASA and across industry. Peer reviews/inspections im-\nprove quality and reduce cost by reducing rework. The \n\nstudies have shown that the rework effort saved not only \npays for the effort spent on inspections, but also provides \nadditional cost savings on the project. By removing de-\nfects at their origin (e.g., requirements and design doc-\numents, test plans and procedures, software code, etc.), \ninspections prevent defects from propagating through \nmultiple phases and work products, and reduce the \noverall amount of rework necessary on projects. In addi-\ntion, improved team efficiency is a side effect of peer re-\nviews/inspections (e.g., by improving team communica-\ntion, more quickly bringing new members up to speed, \nand educating project members about effective develop-\nment practices).\n\nHow to Perform Technical Peer \nReviews/Inspections \nFigure N-1 shows a diagram of the peer review/inspec-\ntion stages, and the text below the figure explains how to \nperform each of the stages. (Figure N-2, at the end of the \n\nThird Hour\n\n  =  Process Stage\n\n=  Person\n=  Stage Transition\n\n=  Form\n\nOverview\nMeeting Moderator\n\nPlanning\n\nAuthor\n\n= Optional Stage\n\nModerator\n\nModerator\n\nAuthor\n\nRework\n\nAuthor\n\nFollow-Up\n\nModerator\n\nAuthor\n\nAuthor\n\nOthers\n\nInspectors\n\nPreparation\n\nModerator\n\nInspectors\n\nInspection\nMeeting \n\nModerator\n\nAuthor\nReader\n\nRecorder\n\nInspectors\n\nInspectors\n\nInspection\nAnnounce-\n\nment\n\nIndividual\nPreparation\n\nLogs\n\nDetailed\nInspection\n\nReport\n\nInspection\nSummary\n\nReport\n\nInspection\nDefect\n\nList\n\nFigure N?1 The peer review/inspection process\n\n\n\nAppendix N: Guidance on Technical Peer Reviews/Inspections\n\nNASA Systems Engineering Handbook ? 313\n\nappendix, summarizes the information as a quick refer-\nence guide.)\n\nIt is recommended that the moderator review the Plan-\nning Inspection Schedule and Estimating Staff Hours, \nGuidelines for Successful Inspections, and 10 Basic \nRules of Inspections in Figure N-2 before beginning the \nplanning stage. (Note: NPR 7150.2, NASA Software En-\ngineering Requirements defines Agency requirements on \nthe use of peer reviews and inspections for software de-\nvelopment. NASA peer review/inspection training is of-\nfered by the NASA Office of the Chief Engineer.)\n\nA. Planning\nThe moderator of the peer review/inspection performs \nthe following activities.1 \n\nDetermine whether peer review/inspection entrance 1. \ncriteria have been met.\nDetermine whether an overview of the product is 2. \nneeded.\nSelect the peer review/inspection team and assign 3. \nroles. For guidance on roles, see Roles of Participants \nin Figure N-2 at the end of this appendix. Reviewers \nhave a vested interest in the work product (e.g., they \nare peers representing areas of the life cycle affected \nby the material being reviewed).\nDetermine if the size of the product is within the 4. \nprescribed guidelines for the type of inspection. (See \nMeeting Rate Guidelines in Figure N-2 for guide-\nlines on the optimal number of pages or lines of \ncode to inspect for each type of inspection.) If the \nproduct exceeds the prescribed guidelines, break the \nproduct into parts and inspect each part separately. \n(It is highly recommended that the peer review/in-\nspection meeting not exceed 2 hours.)\nSchedule the overview (if one is needed).5. \n\n1Langley Research Center, Instructional Handbook for \nFormal Inspections. This document provides more detailed in-\nstructions on how to perform technical peer reviews/inspec-\ntions. It also provides templates for the forms used in the peer \nreview/inspection process described above: inspection an-\nnouncement, individual preparation log, inspection defect \nlist, detailed inspection report, and the inspection summary \nreport.\n\nSchedule peer review/inspection meeting time and 6. \nplace.\nPrepare and distribute the inspection announcement 7. \nand package. Include in the package the product to \nbe reviewed and the appropriate checklist for the \npeer review/inspection.\nRecord total time spent in planning.*8. \n\nB. Overview Meeting\nModerator runs the meeting, and the author pres-1. \nents background information to the reviewers. \nRecord total time spent in the overview.* 2. \n\nC. Peer Review/Inspection Preparation\nPeers review the checklist definitions of defects.1. \nExamine materials for understanding and possible 2. \ndefects.\nPrepare for assigned role in peer review/inspection.3. \nComplete and turn in individual preparation log to 4. \nthe moderator.\nThe moderator reviews the individual preparation 5. \nlogs and makes Go or No-Go decision and organizes \ninspection meeting.\nRecord total time spent in the preparation.* 6. \n\nD. Peer Review/Inspection Meeting\nThe moderator introduces people and identifies 1. \ntheir peer review/inspection roles. \nThe reader presents work products to the peer review/2. \ninspection team in a logical and orderly manner.\nPeer reviewers/inspectors find and classify defects 3. \nby severity, category, and type. (See Classification of \nDefects in Figure N-2.)\nThe recorder writes the major and minor defects on 4. \nthe inspection defect list (for definitions of major \nand minor, see the Severity section of Figure N.2).\nSteps 1 through 4 are repeated until the review of the 5. \nproduct is completed.\nOpen issues are assigned to peer reviewers/inspec-6. \ntors if irresolvable discrepancies occur.\nSummarize the number of defects and their classifi-7. \ncation on the detailed inspection report. \nDetermine the need for a reinspection or third hour. 8. \nOptional: Trivial defects (e.g., redlined documents) \ncan be given directly to the author at the end of the \ninspection.\n\nNote: Where activities have an *, the moderator re-\ncords the time on the inspection summary report.\n\n\n\n314 ? NASA Systems Engineering Handbook\n\nAppendix N: Guidance on Technical Peer Reviews/Inspections\n\nThe moderator obtains an estimate for rework time 9. \nand completion date from the author, and does the \nsame for action items if appropriate.\nThe moderator assigns writing of change requests 10. \nand/or problem reports (if needed).\nRecord total time spent in the peer review/inspec-11. \ntion meeting.* \n\nE. Third Hour\nComplete assigned action items and provide infor-1. \nmation to the author. \nAttend third hour meeting at author\u2019s request.2. \nProvide time spent in third-hour to the moder-3. \nator.*\n\nF. Rework\nAll major defects noted in the inspection defect list 1. \nare resolved by the author.\n\nMinor and trivial defects (which would not result in 2. \nfaulty execution) are resolved at the discretion of the \nauthor as time and cost permit. \nRecord total time spent in the rework on the inspec-3. \ntion defect list.\n\nG. Followup\nThe moderator verifies all major defects have been 1. \ncorrected and no secondary defects have been intro-\nduced.\nThe moderator ensures all open issues are resolved 2. \nand verifies all success criteria for the peer review/\ninspection are met.\nRecord total time spent in rework and followup.*3. \nFile the inspection package.4. \nThe inspection summary report is distributed.5. \nCommunicate that the peer review/inspection has 6. \nbeen passed.\n\n\n\nAppendix N: Guidance on Technical Peer Reviews/Inspections\n\nNASA Systems Engineering Handbook ? 315\n\nClassification of Defects\n\nSeverity\n\nMajor\n\n\u2022 An error that would cause a malfunction or\nprevents attainment of an expected or specified\nresult. \n\n\u2022 Any error that would in the future result in an   \napproved change request or failure report.\n\nMinor\n\n\u2022 A violation of standards, guidelines, or rules \nthat would not result in a deviation from \nrequirements if not corrected, but could result in \ndifficulties in terms of operations, maintenance, \nor future development .\n\nTrivial\n\n\u2022 Editorial errors such as spelling, punctuation, and \ngrammar that do not cause errors or change \nrequests. Recorded only as redlines. Presented\ndirectly to author.\n\nAuthor is required to correct all major defects  \nand should correct minor and trivial defects as\ntime and cost permit.\n\nCategory\n\n\u2022 Missing          \u2022 Wrong            \u2022 Extra\n\nType\n\nTypes of defects are derived from headings on \nchecklist used for the inspection. Defect types can\nbe standardized across inspections from all phases \nof the life cycle. A suggested standard set of defect \ntypes are:\n\n\u2022 Clarity\n\u2022 Completeness\n\u2022 Compliance\n\u2022 Consistency\n\u2022 Correctness/ \n\nLogic\n\u2022 Data Usage\n\u2022 Fault Tolerance \n\u2022 Functionality\n\n\u2022 Interface\n\u2022 Level of Detail\n\u2022 Maintainability\n\u2022 Performance\n\u2022 Reliability\n\u2022 Testability\n\u2022 Traceability\n\u2022 Other\n\nEXAMPLE\nThe following is an example of a defect classification \nthat would be recorded on the Inspection Defect List:\n\nDescription Classification\n\nType\n\nWrongMinor Defect\n\nExtraOpen Issue\n\nMissingMajor Defect X \n\nX \n\nData Usage\n\nOrigin\n\nLine 169 \u2013  While counting \nthe number of leading \nspaces in variable NAME, \nthe wrong \u201cI\u201d is used to \ncalculate \u201cJ.\u201d\n\nRoles of Participants\nModerator\nResponsible for conducting inspection process and\ncollecting inspection data. Plays key role in all stages\nof process except rework. Required to perform special\nduties during an inspection in addition to inspector\u2019s\ntasks.\n\nInspectors\nResponsible for finding defects in work product from \na general point of view, as well as defects that\naffect their area of expertise.\n\nAuthor\nProvides information about work product during all \nstages of process. Responsible for correcting all \nmajor defects and any minor and trivial defects that \ncost and schedule permit. Performs duties of an\ninspector.\n\nReader\nGuides team through work product during inspection \nmeeting. Reads or paraphrases work product in \ndetail. Should be an inspector from same (or next) \nlife-cycle phase as author. Performs duties of an \ninspector in addition to reader\u2019s role. \n\nRecorder\nAccurately records each defect found during \ninspection meeting on the Inspection Defect List.  \nPerforms duties of an inspector in addition to \nrecorder\u2019s role. \n\nGuidelines for Successful Inspections\n\n\u2022 Train moderators, inspectors, and managers\n\u2022 No more than 25% of developers\u2019 time should be\n\ndevoted to inspections\n\u2022 Inspect 100% of  work product\n\u2022 Be prepared\n\u2022 Share responsibility for work product quality\n\u2022 Be willing to associate and communicate\n\u2022 Avoid judgmental language\n\u2022 Do not evaluate author\n\u2022 Have at least one positive and negative input\n\u2022 Raise issues; don\u2019t resolve them\n\u2022 Avoid discussions of style\n\u2022 Stick to standard or change it\n\u2022 Be technically competent\n\u2022 Record all issues in public\n\u2022 Stick to technical issues\n\u2022 Distribute inspection documents as soon as possible\n\u2022 Let author determine when work product is ready\n\nfor inspection\n\u2022 Keep accurate statistics\n\nBased on JCK/LLW/SSP/HS: 10/92\n\n10 Basic Rules of Inspections\n\u2022 Inspections are carried out at a number of points \n\ninside phases of the life cycle. Inspections are not \nsubstitutes for milestone reviews. \n\n\u2022 Inspections are carried out by peers representing \nareas of life cycle affected by material being \ninspected (usually limited to 6 or fewer people). \nAll inspectors should have a vested interest in the\nwork product.\n\n\u2022 Management is not present during inspections. \nInspections are not to be used as a tool to evaluate \nworkers.\n\n\u2022 Inspections are led by a trained moderator.\n\u2022 Trained inspectors are assigned roles. \n\u2022 Inspections are carried out in a prescribed series of \n\nsteps. \n\u2022 Inspection meeting is limited to 2 hours. \n\u2022 Checklists of questions are used to define task and \n\nto stimulate defect finding.\n\u2022 Material is covered during inspection meeting within \n\nan optional page rate, which has been found to give \nmaximum error-finding ability. \n\n\u2022 Statistics on number of defects, types of defects, and \ntime expended by engineers on inspections are kept.\n\nPeer Reviews/Inspections\nQUICK REFERENCE\n\nGUIDE\n\n1. 1-day minimum\n2. 5-day minimum, when included\n3. 3- to 5-day minimum for inspectors to fit preparation time into normal \n\nwork schedule\n4. 3- to 5-day minimum for inspectors to fit preparation time into normal\n\nwork schedule\n5. 4 hour minimum prior to inspection meeting \n6. Immediate: Rework can begin as soon as inspection meeting ends \n7. 1 day recommended \n8. Minimum possible time \n9. 1-week maximum from end of Inspection meeting \n\n10. 2-week maximum\n\n1\n\nPLANNING PREPARATION REWORK FOLLOW-UP\n\nPlanning Inspection Schedule* \nand Estimating Staff Hours\n\nTHIRD HOUR\n\n(Use on approx. \n17% of total \ninspections.)\n\n2 5\n\n8\n\n9\n\n10\n\n TRANSITION TIMES** \n\nstaff hours  \n= 1 to 3 \nhours \n\nOVERVIEW\nMEETING\n\nINSPECTION \nMEETING\n\nstaff hours \n= 5 to 20\n   hours\n\n3\n\n6\n\n ** Entire inspection process should be completed from start to finish within a 3-week period.\n\nstaff hours\n= 0.5 hour \nx # inspectors\n\n(Author)\n\n * Historically, complete inspections have averaged 30.5 total staff hours for 5-person teams.\n\n(Use on approx. 6% \nof total inspections.) (All inspectors)(All inspectors)\n\nSYMBOLS\n\n=  TIME REFERENCE\n\n=  STAGE TRANSITION\n\n =  PROCESS STAGE\n\n= OPTIONAL STAGE\n\n(Author & \nModerator)\n\nstaff hours \n= 1 hour x \n# inspectors \n\nstaff hours  \n= 2 hours x \n\n# inspectors\n\n(Author & \nModerator)\n\nstaff hours  \n= 2 hours \nx # inspectors\n\n staff hours  \n= 1 to 3\n\nhours  \n\nMeeting Length\n\u2022 Overview*   0.5 to 1hrs \n\n\u2022 Inspection  2 hrs Max.\n\n\u2022 Third Hour  1 to 2 hrs\n\n*Author Preparation for Overview: \n3 to 4 hrs over 3 to 5 working days\n\n+ author\n prep time\n\n7\n\n4\nTypes of Inspections\n\nSY1 System Requirements\n\nSY2 System Design\n\nSU1 Subsystem Requirements\n\nSU2 Subsystem Design\n\nR1 Software Requirements\n\nI0 Architectural Design\n\nI1 Detailed Design\n\nI2 Source Code\n\nIT1 Test Plan\n\nIT2 Test Procedures & Functions \n\nMeeting* Rate Guidelines\nfor Various Inspection Types\n\nInspection Meeting\nTarget per 2 Hrs Range per 2 Hrs\n\n20 pages 10 to 30 pages\n\n20 pages 10 to 30 pages\n30 pages 20 to 40 pages\n35 pages 25 to 45 Pages\n\n500 lines of\nsource code**\n\n400 to 600 lines\nof source code**\n\n30 pages 20 to 40 pages\n\n35 pages 25 to 45 pages\n\n* Assumes a 2-hour meeting. Scale down planned\nmeeting duration for  shorter work products.\n\n** Flight software and other highly complex code \nsegments should proceed at about half this rate.\n\nType\n\nR 0\nR1\nI0\nI1\n\nI2\n\nIT1\nIT2\n\nFigure N?2 Peer reviews/inspections quick reference guide\n\n\n\n316 ? NASA Systems Engineering Handbook\n\nAppendix O: Tradeoff Examples\n\nTable O?1 Typical Tradeoffs for Space Systems\n\nDevelopment Related Operations and Support Related\n\nCustom versus commercial-off-the-shelf ?\n\nLight parts (expensive) versus heavy parts (less expensive) ?\n\nOn-board versus remote processing ?\n\nRadio frequency versus optical links ?\n\nLevels of margin versus cost/risk ?\n\nClass S versus non-class S parts ?\n\nRadiation-hardened versus standard components ?\n\nLevels of redundancy ?\n\nDegrees of quality assurance ?\n\nBuilt-in test versus remote diagnostics ?\n\nTypes of environmental exposure prior to operation ?\n\nLevel of test (system versus subsystem) ?\n\nVarious life-cycle approaches (e.g., waterfall versus spiral  ?\nversus incremental)\n\nUpgrade versus new start ?\n\nManned versus unmanned ?\n\nAutonomous versus remotely controlled ?\n\nSystem of systems versus stand-alone system ?\n\nOne long-life unit versus many short-life units ?\n\nLow Earth orbit versus medium Earth orbit versus geosta- ?\ntionary orbit versus high Earth orbit\n\nSingle satellite versus constellation ?\n\nLaunch vehicle type (e.g., Atlas versus Titan) ?\n\nSingle stage versus multistage launch ?\n\nRepair in-situ versus bring down to ground ?\n\nCommercial versus Government assets ?\n\nLimited versus public access ?\n\nControlled versus uncontrolled reentry ?\n\nTable O?2 Typical Tradeoffs in the Acquisition Process\n\nAcquisition Phase Trade Study Purpose\n\nMission needs analysis Prioritize identified user needs\n\nConcept exploration (concept and technol-\nogy development)\n\nCompare new technology with proven concepts1. \n\nSelect concepts best meeting mission needs2. \n\nSelect alternative system configurations3. \n\nFocus on feasibility and affordability4. \n\nDemonstration/validation Select technology1. \n\nReduce alternative configurations to a testable number2. \n\nFull-scale development (system develop-\nment and demonstration\n\nSelect component/part designs1. \n\nSelect test methods2. \n\nSelect operational test and evaluation quantities3. \n\nProduction Examine effectiveness of all proposed design changes1. \n\nPerform make/buy, process, rate, and location decisions2. \n\nTable O?3 Typical Tradeoffs Throughout the Project Life Cycle\n\nPre?Phase A Phase A Phase B Phases C&D Phases D&E Phases E&F\n\nProblem selec- ?\ntion\n\nUpgrade versus  ?\nnew start\n\nOn-board  ?\nversus ground \nprocessing\n\nLow Earth orbit  ?\nversus geo-\nstationary orbit\n\nLevels of  ?\nredundancy\n\nRadio frequency  ?\nlinks versus \noptical links\n\nSingle source  ?\nversus multiple \nsuppliers\n\nLevel of testing ?\n\nPlatform STS-28  ?\nversus STS-3a\n\nLaunch go- ?\nahead (Go or \nNo-Go)\n\nAdjust orbit  ?\ndaily versus \nweekly\n\nDeorbit now  ?\nversus later\n\n\n\nNASA Systems Engineering Handbook ? 317\n\nAppendix P: SOW Review Checklist\n\nEditorial Checklist\nIs the SOW requirement in the form \u201cwho\u201d shall \u201cdo what\u201d? An example is, \u201cThe Contractor shall (perform, pro-1. \nvide, develop, test, analyze, or other verb followed by a description of what).\u201d\n\n  Example SOW requirements:\nThe Contractor shall design the XYZ flight software\u2026 ?\nThe Contractor shall operate the ABC ground system\u2026 ?\nThe Contractor shall provide maintenance on the following\u2026 ?\nThe Contractor shall report software metrics monthly \u2026 ?\nThe Contractor shall integrate the PQR instrument with the spacecraft\u2026 ?\n\nIs the SOW requirement a simple sentence that contains only one requirement? 2. \n Compound sentences that contain more than one SOW requirement need to be split into multiple simple sen-\n\ntences. (For example, \u201cThe Contractor shall do ABC and perform XYZ\u201d should be rewritten as \u201cThe Contractor \nshall do ABC\u201d and \u201cThe Contractor shall perform XYZ.\u201d)\nIs the SOW composed of simple, cohesive paragraphs, each covering a single topic? Paragraphs containing many 3. \nrequirements should be divided into subparagraphs for clarity.\nHas each paragraph and subparagraph been given a unique number or letter identifier? Is the numbering or let-4. \ntering correct? \nIs the SOW requirement in the active rather than the passive voice? Passive voice leads to vague statements. (For 5. \nexample, state, \u201cThe Contractor shall hold monthly management review meetings\u201d instead of \u201cManagement review \nmeeting shall be held monthly.\u201d)\nIs the SOW requirement stated positively as opposed to negatively? (Replace statements such as, \u201cThe Contractor 6. \nshall not exceed the budgetary limits specified\u201d with \u201cThe contractor shall comply with the budgetary limits speci-\nfied.\u201d)\nIs the SOW requirement grammatically correct?7. \nIs the SOW requirement free of typos, misspellings, and punctuation errors?8. \nHave all acronyms been defined in an acronym list or spelled out in the first occurrence?9. \nHave the quantities, delivery schedules, and delivery method been identified for each deliverable within the SOW 10. \nor in a separate attachment/section?\nHas the content of documents to be delivered been defined in a separate attachment/section and submitted with 11. \nthe SOW?\nHas the file format of each electronic deliverable been defined (e.g., Microsoft\u2014Project, Adobe\u2014Acrobat PDF, Na-12. \ntional Instruments\u2014Labview VIs)?\n\nContent Checklist\nAre correct terms used to define the requirements?1. \n\nShall  ? = requirement (binds the contractor)\nShould  ? = goal (leaves decision to contractor; avoid using this word)\nMay  ? = allowable action (leaves decision to contractor; avoid using this word)\n\n\n\n318 ? NASA Systems Engineering Handbook\n\nAppendix P: SOW Review Checklist\n\nWill ?  = facts or declaration of intent by the Government (use only in referring to the Government)\nPresent tense ?  (e.g., \u201cis\u201d) = descriptive text only (avoid using in requirements statements; use \u201cshall\u201d instead)\nNEVER ?  use \u201dmust\u201d\n\nIs the scope of the SOW clearly defined? Is it clear what you are buying?2. \nIs the flow and organizational structure of the document logical and understandable? (See LPR 5000.2 \u201cProcure-3. \nment Initiator\u2019s Guide,\u201d Section 12, for helpful hints.) Is the text compatible with the title of the section it\u2019s under? \nAre subheadings compatible with the subject matter of headings? \nIs the SOW requirement clear and understandable?4. \n\nCan the sentence be understood only one way?  ?\nWill all terminology used have the same meaning to different readers without definition? Has any terminology  ?\nfor which this is not the case been defined in the SOW? (E.g., in a definitions section or glossary.)\nIs it free from indefinite pronouns (\u201cthis,\u201d \u201cthat,\u201d \u201cthese,\u201d \u201cthose\u201d) without clear antecedents (e.g., replace state- ?\nments such as, \u201cThese shall be inspected on an annual basis\u201d with \u201cThe fan blades shall be inspected on an an-\nnual basis\u201d)?\nIs it stated concisely? ?\n\nHave all redundant requirements been removed? Redundant requirements can reduce clarity, increase ambiguity, 5. \nand lead to contradictions.\nIs the requirement consistent with other requirements in the SOW, without contradicting itself, without using the 6. \nsame terminology with different meanings, without using different terminology for the same thing?\nIf the SOW includes the delivery of a product (as opposed to just a services SOW): 7. \n\nAre the technical product requirements in a separate section or attachment, apart from the activities that the  ?\ncontractor is required to perform? The intent is to clearly delineate between the technical product requirements \nand requirements for activities the contractor is to perform (e.g., separate SOW statements \u201cThe contractor \nshall\u201d from technical product requirement statements such as \u201cThe system shall\u201d and \u201cThe software shall\u201d).\nAre references to the product and its subelements in the SOW at the level described in the technical product re- ?\nquirements? \nIs the SOW consistent with and does it use the same terminology as the technical product requirements?  ?\n\nIs the SOW requirement free of ambiguities? Make sure the SOW requirement is free of vague terms (for example, 8. \n\u201cas appropriate,\u201d \u201cany,\u201d \u201ceither,\u201d \u201cetc.,\u201d \u201cand/or,\u201d \u201csupport,\u201d \u201cnecessary,\u201d \u201cbut not limited to,\u201d \u201cbe capable of,\u201d \u201cbe able \nto\u201d).\nIs the SOW requirement verifiable? Make sure the SOW requirement is free of unverifiable terms (for example, 9. \n\u201cflexible,\u201d \u201ceasy,\u201d \u201csufficient,\u201d \u201csafe,\u201d \u201cad hoc,\u201d \u201cadequate,\u201d \u201caccommodate,\u201d \u201cuser-friendly,\u201d \u201cusable,\u201d \u201cwhen required,\u201d \n\u201cif required,\u201d \u201cappropriate,\u201d \u201cfast,\u201d \u201cportable,\u201d \u201clightweight,\u201d \u201csmall,\u201d \u201clarge,\u201d \u201cmaximize,\u201d \u201cminimize,\u201d \u201coptimize,\u201d \n\u201csufficient,\u201d \u201crobust,\u201d \u201cquickly,\u201d \u201ceasily,\u201d \u201cclearly,\u201d other \u201cly\u201d words, other \u201cize\u201d words).\nIs the SOW requirement free of implementation constraints? SOW requirements should state WHAT the con-10. \ntractor is to do, NOT HOW they are to do it (for example, \u201cThe Contractor shall design the XYZ flight software\u201d \nstates WHAT the contractor is to do, while \u201cThe Contractor shall design the XYZ software using object-oriented \ndesign\u201d states HOW the contractor is to implement the activity of designing the software. In addition, too low a \nlevel of decomposition of activities can result in specifying how the activities are to be done, rather than what ac-\ntivities are to be done).\nIs the SOW requirement stated in such a way that compliance with the requirement is verifiable? Do the means 11. \nexist to measure or otherwise assess its accomplishment? Can a method for verifying compliance with the require-\nment be defined (e.g., described in a quality assurance surveillance plan)?\nIs the background material clearly labeled as such (i.e., included in the background section of the SOW if one is 12. \nused)?\n\n\n\nAppendix P: SOW Review Checklist\n\nNASA Systems Engineering Handbook ? 319\n\nAre any assumptions able to be validated and restated as requirements? If not, the assumptions should be deleted 13. \nfrom the SOW. Assumptions should be recorded in a document separate from the SOW.\nIs the SOW complete, covering all of the work the contractor is to do?14. \n\nAre all of the activities necessary to develop the product included (e.g., system, software, and hardware activi- ?\nties for the following: requirements, architecture, and design development; implementation and manufacturing; \nverification and validation; integration testing and qualification testing.)?\nAre all safety, reliability, maintainability (e.g., mean time to restore), availability, quality assurance, and security  ?\nrequirements defined for the total life of the contract? \nDoes the SOW include a requirement for the contractor to have a quality system (e.g., ISO certified), if one is  ?\nneeded?\nAre all of the necessary management and support requirements included in the SOW (for example, project  ?\nmanagement; configuration management; systems engineering; system integration and test; risk management; \ninterface definition and management; metrics collection, reporting, analysis, and use; acceptance testing; NASA \nIndependent Verification and Validation (IV&V) support tasks.)?\nAre clear performance standards included and sufficient to measure contractor performance (e.g., systems, soft- ?\nware, hardware, and service performance standards for schedule, progress, size, stability, cost, resources, and \ndefects)? See Langley\u2019s Guidance on System and Software Metrics for Performance-Based Contracting for more \ninformation and examples on performance standards.\nAre all of the necessary service activities included (for example, transition to operations, operations, mainte- ?\nnance, database administration, system administration, and data management)?\nAre all of the Government surveillance activities included (for example, project management meetings; decision  ?\npoints; requirements and design peer reviews for systems, software, and hardware; demonstrations; test readi-\nness reviews; other desired meetings (e.g., technical interchange meetings); collection and delivery of metrics for \nsystems, software, hardware, and services (to provide visibility into development progress and cost); electronic \naccess to technical and management data; and access to subcontractors and other team members for the pur-\nposes of communication)?\nAre the Government requirements for contractor inspection and testing addressed, if necessary?  ?\nAre the requirements for contractor support of Government acceptance activities addressed, if necessary? ?\n\nDoes the SOW only include contractor requirements? It should not include Government requirements.15. \nDoes the SOW give contractors full management responsibility and hold them accountable for the end result?16. \nIs the SOW sufficiently detailed to permit a realistic estimate of cost, labor, and other resources required to accom-17. \nplish each activity?\nAre all deliverables identified (e.g., status, financial, product deliverables)? The following are examples of deliver-18. \nables that are sometimes overlooked: management and development plans; technical progress reports that identify \ncurrent work status, problems and proposed corrective actions, and planned work; financial reports that identify \ncosts (planned, actual, projected) by category (e.g., software, hardware, quality assurance); products (e.g., source \ncode, maintenance/user manual, test equipment); and discrepancy data (e.g., defect reports, anomalies). All deliv-\nerables should be specified in a separate document except for technical deliverables (e.g., hardware, software, pro-\ntotypes), which should be included in the SOW.\nDoes each technical and management deliverable track to a paragraph in the SOW? Each deliverable should have 19. \na corresponding SOW requirement for its preparation (i.e., the SOW identifies the title of the deliverable in paren-\ntheses after the task requiring the generation of the deliverable).\nAre all reference citations complete?20. \n\nAre the complete number, title, and date or version of each reference specified? ?\nDoes the SOW reference the standards and other compliance documents in the proper SOW paragraphs?  ?\n\n\n\n320 ? NASA Systems Engineering Handbook\n\nAppendix P: SOW Review Checklist\n\nIs the correct reference document cited and is it referenced at least once? ?\nIs the reference document either furnished with the SOW or available at a location identified in the SOW? ?\nIf the referenced standard or compliance document is only partially applicable, does the SOW explicitly and un- ?\nambiguously reference the portion that is required of the contractor?\n\n\n\nNASA Systems Engineering Handbook ? 321\n\nAppendix Q: Project Protection Plan Outline\n\nThe following outline will assist systems engineers in preparing a project protection \nplan. The plan is a living document that will be written and updated as the project pro-\ngresses through major milestones and ultimately through end of life.\n\n1. Introduction\n 1.1 Protection Plan Overview\n 1.2 Project Overview\n 1.3 Acquisition Status\n\n2. References\n 2.1 Directives and Instructions\n 2.2 Requirements\n 2.3 Studies and Analyses\n\n3. References\n 3.1 Threats: Hostile Action\n  3.1.1 Overview\n  3.1.2 Threat Characterization\n   3.1.2.1 Cyber Attack\n   3.1.2.2 Electronic Attack\n   3.1.2.3 Lasers\n   3.1.2.4 Ground Attack\n   3.1.2.5 Asymmetric Attack on Critical Commercial Infrastructure\n   3.1.2.6 Anti-Satellite Weapons\n   3.1.2.7 High-Energy Radio Frequency Weapons\n   3.1.2.8 Artificially Enhanced Radiation Environment\n 3.2 Threats: Environmental\n  3.2.1 Overview\n  3.2.2 Threat Characterization\n   3.2.2.1 Natural Environment Storms\n   3.2.2.2 Earthquakes\n   3.2.2.3 Floods\n   3.2.2.4 Fires\n   3.2.2.5 Radiation Effects in the Natural Environment\n   3.2.2.6 Radiation Effects to Spacecraft Electronics\n\n4. Protection Vulnerabilities\n 4.1 Ground Segment Vulnerabilities\n  4.1.1 Command and Control Facilities\n  4.1.2 Remote Tracking Stations\n  4.1.3 Spacecraft Simulator(s)\n  4.1.4 Mission Data Processing Facilities\n  4.1.5 Flight Dynamic Facilities\n  4.1.6 Flight Software Production/Verification/Validation Facilities\n\n\n\n322 ? NASA Systems Engineering Handbook\n\nAppendix Q: Project Protection Plan Outline\n\n 4.2 Communications/Information Segment Vulnerabilities\n  4.2.1 Command Link\n  4.2.2 Telemetry Link (Mission Data)\n  4.2.3 Telemetry Link (Engineering Data)\n  4.2.4 Ground Network\n 4.3 Space Segment Vulnerabilities\n  4.3.1 Spacecraft Physical Characteristics\n  4.3.2 Spacecraft Operational Characteristics \n  4.3.3 Orbital Parameters\n  4.3.4 Optical Devices (Sensors/Transmitters/Receivers)\n  4.3.5 Communications Subsystem\n  4.3.6 Command and Data Handling Subsystem\n  4.3.7 Instruments\n 4.4 Launch Segment Vulnerabilities\n  4.4.1 Launch Parameters\n  4.4.2 Launch Site Integration and Test Activities\n 4.5 Commercial Infrastructure Vulnerabilities\n  4.5.1 Electrical Power\n  4.5.2 Natural Gas\n  4.5.3 Telecommunications\n  4.5.4 Transportation\n\n5. Protection Countermeasures\n 5.1 Protection Strategy\n 5.2 Mission Threat Mitigation\n 5.3 Mission Restoration Options\n 5.4 Mission Survivability Characteristics\n\n6. Debris Mitigation\n 6.1 Design Guidelines\n 6.2 End-Of-Life Mitigation Procedures\n 6.3 Collision Avoidance\n\n7. Critical Program Information and Technologies\n 7.1 Critical Program Information Elements\n 7.2 Critical Information Program\n\n8. Program Protection Costs\n 8.1 System Trade Analyses\n 8.2 Cost/Benefit Analyses\n\n\n\nNASA Systems Engineering Handbook ? 323\n\nReferences\n\nThis appendix contains references that were cited in the \nsections of the handbook and sources for developing the \nmaterial in the indicated sections. See the Bibliography \nfor complete citations.\n\nSection 2.0 Fundamentals of Systems \nEngineering\nGriffin, Michael D., \u201cSystem Engineering and the Two \nCultures of Engineering.\u201d 2007.\n\nRechtin, Eberhardt. Systems Architecting of Organiza-\ntions: Why Eagles Can\u2019t Swim. 2000. \n\nSection 3.4 Project Phase A: Concept and \nTechnology Development\nNASA. NASA Safety Standard 1740.14, Guidelines and \nAssessment Procedures for Limiting Orbital Debris. 1995. \n\nSection 4.1 Stakeholder Expectations \nDefinition\nANSI. Guide for the Preparation of Operational Concept \nDocuments. 1992.\n\nSection 4.2 Technical Requirements Definition\nNASA. NASA Space Flight Human System Standard. 2007.\n\nSection 4.3 Logical Decomposition\nInstitute of Electrical and Electronics Engineers. Stan-\ndard Glossary of Software Engineering Terminology. \n1999.\n\nSection 4.4 Design Solution\nBlanchard, Benjamin S. System Engineering Manage-\nment. 2006.\n\nDOD. MIL-STD-1472, Human Engineering. 2003.\n\nFederal Aviation Administration. Human Factors Design \nStandard. 2003.\n\nInternational Organization for Standardization. Quality \nSystems Aerospace\u2014Model for Quality Assurance in De-\nsign, Development, Production, Installation, and Ser-\nvicing. 1999.\n\nNASA. NASA Space Flight Human System Standard. 2007.\n\nNASA. Planning, Developing, and Maintaining and Ef-\nfective Reliability and Maintainability (R&M) Program. \n1998. \n\nU. S. Army Research Laboratory. MIL HDBK 727, Design \nGuidance for Producibility. 1990.\n\nU.S. Nuclear Regulatory Commission. Human-System \nInterface Design Review Guidelines. 2002.\n\nSection 5.1 Product Implementation\nAmerican Institute of Aeronautics and Astronautics. \nAIAA Guide for Managing the Use of Commercial Off the \nShelf (COTS) Software Components for Mission-Critical \nSystems. 2006.\n\nInternational Council on Systems Engineering. Systems \nEngineering Handbook. 2006.\n\nNASA. Off-the-Shelf Hardware Utilization in Flight Hard-\nware Development. 2004.\n\nSection 5.3 Verification\n\nElectronic Industries Alliance. Processes for Engineering \na System. 1999.\n\nInstitute of Electrical and Electronics Engineers. Stan-\ndard for Application and Management of the Systems En-\ngineering Process. 1998.\n\nInternational Organization for Standardization. Systems \nEngineering\u2014System Life Cycle Processes. 2002. \n\nNASA. Project Management: Systems Engineering & \nProject Control Processes and Requirements. 2004.\n\nU.S. Air Force. SMC Systems Engineering Primer and \nHandbook. 2005. \n\nSection 5.4 Validation\nElectronic Industries Alliance. Processes for Engineering \na System. 1999.\n\nInstitute of Electrical and Electronics Engineers. Stan-\ndard for Application and Management of the Systems En-\ngineering Process. 1998.\n\n\n\n324 ? NASA Systems Engineering Handbook\n\nReferences\n\nInternational Organization for Standardization. Systems \nEngineering\u2014System Life Cycle Processes. 2002.\n\nNASA. Project Management: Systems Engineering & \nProject Control Processes and Requirements. 2004.\n\nU.S. Air Force. SMC Systems Engineering Primer and \nHandbook. 2005.\n\nSection 5.5 Product Transition\nDOD. Defense Acquisition Guidebook. 2004.\n\nElectronic Industries Alliance. Processes for Engineering \na System. 1999. \n\nInternational Council on Systems Engineering. Systems \nEngineering Handbook. 2006.\n\nInternational Organization for Standardization. Systems \nEngineering\u2014A Guide for the Application of ISO/IEC \n15288. 2003.\n\n\u2014. Systems Engineering\u2014System Life Cycle Pro-\ncesses. 2002.\n\nNaval Air Systems Command. Systems Command SE \nGuide: 2003. 2003. \n\nSection 6.1 Technical Planning\nAmerican Institute of Aeronautics and Astronautics. \nAIAA Guide for Managing the Use of Commercial \nOff the Shelf  (COTS) Software Components for Mis-\nsion-Critical Systems. 2006.\nInstitute of Electrical and Electronics Engineers. Stan-\ndard for Application and Management of the Systems En-\ngineering Process. 1998.\n\nMartin, James N. Systems Engineering Guidebook: A Pro-\ncess for Developing Systems and Products. 1996.\n\nNASA. NASA Cost Estimating Handbook. 2004.\n\n\u2014. Standard for Models and Simulations. 2006.\n\nSection 6.4 Technical Risk Management\nClemen, R., and T. Reilly. Making Hard Decisions with \nDecisionTools Suite. 2002.\n\nDezfuli, H. \u201cRole of System Safety in Risk-Informed De-\ncisionmaking.\u201d 2005.\n\nKaplan, S., and B. John Garrick. \u201cOn the Quantitative \nDefinition of Risk.\u201d 1981.\n\nMorgan, M. Granger, and M. Henrion. Uncertainty: A \nGuide to Dealing with Uncertainty in Quantitative Risk \nand Policy Analysis. 1990.\n\nStamelatos, M., H. Dezfuli, and G. Apostolakis. \u201cA Pro-\nposed Risk-Informed Decisionmaking Framework for \nNASA.\u201d 2006.\n\nStern, Paul C., and Harvey V. Fineberg, eds. Under-\nstanding Risk: Informing Decisions in a Democratic So-\nciety. 1996. \n\nU.S. Nuclear Regulatory Commission. White Paper on \nRisk-Informed and Performance-Based Regulation. 1998.\n\nSection 6.5 Configuration Management\nAmerican Society of Mechanical Engineers. Engineering \nDrawing Practices. 2004.\n\n\u2014. Types and Applications of Engineering Drawings. \n1999.\n\nDOD. Defense Logistics Agency (DLA) Cataloging Hand-\nbook.\n\n\u2014. MIL-HDBK-965, Parts Control Program. 1996.\n\n\u2014. MIL-STD-881B, Work Breakdown Structure \n(WBS) for Defense Materiel Items. 1993.\n\nDOD, U.S. General Services Administration, and NASA. \nAcquisition of Commercial Items. 2007.\n\n\u2014. Quality Assurance, Nonconforming Supplies or \nServices. 2007.\n\nInstitute of Electrical and Electronics Engineers. EIA \nGuide for Information Technology Software Life Cycle \nProcesses\u2014Life Cycle Data. 1997.\n\n\u2014. IEEE Guide to Software Configuration Manage-\nment. 1987.\n\n\u2014. Standard for Software Configuration Manage-\nment Plans. 1998.\n\nInternational Organization for Standardization. Infor-\nmation Technology\u2014Software Life Cycle Processes Con-\nfiguration Management. 1998.\n\n\u2014. Quality Management\u2014Guidelines for Configuration \nManagement. 1995.\n\nNASA. NOAA-N Prime Mishap Investigation Final Report.  \n2004.\n\nNational Defense Industrial Association. Data Manage-\nment. 2004.\n\n\u2014. National Consensus Standard for Configuration \nManagement. 1998.\n\n\n\nReferences\n\nNASA Systems Engineering Handbook ? 325\n\nSection 6.6 Technical Data Management\nNational Defense Industrial Association. Data Manage-\nment. 2004.\n\n\u2014. National Consensus Standard for Configuration \nManagement. 1998.\n\nSection 6.8 Decision Analysis\nBlanchard, Benjamin S. System Engineering Manage-\nment. 2006.\n\nBlanchard, Benjamin S., and Wolter Fabrycky. Systems \nEngineering and Analysis. 2006.\n\nClemen, R., and T. Reilly. Making Hard Decisions with \nDecisionTools Suite. 2002.\n\nKeeney, Ralph L. Value-Focused Thinking: A Path to Cre-\native Decisionmaking. 1992.\n\nKeeney, Ralph L., and Timothy L. McDaniels. \u201cA Frame-\nwork to Guide Thinking and Analysis Regarding Cli-\nmate Change Policies.\u201d 2001.\n\nKeeney, Ralph L., and Howard Raiffa. Decisions with Mul-\ntiple Objectives: Preferences and Value Tradeoffs. 1993.\n\nMorgan, M. Granger, and M. Henrion. Uncertainty: A \nGuide to Dealing with Uncertainty in Quantitative Risk \nand Policy Analysis. 1990.\n\nSaaty,Thomas L. The Analytic Hierarchy Process. 1980.\n\nSection 7.1 Engineering with Contracts\nAdams, R. J., S. Eslinger, P. Hantos, K. L. Owens, et al. \nSoftware Development Standard for Space Systems. 2005.\n\nDOD, U.S. General Services Administration, and NASA. \nContracting Office Responsibilities. 2007.\n\nEslinger, Suellen. Software Acquisition Best Practices for \nthe Early Acquisition Phases. 2004.\n\nHofmann, Hubert F., Kathryn M. Dodson, Gowri S. Ra-\nmani, and Deborah K. Yedlin. Adapting CMMI\u00ae for Ac-\nquisition Organizations: A Preliminary Report. 2006.\n\nInternational Council on Systems Engineering. Systems \nEngineering Handbook: A \u201cWhat To\u201d Guide for all SE \nPractitioners. 2004.\n\nThe Mitre Corporation. Common Risks and Risk Mitiga-\ntion Actions for a COTS-Based System. \n\nNASA. Final Memorandum on NASA\u2019s Acquisition Ap-\nproach Regarding Requirements for Certain Software En-\ngineering Tools to Support NASA Programs. 2006.\n\n\u2014. The SEB Source Evaluation Process. 2001.\n\n\u2014. Solicitation to Contract Award. 2007.\n\n\u2014. Standard for Models and Simulations. 2006.\n\n\u2014. Statement of Work Checklist. \n\n\u2014. System and Software Metrics for Performance-\nBased Contracting.\n\nNaval Air Systems Command. Systems Engineering \nGuide. 2003.\n\nSection 7.2 Integrated Design Facilities\nMiao, Y., and J. M. Haake. \u201cSupporting Concurrent De-\nsign by Integrating Information Sharing and Activity \nSynchronization.\u201d 1998.\n\nSection 7.4 Human Factors Engineering\nBlanchard, Benjamin S., and Wolter Fabrycky. Systems \nEngineering and Analysis. 2006.\n\nChapanis, A. \u201cThe Error-Provocative Situation: A Cen-\ntral Measurement Problem in Human Factors Engi-\nneering.\u201d 1980.\n\nDOD. Human Engineering Procedures Guide. 1987.\n\n\u2014. MIL-HDBK-46855A, Human Engineering Pro-\ngram Process and Procedures. 1996.\n\nEggemeier, F. T., and G. F. Wilson. \u201cPerformance and \nSubjective Measures of Workload in Multitask Environ-\nments.\u201d 1991.\n\nEndsley, M. R., and M. D. Rogers. \u201cSituation Awareness \nInformation Requirements Analysis for En Route Air \nTraffic Control.\u201d 1994.\n\nFuld, R. B. \u201cThe Fiction of Function Allocation.\u201d 1993. \n\nGlass, J. T., V. Zaloom, and D. Gates. \u201cA Micro-Com-\nputer-Aided Link Analysis Tool.\u201d 1991.\n\nGopher, D., and E. Donchin. \u201cWorkload: An Examina-\ntion of the Concept.\u201d 1986.\n\nHart, S. G., and C. D. Wickens. \u201cWorkload Assessment \nand Prediction.\u201d 1990.\n\nHuey, B. M., and C. D. Wickens, eds. Workload Transi-\ntion. 1993.\n\nJones, E. R., R. T. Hennessy, and S. Deutsch, eds. Human \nFactors Aspects of Simulation. 1985.\n\nKirwin, B., and L. K. Ainsworth. A Guide to Task Anal-\nysis. 1992.\n\n\n\n326 ? NASA Systems Engineering Handbook\n\nReferences\n\nKurke, M. I. \u201cOperational Sequence Diagrams in System \nDesign.\u201d 1961. \n\nMeister, David. Behavioral Analysis and Measurement \nMethods. 1985.\n\n\u2014. Human Factors: Theory and Practice. 1971.\n\nPrice, H. E. \u201cThe Allocation of Functions in Systems.\u201d \n1985.\n\nShafer, J. B. \u201cPractical Workload Assessment in the De-\nvelopment Process.\u201d 1987.\n\nSection 7.6 Use of Metric System\nDOD. DoD Guide for Identification and Development of \nMetric Standards. 2003.\n\nTaylor, Barry. Guide for the Use of the International System \nof Units (SI). 2007. \n\nAppendix F: Functional, Timing, and State \nAnalysis\nBuede, Dennis. The Engineering Design of Systems: \nModels and Methods. 2000.\n\nDefense Acquisition University. Systems Engineering \nFundamentals Guide. 2001.\n\nLong, Jim. Relationships Between Common Graphical \nRepresentations in Systems Engineering. 2002.\n\nNASA. Training Manual for Elements of Interface Defini-\ntion and Control. 1997.\n\nSage, Andrew, and William Rouse. The Handbook of Sys-\ntems Engineering and Management. 1999.\n\nAppendix H: Integration Plan Outline\nFederal Highway Administration and CalTrans. Systems \nEngineering Guidebook for ITS. 2007.\n\nAppendix J: SEMP Content Outline\nDOD. MIL-HDBK-881, Work Breakdown Structures for \nDefense Materiel Systems. 2005.\n\nDOD Systems Management College. Systems Engineering \nFundamentals. 2001.\n\nMartin, James N. Systems Engineering Guidebook: A Pro-\ncess for Developing Systems and Products. 1996.\n\nNASA. NASA Cost Estimating Handbook. 2004.\n\nThe Project Management Institute\u00ae. Practice Standards \nfor Work Breakdown Structures. 2001.\n\n\n\nNASA Systems Engineering Handbook ? 327\n\nBibliography\n\nAdams, R. J., et al. Software Development Standard for \nSpace Systems, Aerospace Report No. TOR\u20142004(3909)-\n3537, Revision B. March 11, 2005.\n\nAmerican Institute of Aeronautics and Astronautics. \nAIAA Guide for Managing the Use of Commercial Off the \nShelf (COTS) Software Components for Mission-Critical \nSystems, AIAA G-118-2006e. Reston, VA, 2006.\n\nAmerican National Standards Institute. Guide for the \nPreparation of Operational Concept Documents, ANSI/\nAIAA G-043-1992. Washington, DC, 1992. \n\nAmerican Society of Mechanical Engineers. Engineering \nDrawing Practices, ASME Y14.100. New York, 2004.\n\n\u2014. Types and Applications of Engineering Drawings, \nASME Y14.24. New York, 1999.\n\nBlanchard, Benjamin S. System Engineering Manage-\nment, 6th ed. New Dehli: Prentice Hall of India Private \nLimited, 2006.\n\nBlanchard, Benjamin S., and Wolter Fabrycky. Systems \nEngineering and Analysis, 6th ed. New Dehli: Prentice \nHall of India Private Limited, 2006.\n\nBuede, Dennis. The Engineering Design of Systems: Models \nand Methods. New York: Wiley & Sons, 2000.\n\nChapanis, A. \u201cThe Error-Provocative Situation: A Central \nMeasurement Problem in Human Factors Engineering.\u201d \nIn The Measurement of Safety Performance. Edited by W. \nE. Tarrants. New York: Garland STPM Press, 1980.\n\nClemen, R., and T. Reilly. Making Hard Decisions with \nDecisionTools Suite. Pacific Grove, CA: Duxbury Re-\nsource Center, 2002.\n\nDefense Acquisition University. Systems Engineering \nFundamentals Guide. Fort Belvoir, VA, 2001.\n\nDepartment of Defense. DOD Architecture Framework, \nVersion 1.5, Vol. 1. Washington, DC, 2007.\n\n\u2014. Defense Logistics Agency (DLA) Cataloging \nHandbook, H4/H8 Series. Washington, DC.\n\n\u2014. DoD Guide for Identification and Development of \nMetric Standards, SD-10. Washington, DC: DOD, Office \n\nof the Under Secretary of Defense, Acquisition, Tech-\nnology, & Logistics, 2003.\n\n\u2014. DOD-HDBK-763, Human Engineering Proce-\ndures Guide. Washington, DC, 1987.\n\n\u2014. MIL-HDBK-965, Parts Control Program. Wash-\nington, DC, 1996.\n\n\u2014. MIL-HDBK-46855A, Human Engineering Pro-\ngram Process and Procedures. Washington, DC, 1996.\n\n\u2014. MIL-STD-881B, Work Breakdown Structure \n(WBS) for Defense Materiel Items. Washington, DC, \n1993.\n\n\u2014. MIL-STD-1472, Human Engineering. Wash-\nington, DC, 2003.\n\nDOD, Systems Management College. Systems Engi-\nneering Fundamentals. Fort Belvoir, VA: Defense Acqui-\nsition Press, 2001.\n\nDOD, U.S. General Services Administration, and NASA. \nAcquisition of Commercial Items, 14CFR1214\u2013Part 1214\u2013\nSpace Flight 48CFR1814. Washington, DC, 2007.\n\n\u2014. Contracting Office Responsibilities, i 46.103(a). \nWashington, DC, 2007.\n\n\u2014. Quality Assurance, Nonconforming Supplies or \nServices, FAR Part 46.407. Washington, DC, 2007.\n\nDezfuli, H. \u201cRole of System Safety in Risk-informed De-\ncisionmaking.\u201d In Proceedings, the NASA Risk Manage-\nment Conference 2005. Orlando, December 7, 2005.\n\nEggemeier, F. T., and G. F. Wilson. \u201cPerformance and \nSubjective Measures of Workload in Multitask Envi-\nronments.\u201d In Multiple-Task Performance. Edited by D. \nDamos. London: Taylor and Francis, 1991.\n\nElectronic Industries Alliance. Processes for Engineering \na System, ANSI/EIA\u2013632. Arlington, VA, 1999.\n\nEndsley, M. R., and M. D. Rogers. \u201cSituation Awareness \nInformation Requirements Analysis for En Route Air \nTraffic Control.\u201d In Proceedings of the Human Factors and \nErgonomics Society 38th Annual Meeting. Santa Monica: \nHuman Factors and Ergonomics Society, 1994.\n\n\n\n328 ? NASA Systems Engineering Handbook\n\nBibliography\n\nEslinger, Suellen. Software Acquisition Best Practices for \nthe Early Acquisition Phases. El Segundo, CA: The Aero-\nspace Corporation, 2004.\n\nFederal Aviation Administration. HF-STD-001, Human \nFactors Design Standard. Washington, DC, 2003.\n\nFederal Highway Administration, and CalTrans. Systems \nEngineering Guidebook for ITS, Version 2.0. Washington, \nDC: U.S. Department of Transportation, 2007.\n\nFuld, R. B. \u201cThe Fiction of Function Allocation.\u201d Ergo-\nnomics in Design (January 1993): 20\u201324. \n\nGlass, J. T., V. Zaloom, and D. Gates. \u201cA Micro-Com-\nputer-Aided Link Analysis Tool.\u201d Computers in Industry \n16, (1991): 179\u201387.\n\nGopher, D., and E. Donchin. \u201cWorkload: An Exami-\nnation of the Concept.\u201d In Handbook of Perception and \nHuman Performance: Vol. II. Cognitive Processes and \nPerformance. Edited by K. R. Boff, L. Kaufman, and J. P. \nThomas. New York: John Wiley & Sons, 1986.\n\nGriffin, Michael D., NASA Administrator. \u201cSystem En-\ngineering and the Two Cultures of Engineering.\u201d Boeing \nLecture, Purdue University, March 28, 2007.\n\nHart, S. G., and C. D. Wickens. \u201cWorkload Assessment \nand Prediction.\u201d In MANPRINT: An Approach to Systems \nIntegration. Edited by H. R. Booher. New York: Van Nos-\ntrand Reinhold, 1990.\n\nHofmann, Hubert F., Kathryn M. Dodson, Gowri S. Ra-\nmani, and Deborah K. Yedlin. Adapting CMMI\u00ae for Ac-\nquisition Organizations: A Preliminary Report, CMU/\nSEI-2006-SR-005. Pittsburgh: Software Engineering In-\nstitute, Carnegie Mellon University, 2006, pp. 338\u201340.\n\nHuey, B. M., and C. D. Wickens, eds. Workload Transi-\ntion. Washington, DC: National Academy Press, 1993.\n\nInstitute of Electrical and Electronics Engineers. EIA \nGuide for Information Technology Software Life Cycle Pro-\ncesses\u2014Life Cycle Data, IEEE Std 12207.1. Washington, \nDC, 1997.\n\n\u2014. IEEE Guide to Software Configuration Manage-\nment, ANSI/IEEE 1042. Washington, DC, 1987.\n\n\u2014. Standard for Application and Management of the \nSystems Engineering Process, IEEE Std 1220. Washington, \nDC, 1998.\n\n\u2014. Standard Glossary of Software Engineering Termi-\nnology, IEEE Std 610.12-1990. Washington, DC, 1999.\n\n\u2014. Standard for Software Configuration Manage-\nment Plans, IEEE Std 828. Washington, DC, 1998.\n\nInternational Council on Systems Engineering. Systems \nEngineering Handbook, version 3. Seattle, 2006.\n\n\u2014. Systems Engineering Handbook: A \u201cWhat To\u201d \nGuide for All SE Practitioners, INCOSE-TP-2003-016-\n02, Version 2a. Seattle, 2004.\n\nInternational Organization for Standardization. Infor-\nmation Technology\u2014Software Life Cycle Processes Con-\nfiguration Management, ISO TR 15846. Geneva, 1998.\n\n\u2014. Quality Management\u2014Guidelines for Configura-\ntion Management, ISO 10007: 1995(E). Geneva, 1995.\n\n\u2014. Quality Systems Aerospace\u2014Model for Quality \nAssurance in Design, Development, Production, Installa-\ntion, and Servicing, ISO 9100/AS9100. Geneva: Interna-\ntional Organization for Standardization, 1999.\n\n\u2014. Systems Engineering\u2014A Guide for the Applica-\ntion of ISO/IEC 15288, ISO/IEC TR 19760: 2003. Ge-\nneva, 2003.\n\n\u2014. Systems Engineering\u2014System Life Cycle Processes, \nISO/IEC 15288: 2002. Geneva, 2002.\n\nJones, E. R., R. T. Hennessy, and S. Deutsch, eds. Human \nFactors Aspects of Simulation. Washington, DC: National \nAcademy Press, 1985.\n\nKaplan, S., and B. John Garrick. \u201cOn the Quantitative \nDefinition of Risk.\u201d Risk Analysis 1(1). 1981.\n\nKeeney, Ralph L. Value-Focused Thinking: A Path to Cre-\native Decisionmaking. Cambridge, MA: Harvard Univer-\nsity Press, 1992.\n\nKeeney, Ralph L., and Timothy L. McDaniels. \u201cA Frame-\nwork to Guide Thinking and Analysis Regarding Climate \nChange Policies.\u201d Risk Analysis 21(6): 989\u20131000. 2001.\n\nKeeney, Ralph L., and Howard Raiffa. Decisions with \nMultiple Objectives: Preferences and Value Tradeoffs. \nCambridge, UK: Cambridge University Press, 1993.\n\nKirwin, B., and L. K. Ainsworth. A Guide to Task Anal-\nysis. London: Taylor and Francis, 1992.\n\nKurke, M. I. \u201cOperational Sequence Diagrams in System \nDesign.\u201d Human Factors 3: 66\u201373. 1961.\n\nLong, Jim. Relationships Between Common Graphical \nRepresentations in Systems Engineering. Vienna, VA: Vi-\ntech Corporation, 2002.\n\n\n\nBibliography\n\nNASA Systems Engineering Handbook ? 329\n\nMartin, James N. Processes for Engineering a System: An \nOverview of the ANSI/GEIA EIA-632 Standard and Its \nHeritage. New York: Wiley & Sons, 2000.\n\n\u2014. Systems Engineering Guidebook: A Process for De-\nveloping Systems and Products. Boca Raton: CRC Press, \n1996.\n\nMeister, David. Behavioral Analysis and Measurement \nMethods. New York: John Wiley & Sons, 1985.\n\n\u2014. Human Factors: Theory and Practice. New York: \nJohn Wiley & Sons, 1971.\n\nMiao, Y., and J. M. Haake. \u201cSupporting Concurrent De-\nsign by Integrating Information Sharing and Activity \nSynchronization.\u201d In Proceedings of the 5th ISPE Inter-\nnational Conference on Concurrent Engineering Research \nand Applications (CE98). Tokyo, 1998, pp. 165\u201374.\n\nThe Mitre Corporation. Common Risks and Risk Mitiga-\ntion Actions for a COTS-based System. McLean, VA.\n\nMorgan, M. Granger, and M. Henrion. Uncertainty: A \nGuide to Dealing with Uncertainty in Quantitative Risk \nand Policy Analysis. Cambridge, UK: Cambridge Univer-\nsity Press, 1990.\n\nNASA. Final Memorandum on NASA\u2019s Acquisition Ap-\nproach Regarding Requirements for Certain Software En-\ngineering Tools to Support NASA Programs, Assignment \nNo. S06012. Washington, DC, NASA Office of Inspector \nGeneral, 2006.\n\n\u2014. NASA Cost Estimating Handbook. Washington, \nDC, 2004.\n\n\u2014. NASA-STD-3001, NASA Space Flight Human \nSystem Standard Volume 1: Crew Health. Washington, \nDC, 2007.\n\n\u2014. NASA-STD-(I)-7009, Standard for Models and \nSimulations. Washington, DC, 2006.\n\n\u2014. NASA-STD-8719.13, Software Safety Standard, \nNASA Technical Standard, Rev B. Washington, DC, 2004.\n\n\u2014. NASA-STD-8729.1, Planning, Developing, and \nMaintaining and Effective Reliability and Maintainability \n(R&M) Program. Washington, DC, 1998.\n\n\u2014. NOAA N-Prime Mishap Investigation Final Report.  \nWashington, DC, 2004.\n\n\u2014. NPD 2820.1, NASA Software Policy. Washington, \nDC, 2005.\n\n\u2014. NPD 8010.2, Use of the SI (Metric) System of Mea-\nsurement in NASA Programs. Washington, DC, 2007.\n\n\u2014. NPD 8010.3, Notification of Intent to Decommis-\nsion or Terminate Operating Space Systems and Terminate \nMissions. Washington, DC, 2004.\n\n\u2014. NPD 8020.7, Biological Contamination Control \nfor Outbound and Inbound Planetary Spacecraft. Wash-\nington, DC, 1999.\n\n\u2014. NPD 8070.6, Technical Standards. Washington, \nDC, 2003.\n\n\u2014. NPD 8730.5, NASA Quality Assurance Program \nPolicy. Washington, DC, 2005.\n\n\u2014. NPR 1441.1, NASA Records Retention Schedules. \nWashington, DC, 2003.\n\n\u2014. NPR 1600.1, NASA Security Program Procedural \nRequirements. Washington, DC, 2004.\n\n\u2014. NPR 2810.1, Security of Information Technology. \nWashington, DC, 2006.\n\n\u2014. NPR 7120.5, NASA Space Flight Program and \nProject Management Processes and Requirements. Wash-\nington, DC, 2007.\n\n\u2014. NPR 7120.6, Lessons Learned Process. Wash-\nington, DC, 2007.\n\n\u2014. NPR 7123.1, Systems Engineering Processes and \nRequirements. Washington, DC, 2007.\n\n\u2014. NPR 7150.2, NASA Software Engineering Re-\nquirements. Washington, DC, 2004.\n\n\u2014. NPR 8000.4, Risk Management Procedural Re-\nquirements. Washington, DC, NASA Office of Safety and \nMission Assurance, 2007.\n\n\u2014. NPR 8020.12, Planetary Protection Provisions for \nRobotic Extraterrestrial Missions. Washington, DC, 2004.\n\n\u2014. NPR 8580.1, Implementing The National Envi-\nronmental Policy Act and Executive Order 12114. Wash-\nington, DC, 2001.\n\n\u2014. NPR 8705.2, Human-Rating Requirements for \nSpace Systems. Washington, DC, 2005.\n\n\u2014. NPR 8705.3, Probabilistic Risk Assessment Pro-\ncedures Guide for NASA Managers and Practitioners. \nWashington, DC, 2002.\n\n\n\n330 ? NASA Systems Engineering Handbook\n\nBibliography\n\n\u2014. NPR 8705.4, Risk Classification for NASA Pay-\nloads. Washington, DC, 2004.\n\n\u2014. NPR 8705.5, Probabilistic Risk Assessment (PRA) \nProcedures for NASA Programs and Projects. Washington, \nDC, 2004. \n\n\u2014. NPR 8710.1, Emergency Preparedness Program. \nWashington, DC, 2006.\n\n\u2014. NPR 8715.2, NASA Emergency Preparedness Plan \nProcedural Requirements\u2014Revalidated. Washington, \nDC, 1999.\n\n\u2014. NPR 8715.3, NASA General Safety Program Re-\nquirements. Washington, DC, 2007.\n\n\u2014. NPR 8715.6, NASA Procedural Requirements for \nLimiting Orbital Debris. Washington, DC, 2007.\n\n\u2014. NPR 8735.2, Management of Government Quality \nAssurance Functions for NASA Contracts. Washington, \nDC, 2006.\n\n\u2014. NSS-1740.14, NASA Safety Standard Guidelines \nand Assessment Procedures for Limiting Orbital Debris. \nWashington, DC, 1995.\n\n\u2014. Off-the-Shelf Hardware Utilization in Flight Hard-\nware Development, MSFC NASA MWI 8060.1 Rev A. \nWashington, DC, 2004.\n\n\u2014. Off-the-Shelf Hardware Utilization in Flight \nHardware Development, JSC Work Instruction EA-WI-\n016. Washington, DC.\n\n\u2014. Project Management: Systems Engineering & \nProject Control Processes and Requirements, JPR 7120.3. \nWashington, DC, 2004.\n\n\u2014. The SEB Source Evaluation Process. Washington, \nDC, 2001.\n\n\u2014. Solicitation to Contract Award. Washington, DC, \nNASA Procurement Library, 2007.\n\n\u2014. Statement of Work Checklist. Washington, DC.\n\n\u2014. System and Software Metrics for Performance-\nBased Contracting. Washington, DC.\n\n\u2014. Systems Engineering Handbook, SP-6105. Wash-\nington, DC, 1995.\n\n\u2014. Training Manual for Elements of Interface Defi-\nnition and Control, NASA Reference Publication 1370. \nWashington, DC, 1997.\n\nNASA Langley Research Center. Instructional Handbook \nfor Formal Inspections.\n\n______. Guidance on System and Software Metrics for \nPerformance-Based Contracting.\n\nNational Defense Industrial Association. Data Manage-\nment, ANSI/GEIA GEIA-859. Arlington, VA, 2004.\n\n\u2014. National Consensus Standard for Configura-\ntion Management, ANSI/GEIA EIA-649, Arlington, VA, \n1998.\n\nNaval Air Systems Command. Systems Command SE \nGuide: 2003 (based on requirements of ANSI/EIA 632: \n1998). Patuxent River, MD, 2003. \n\nNuclear Regulatory Commission. NUREG-0700, \nHuman-System Interface Design Review Guidelines, Rev. \n2. Washington, DC, Office of Nuclear Regulatory Re-\nsearch, 2002.\n\n\u2014. Systems Engineering Guide. Patuxent River, MD, \n2003.\n\nPrice, H. E. \u201cThe Allocation of Functions in Systems.\u201d \nHuman Factors 27: 33\u201345. 1985.\n\nThe Project Management Institute\u00ae. Practice Standards for \nWork Breakdown Structures. Newtown Square, PA, 2001.\n\nRechtin, Eberhardt. Systems Architecting of Organiza-\ntions: Why Eagles Can\u2019t Swim. Boca Raton: CRC Press, \n2000.\n\nSaaty, Thomas L. The Analytic Hierarchy Process. New \nYork: McGraw-Hill, 1980.\n\nSage, Andrew, and William Rouse. The Handbook of Sys-\ntems Engineering and Management. New York: Wiley & \nSons, 1999.\n\nShafer, J. B. \u201cPractical Workload Assessment in the De-\nvelopment Process.\u201d In Proceedings of the Human Factors \nSociety 31st Annual Meeting. Santa Monica: Human Fac-\ntors Society, 1987.\n\nStamelatos, M., H. Dezfuli, and G. Apostolakis. \u201cA Pro-\nposed Risk-Informed Decisionmaking Framework for \nNASA.\u201d In Proceedings of the 8th International Confer-\nence on Probabilistic Safety Assessment and Management. \nNew Orleans, LA, May 14\u201318, 2006.\n\nStern, Paul C., and Harvey V. Fineberg, eds. Under-\nstanding Risk: Informing Decisions in a Democratic So-\nciety. Washington, DC: National Academies Press, 1996. \n\n\n\nBibliography\n\nNASA Systems Engineering Handbook ? 331\n\nTaylor, Barry. Guide for the Use of the International System \nof Units (SI), Special Publication 811. Gaithersburg, MD: \nNational Institute of Standards and Technology, Physics \nLaboratory, 2007. \n\nU.S. Air Force. SMC Systems Engineering Primer and \nHandbook, 3rd ed. Los Angeles: Space & Missile Systems \nCenter, 2005. \n\nU.S. Army Research Laboratory. Design Guidance for \nProducibility, MIL HDBK 727. Adelphi, MD: Weapons \nand Materials Research Directorate, 1990.\n\n\u2014. White Paper on Risk-Informed and Perfor-\nmance-Based Regulation, SECY-98-144. Washington, \nDC, 1998.\n\n\n\n332 ? NASA Systems Engineering Handbook\n\nIndex\n\nacceptance verification, 91\nacknowledgment of receipt of system, 82\nacquisition, product, 129, 175, 217\u2013227, 316\naction-information analysis for HF, 250\u2013251\nactivity-on-arrow diagram, 115, 116\nactual cost of work performed (ACWP), 190\nadvancement degree of difficulty assessment (AD2), 293\nagreements, 120, 125, 172\n\n(see also contracting)\nAHP (analytic hierarchy process), 211\u2013212\nallocated baseline, CI, 152, 153\nanalogous cost models, 128\nanalysis validation type, 100\nanalysis verification type, 86\nanalytic hierarchy process (AHP), 211\u2013212\nanomaly resolution and maintenance operations, 39\napproval phase, 19, 20, 21\n\n(see also formulation)\narchitecture\n\nIT, 243\nmodeling of, 49\u201351\nsystem, 56\nand technology assessment, 296\n\nas-deployed baseline, CI, 153\nassembly, product, 78, 79\u201382\n\n(see also Phase D)\nassessments\n\n(see also performance; reviews)\nTA, 62, 293\u2013298\ntechnical, 5, 61, 166\u2013170, 190\u2013196, 222\nworkload assessment for HF, 68\n\naudits, 91, 154, 157, 168, 189\nauthority\n\nCM, 152, 154\ndecision analysis, 199\nand KDPs, 19\nmission, 34\nrequirements management, 134, 135\nstakeholder, 35\nand standards, 48\ntechnical assessments, 168\u2013170\nand technical planning, 120, 122\n\nBAC (budget at completion), 191\nbackward compatibility of engineering tools, 244\nbaselines\n\nconfiguration identification, 152\u2013153\ndesign solution, 61\nand life cycle phases, 24\nrequirements, 134\n\nsystem design processes, 32\nBCWP (budgeted cost of work performed), 190\nBCWS (budgeted cost of work scheduled), 122, 190\nbeta curve in cost estimate, 129\nbidirectional traceability of requirements, 132\nbounding approaches to quantification of risk, 147\nbudget at completion (BAC), 191\nbudget considerations in technical planning, 117, 118\nbudget cycle, 29\u201330\nbudgeted cost of work performed (BCWP), 190\nbudgeted cost of work scheduled (BCWS), 122, 190\n\nCAMs (cost account managers), 121, 190\ncapability for accelerated concurrent engineering (CACE), \n\n234\u2013241\nCCB (configuration control board), 133, 154, 155\nCDR (Critical Design Review), 25, 77, 178\nCE (concurrent engineering), 234\u2013241\nCERR (Critical Event Readiness Review), 186\ncertification, model, 104\nChandra project, 193\u2013194\nCIs (configuration items), 152\nclassified national security information (CNSI), 162\u2013163\ncloseout, project (see Phase F)\nCM (configuration management) (see configuration manage-\n\nment (CM))\nCMO (configuration management organization), 152, 155\nCM topic evaluators list, 133\nCNSI (classified national security information), 162\u2013163\ncoding/making end products, 73\u201374\ncollaboration design paradigm, 234\u2013241, 242\u2013243\nColumbia disaster investigation, 156\u2013157\nCommittee on Space Research (COSPAR), 260\ncompatibility analysis and product integration, 81\nconcept development (see Phase A)\nconcept of operations (ConOps) (see ConOps (concept of op-\n\nerations))\nconcept studies (Pre-Phase A), 7, 8, 22\nconcurrent engineering (CE), 234\u2013241\nconfiguration audits, 189\nconfiguration change management, 154\nconfiguration control board (CCB), 133, 154, 155\nconfiguration inspection (PCA), 189\nconfiguration items (CIs), 152\nconfiguration management (CM)\n\nand contracting, 222\nand data management, 161\nidentification, 152\u2013153\nplanning, 152, 311\nand requirements changes, 133\u2013134\n\n\n\nIndex\n\nNASA Systems Engineering Handbook ? 333\n\nin technical management process, 111, 122, 151\u2013157\nconfiguration management organization (CMO), 152, 155\nconfiguration status accounting (CSA), 154\nconfiguration verification, 91, 155\u2013156\nConOps (concept of operations)\n\nHF participation in, 247\nin product realization, 88\nin requirements definition, 41, 42\u201343\nSE role of, 9\u201313, 15\nin stakeholder expectations definition, 35\u201337\nin system design processes, 35\u201336, 37\u201338, 41\n\nconstraints in system design processes, 35, 41\ncontext diagrams, 44, 291\ncontingency planning, technical, 114\ncontinuous risk management (CRM), 142\u2013143, 227\ncontracting\n\nacquisition strategy, 217\u2013219\ncompletion, 230\u2013233\nintroduction, 217\nperformance, 227\u2013230\nplanning and preparation, 219\u2013227\n\ncontractor off-the-shelf (COTS) products, 226, 227\ncontractors, working with, 159, 217\ncontract specialist, 219\ncontract WBS (CWBS), 123\ncontrolled experimentation and HF analysis, 254\nCOSPAR (Committee on Space Research), 260\ncost account managers (CAMs), 121, 190\ncost account plans, 121\u2013122\ncost aspects of SE\n\ncost-effectiveness, 16\u201317, 21, 44, 58, 209\u2013210\nestimates, 119, 126, 127, 128\u2013129, 226\nsystem architecture, 50\ntechnical assessment, 190\u2013191\ntechnical planning, 115, 117, 118\u2013119, 121, 125, 126\u2013129\nvalidation, 100\nverification, 89\n\ncost-benefit analysis, 209\u2013210\ncost cap, 118\u2013119\ncost performance index (CPI), 191\ncost risk, 139, 144\nCOTS (contractor off-the-shelf) products, 226, 227\nCPI (cost performance index), 191\ncriteria\n\n(see also reviews)\nacceptance for contracted deliverables, 230\ndecision, 199\u2013201\nand design solution definition, 57\nMCDA, 211\u2013212\nperformance, 41\nproposal evaluation, 227\nsuccess, 31, 34, 35, 57\u201359\n\nCritical Design Review (CDR), 25, 77, 178\nCritical Event Readiness Review (CERR), 186\ncritical incident study, HF, 250\ncritical path sequence, 115\nCRM (continuous risk management), 142\u2013143, 227\n\ncrosscutting processes, 111\n(see also technical management processes)\n\nCSA (configuration status accounting), 154\ncumulative average curve approach, 129\ncustomers in system design processes, 33\u201334\n\n(see also stakeholders)\nCWBS (contract WBS), 123\n\ndata, definition, 160\ndata call, 160\ndata capture requirements, 47, 48, 159\ndata formats and interoperability, 243\ndata management (DM), 122, 158\u2013165, 222\nDCR (Design Certification Review), 188\ndebris, space, limitation of, 29\ndecision analysis\n\nand contracting, 222\nand product validation, 102\nand product verification, 87\nrisk-informed, 142, 143\u2013148\nin system design processes, 31\nin technical management processes, 197\u2013215\n\ndecision networks, 210\ndecision trees, 210\u2013211\ndecommissioning, 28, 233\nDecommissioning Review (DR), 187\ndefense article, and ITAR, 165\ndemonstration validation type, 100\ndemonstration verification type, 86\ndeployment\n\nas-deployed baseline, 153\nin launch operations, 39\nverification of, 91\u201392\n\ndesign\n(see also Phase B; system design processes)\nCDR, 25, 77, 178\ncollaboration paradigm for, 234\u2013241, 242\u2013243\nintegrated facilities for, 234\u2013241\nin life cycle phases, 22, 24\u201325, 26\nprocess metrics for, 196\nand qualification verification, 91\nrealization processes for, 71, 73\u201382\nsolution definition for, 31, 55\u201369, 81, 234\u2013241\ntool selection, 242\u2013245\nand verification vs. validation, 83\n\nDesign Certification Review (DCR), 188\ndesign drivers, 34, 35\ndesign-to-life-cycle cost, 127\u2013128\ndeterministic safety requirements, 45\ndevelopment phase and ILS, 65\n\n(see also Phase D)\ndiscrepancies, product, and verification, 88\ndisposal process, 28, 39, 92, 187, 233\nDM (data management), 122, 158\u2013165, 222\nDR (Decommissioning Review), 187\n\nEAC (estimate at completion), 191\nearned value management (EVM), 121\u2013122, 190, 196\n\n\n\n334 ? NASA Systems Engineering Handbook\n\nIndex\n\nEcho balloons, 17\nEFFBDs (enhanced functional flow block diagrams), 285\neffectiveness vs. cost for SE, 16\nefficient solutions, 16\nEMO (Environmental Management Office), 256, 257\nemulators and interface verification, 82\nenabling products, 60\u201361, 79, 102\nend of mission (EOM), planetary protection report, 259\u2013260\nend-to-end system testing, 93\u201396\nengineering (see systems engineering (SE))\nengineering (grassroots) cost models, 128\nenhanced functional flow block diagrams (EFFBDs), 285\nentrance criteria (see reviews)\nenvironmental compliance and restoration (ECR), 195\nenvironmental considerations\n\nand HF, 247\nNEPA compliance, 256\u2013267\nplanetary protection policy, 258\u2013260\nand product realization, 76, 87, 102, 109\nradioactive materials management, 257\u2013258\nand technical requirements, 41, 44\u201345\n\nEnvironmental Management Office (EMO), 256, 257\nEO (executive order) 12114, 257\nEOM (end of mission), planetary protection report, 259\u2013260\nestimate at completion (EAC), 191\nestimates, cost, 119, 126, 127, 128\u2013129, 226\nevaluation\n\n(see also assessments; validation; verification)\nand contracting, 226, 227, 234\ndecision analysis methods and tools, 200\u2013201\nhuman factors engineering, 68, 247\u2013255\noverview, 71\nPERT chart, 115\nsafety, 257\nT&E, 100\n\nevent sequence diagrams/event trees, 63\nEVM (earned value management), 121\u2013122, 190, 196\nexecutive order (EO) 12114, 257\nexploration projects, 36\nextensibility attributes in decision analysis, 212\n\nfabrication (see Phase C)\nfacilities, integrated design, 234\u2013241\nFAD (formulation authorization document), 19, 125\nfailure modes and effects, and criticality analyses (FMECAs), 146\nfailure modes and effects analyses (FMEAs), 63\u201364, 146, 252\u2013\n\n253\nfault trees, 146, 252\nFCA (functional configuration audit), 189\nfixed-cost profile, 118\u2013119\nflexibility attributes in decision analysis, 212\nFlight Readiness Review (FRR), 25, 184\nFlight Systems and Ground Support (FS&GS), 19\nFMEAs (failure modes and effects analyses), 63\u201364, 146, 252\u2013\n\n253\nFMECAs (failure modes and effects, and criticality analyses), 146\nformulation\n\nactivities of, 19, 21, 125\nlife cycle role of, 20\noverview, 7, 8\nand system architecture, 50\n\nformulation authorization document (FAD), 19, 125\nFRR (Flight Readiness Review), 25, 184\nFS&GS (Flight Systems and Ground Support), 19\nfunctional allocation, HF, 251\nfunctional analysis\n\nFFBDs, 52\u201354, 285\u2013288\nrequirements allocation sheets, 286\u2013287, 289\nsystem architecture, 49\u201351\nand trade study process, 205\u2013206\n\nfunctional baseline, CI, 152, 153\nfunctional configuration audit (FCA), 189\nfunctional flow analysis, HF, 250\nfunctional flow block diagram (FFBD), 52\u201354, 285\u2013288\nfunctional needs requirements, 41\u201344\nfunding issues\n\nBAC, 191\nBCWS, 122, 190\nbudget cycle, 29\u201330\nin technical planning, 117, 118\n\nGantt chart, 117, 118\ngeostationary (GEO) satellites, 29\ngoal setting in system design processes, 35\nGovernment mandatory inspection points (GMIPs), 65\ngrassroots cost estimates, 128\n\nhardware-in-the-loop (HWIL) testing, 96\u201397, 115\nhazard analysis, 64\nhazard vs. risk, 139\nheritage products, 76\u201377, 89\nhuman error, 68\nhuman factors (HF) engineering, 45, 67\u201369, 246\u2013255\nhuman reliability analysis, 64, 68\nhuman spaceflight projects, 36, 45, 176\nHWIL (hardware-in-the-loop) testing, 96\u201397, 115\n\nICD (interface control document/drawing), 81, 137\u2013138\nICP (interface control plan), 138\nIDD (interface definition document), 138\nidentification, configuration, 152\u2013153\nILS (integrated logistics support), 65, 66\nimplementation\n\nactivities of, 21\nand integration, 80\nlife cycle role, 20\noverview, 7, 8\nproduct, 73\u201377, 80\nand transition, 108\n\ninfluence diagrams, 210\ninformation, data definition, 160\ninformation infrastructure for CACE, 238\u2013239\ninformation technology (IT) architectures, 243\nin-orbit checkout in launch operations, 39\nin-process testing, 91\n\n\n\nIndex\n\nNASA Systems Engineering Handbook ? 335\n\ninspections\nconfiguration, 189\nGMIPs, 65\nand product integration, 82\nof purchased products, 74\u201375\n\ninspection validation type, 100\ninspection verification type, 86\nINSRP (Interagency Nuclear Safety Review Panel ), 257\nintegrated design facilities, 234\u2013241\nintegrated logistics support (ILS), 65, 66\nintegration\n\n(see also Phase D)\ncomponents of, 39\nand design solution, 81, 234\u2013241\nand interface management, 137\nplan outline, 299\u2013300\nproduct, 78\u201382\nand SEMP content outline, 303\u2013307\nSIR, 180\n\nInteragency Nuclear Safety Review Panel (INSRP), 257\ninterface control document/drawing (ICD), 81, 137\u2013138\ninterface control plan (ICP), 138\ninterface definition document (IDD), 138\ninterface requirements document (IRD), 81, 137\ninterfaces\n\ndefining, 82\nand end-to-end testing, 94\nand HF, 246\u2013255\ninformation, 243\nmanagement of, 54, 81\u201382, 111, 136\u2013138, 221\nN2 diagrams, 52, 54, 288, 289\u2013290\nand product integration, 79, 80\u201381\nrequirements, 41, 44, 81, 137, 309\u2013310\nverification, 82\n\ninterface working group (IWG), 136\u2013137\ninternal task agreement (ITA), 120\ninternational environmental considerations, 257\nInternational Traffic in Arms Regulations (ITAR), 164\u2013165\ninteroperability and engineering tool selection, 243\nIRD (interface requirements document), 81\nITA (Internal Task Agreement), 120\nITAR (International Traffic in Arms Regulations), 164\u2013165\niterative processes, 5\u201315\nIT (information technology) architectures, 243\nIWG (interface working group), 136\u2013137\n\nJet Propulsion Laboratory (JPL), 258\n\nKennedy Space Center (KSC), 258\nkey decision points (KDPs), 19, 20, 21, 111, 168\n\nlaunch, product, 39\n(see also Phase D)\n\nlaunch vehicle databook, 257\u2013258\nlayers, definition, 8\nLCCE (life-cycle cost estimate), 119\nlearning curve concept and cost estimates, 129\nleast-cost analysis, 209\n\nLEO (low Earth orbiting) missions, 29\nlessons learned in technical planning, 129\u2013130\nlevels, definition, 8\nlicense schemes for engineering tools, 244\nlife cycle\n\n(see also phases)\nacquisition, 218\nbudget cycle, 29\u201330\nCACE, 234\nCM role in, 155\ncost considerations, 126\u2013129\ndecision analysis, 197\nenvironmental considerations, 256\nfunctional analysis in, 285\nHF in, 248\noverview, 6, 7\nphase overview, 19\u201329\nplanetary protection considerations, 258\u2013259\nplanning and status reporting feedback loop, 167\nproduct realization, 71\u201372, 78\u201379, 80, 89\u201390, 103, 106, 108\nsystems analysis, 203\u2013205\ntechnical planning, 112\u2013113\ntechnology assessment, 293\u2013295\ntradeoffs during, 316\nWBS, 124\u2013125\n\nlife-cycle cost estimate (LCCE), 119\nlink analysis and HF, 253\nlogical decomposition, 31, 49\u201354, 120\nloosely coupled programs, 169\nlow Earth orbiting (LEO) missions, 29\n\nmaintainability, product/design, 65, 66, 110\nmaking/coding end products, 73\u201374, 75\nmargin risk management method, 149\u2013150, 194\nmaturity, system, 6, 56\u201358, 62\nMAUT (multi-attribute utility theory), 213\nMCDA (multi-criteria decision analysis), 211\u2013212\nMCR (Mission Concept Review), 173\nMDAA (mission directorate associate administrator), 21\nMDR (Mission Definition Review), 175\nmeasures and measurement methods, defining, 205\nmeasures of effectiveness (MOEs), 120, 191\u2013192, 193\nmeasures of performance (MOPs), 120, 192, 193\nmemorandum of understanding (MOU), 120\nmetric system usage, 261\u2013262\nmilestone reviews, 170\nmission\n\nin life cycle phases, 22, 28\nand stakeholder expectations, 34\n\nmission assurance, 225\nmission authority, 34\nMission Concept Review (MCR), 173\nMission Definition Review (MDR), 175\nmission directorate associate administrator (MDAA), 21\nmitigation of risk, 148, 149\nmodeling\n\nof architecture, 49\u201351\n\n\n\n336 ? NASA Systems Engineering Handbook\n\nIndex\n\ncertification, 104\ncost, 128, 129\nHF, 68\nlogic, 64\nscenario-based hazard, 141\nsimulations, 68, 96, 104, 204\u2013205, 253\u2013254\nvalidation, 104\nverification, 96\u201397\n\nmodeling and simulation (M&S), 96\u201397, 103\nMOEs (measures of effectiveness), 120, 191\u2013192, 193\nMOPs (measures of performance), 120, 192, 193\nMOU (memorandum of understanding), 120\nM&S (modeling and simulation), 96\u201397, 103\nmulti-attribute utility theory (MAUT), 213\nmulti-criteria decision analysis (MCDA), 211\u2013212\n\nN2 diagrams (N x N interaction matrix), 52, 54, 288, 289\u2013290\nNASA, contracting responsibilities, 232\nNational Environmental Policy Act (NEPA), 256\u2013257\nNational Space Policy (2006), 260\nnetwork infrastructure for CACE, 239\nnetwork scheduling systems, 115\u2013117\nNOAA N-Prime mishap, 157\nnominal testing, 101\nnondominated solutions, 16\nnuclear materials management, 257\u2013258\n\nobjectives, mission, 34\nobjectives hierarchy, 213, 214\u2013215\nOffice of General Counsel (OGC), 257\noff-nominal testing, 101\noff-the-shelf (OTS) products, 76\u201377\nOperational Readiness Review (ORR), 153, 183\noperations\n\n(see also Phase E)\nanalysis of, 249, 254, 285\u2013286\nobjectives for, 34, 35\nphases of, 39, 65, 232\nand requirements, 43\nverification of, 92\n\nORR (Operational Readiness Review), 153, 183\nOTS (off-the-shelf) products, 76\u201377\noutcome variables in trade studies, 205, 207\u2013208\nOuter Space Treaty, 258\n\nparallelism in design integration, 234\nparametric cost models, 128, 129\npayload classification and verification, 89\nPBS (product breakdown structure) (see product breakdown \n\nstructure (PBS))\nPCA (physical configuration audit), 189\nPCA (program commitment agreement ), 125, 172\nPDR (Preliminary Design Review), 25, 77, 177\npeer reviews, 59\u201360, 87, 170\u2013171, 189\u2013190, 312\u2013315\nperformance\n\n(see also evaluation; technical performance measures \n(TPMs))\n\nassessment of contractor, 232, 233\nHF evaluation of, 255\n\nrequirements for, 41\u201342, 43, 133, 152, 153, 224, 227\nperiodical technical reviews (PTRs), 166\n\n(see also reviews)\nPERT (program evaluation and review technique) chart, 115\nPFAR (Post Flight Assessment Review), 186\nPHA (preliminary hazard analysis), 64\nPhase A\n\nactivities of, 22, 23\nlife cycle role of, 19, 20\noverview, 7, 8\u20139\n\nPhase B\nactivities of, 24\nlife cycle role of, 19, 20\noverview, 7, 8, 14\u201315\n\nPhase C\nactivities of, 25, 26\nlife cycle role of, 19, 20\noverview, 6, 7, 8, 14\u201315\n\nPhase D\nactivities of, 25, 27, 65\nlife cycle role of, 19, 20\noverview, 6, 7, 8, 14\u201315\n\nPhase E\nactivities of, 28\nlife cycle role of, 19, 20\noverview, 6, 7, 8, 14\n\nPhase F\nactivities of, 28\u201329\nlife cycle role of, 19, 20\noverview, 6, 7, 8, 14\n\nphases\n(see also formulation; implementation; Phase C; Phase D; \n\nPhase E; Phase F)\nA, 7, 8\u20139, 19, 20, 22, 23\nB, 7, 8, 14\u201315, 19, 20, 24\nlife cycle role of, 20\noverview and example, 6\u201315\nPre-Phase A, 7, 8, 22\n\nphysical configuration audit (PCA), 189\nPKI (public key infrastructure), 163\nplanetary protection officer (PPO), 258\nplanetary protection policy, 258\u2013260\nplanning, programming, budgeting, and execution (PPBE), \n\n29\nPLAR (Post Launch Assessment Review), 185\nPMC (Program Management Council), 168\nPM (program manager), 19\npolicy compliance issues, 242, 256\u2013260\nPost Flight Assessment Review (PFAR), 186\nPost Launch Assessment Review (PLAR), 185\nPPBE (planning, programming, budgeting, and execution), \n\n29\nPPO (planetary protection officer), 258\nPQASP (program/project quality assurance surveillance \n\nplant), 65\nPRA (probabilistic risk assessment), 139, 146\nprecedence diagrams, 115\u2013116\n\n\n\nIndex\n\nNASA Systems Engineering Handbook ? 337\n\npre-launch verification stage, 91\u201392\npreliminary design (see Phase B)\nPreliminary Design Review (PDR), 25, 77, 177\npreliminary hazard analysis (PHA), 64\nPre-Phase A, 7, 8, 22\nprobabilistic risk assessment (PRA), 139, 146\nprobabilistic structural analysis, 64\nprocess metrics, 195\u2013196\nprocurement process and contracts, 217\nproducibility and system design, 66\u201367\nproduct baseline, CI, 153\nproduct breakdown structure (PBS)\n\nlogical decomposition, 52\nin systems design processes, 40\nin technology assessment, 294, 295\nand WBS, 123, 124, 125, 126\n\nProduction Readiness Review (PRR), 179\nproductivity-related process metrics, 195\nproduct management and contracts, 219\nproduct realization processes\n\nacquisition, 129, 175, 217\u2013227, 316\nexample, 12\u201314\nimplementation, 73\u201377, 80\nintegration, 78\u201382\noverview, 5, 71\u201372\ntransition, 71, 106\u2013110\nvalidation, 98\u2013105\nverification, 83\u201397\n\nproducts, systems analysis of, 203\u2013205\nprogram commitment agreement (PCA), 125, 172\nprogram evaluation and review technique (PERT) chart, 115\nProgram Management Council (PMC), 168\nprogram manager (PM), 19\nprogrammatic risk, 139, 144\nprogram/project quality assurance surveillance plant (PQASP), \n\n65\nprograms, 4, 169\u2013170\n\n(see also life cycle; phases)\nProgram/System Definition Review (P/SDR), 170, 172\nProgram/Systems Requirements Review (P/SRR), 170, 171\nproject protection plan, 260, 321\u2013322\nprojects, 4, 119, 169\u2013170\n\n(see also life cycle; phases)\nproposal, contract solicitation, 226, 227\nPRR (Production Readiness Review), 179\nP/SDR (Program/System Definition Review), 170, 172\nP/SRR (Program/Systems Requirements Review), 170, 171\nPTRs (periodical technical reviews), 166\n\n(see also reviews)\npublic key infrastructure (PKI), 163\npurchasing end products, 73, 74\u201375, 76\n\nqualification verification, 91\nqualitative logic models, 64\nquality assurance (QA), 64\u201365, 90, 229, 230\nquality-related process metrics, 195\nquantitative logic models, 64\n\nradioactive materials management, 257\u2013258\nR&D (research and development), 107\nrealization processes (see product realization processes)\nrecursive processes, 5\u201315\nredline drawings and CM, 157\nreengineering and validation discrepancies, 103\nreengineering and verification discrepancies, 88\nreliability\n\nhuman, 64, 68\nrequirements for, 43, 44\u201345\nin system design process, 63\u201365\n\nreliability block diagrams, 64\nreports\n\ndecision analysis, 201\u2013203\nEOM, 259\u2013260\nlife cycle feedback loop, 167\nrisk management, 150\nSARs, 257\nand SEMP, 120\nSER, 257\nin technical assessment, 167, 190, 195\u2013106\ntechnical planning types, 117\ntrade study, 208\nTRAR, 293, 295\nvalidation, 102\u2013103\nverification, 93\n\nrequest for information (RFI), 218\nrequest for proposal (RFP), 159\nrequirements\n\nallocation of, 45\u201346, 47\ncomposition guidance, 279\u2013281\nand contracts, 218, 221, 223\u2013225, 229\ndata capture, 47, 48, 159\ndecomposition of, 45\u201346\ndefinition processes, 5, 31, 40\u201348, 246\u2013247\nand design solution, 59\nengineering tools, 242\nand HF, 68\nand integration, 79\ninterface, 41, 44, 81, 137, 309\u2013310\nmanagement of, 131\u2013135, 166\nobsolescence of, 93\nOTS products, 77\nperformance, 41\u201342, 43, 133, 152, 153, 224, 227\nprocess metrics for, 196\nproduct transition, 108, 110\nand resource leveling, 117\nsystem, 24, 174\nand system design processes, 32, 33\ntraceability of, 132\nvalidation of, 46, 132\u2013133, 280\u2013281, 284\nverification of, 47, 48, 282\u2013283\n\nrequirements allocation sheets, 286\u2013287, 289\nrequirements \u201ccreep,\u201d 134\nresearch and development (R&D), 107\nresource leveling, 117\nreusing end products, 73\u201374, 75, 89\n\n\n\n338 ? NASA Systems Engineering Handbook\n\nIndex\n\nreviews\nCDR, 25, 77, 178\nCERR, 186\nconfiguration audits, 189\nand configuration verification, 91\nDCR, 188\nDR, 187\nFRR, 25, 184\nheritage, 76\u201377\ninternal, 170\u2013171\nlife cycle overview, 20, 21, 23, 24, 25, 26\u201327, 28\nMCR, 173\nMDR, 175\nORR, 153, 183\nPDR, 25, 77, 177\npeer reviews, 59\u201360, 87, 170\u2013171, 189\u2013190, 312\u2013315\nPFAR, 186\nPLAR, 185\npre-verification, 87\nprocess metrics for, 196\nPRR, 179\nP/SDR, 170, 172\nP/SRR, 170, 171\nPTRs, 166\nQA for product verification, 90\nSAR, 91, 182\nSDR, 176\nSIR, 180\nSRR, 174\ntechnical assessment role of, 167\u2013170\nTRR, 92, 181\n\nRFI (request for information), 218\nRFP (request for proposal), 159\nrisk-informed decision analysis, 213\u2013214\nrisk-informed safety requirements, 45\nrisk matrices, 145\u2013146\nrisk metrics, 146\nrisk/risk management\n\nand CM, 151\nand contracting, 221, 227, 228\nand cost-effectiveness, 17\ndefinition, 139\nand design alternatives, 58\nlevels of, 145\u2013146\nprocess of, 139\u2013150\nand requirements changes, 133\nsources of, 145\nand system design process, 63\u201365\nand technical assessment, 166\ntypes of risk, 139, 144\nand verification, 89\n\nrobotic missions, 175\nsafe-hold operations, 39\nsafety, 41, 43, 45, 63\u201364, 110\nsafety analysis reports (SARs), 257\nsafety and mission assurance (SMA) organization, 225\nsafety evaluation report (SER), 257\n\nSARs (safety analysis reports), 257\nSAR (System Acceptance Review), 91, 182\nsatellites, disposal of, 29\nSBU (sensitive but unclassified) information, handling of, \n\n162\u2013164\nscenario-based modeling of hazards, 141\nschedule performance index (SPI), 191\nschedule-related process metrics, 195\u2013196\nschedule risk, 139, 144\nscheduling, 17, 50, 89, 115\u2013117, 190\u2013191\nscience projects, 35\u201337, 39, 46\u201347\nSDR (System Definition Review), 176\nsecurity\n\ndata, 162\u2013165, 244\nspace asset protection, 260\n\nSE engine, 5, 6\u201315, 59\nselection rule, defining, 206\nSEMP (system engineering management plan)\n\ncontent outline, 303\u2013307\nand contracting process, 219, 225\nproduct realization role, 74, 85, 111, 113, 119\u2013120, 122, \n\n208\nand TPM assessment, 193\u2013194\n\nsensitive but unclassified (SBU) information, handling of, \n162\u2013164\n\nSER (safety evaluation report), 257\nservice contracts, 232\nSE (systems engineering) (see systems engineering (SE))\nshall statements in technical requirements definition, 40, 41\nsimilar systems analysis, HF, 249\nsimulations, 68, 96, 104, 204\u2013205, 253\u2013254\nsingle-project programs, 169, 170\nSIR (System Integration Review), 180\nSI (System Internationale) (metric system), 261\nsituational awareness and HF, 255\nSMA (safety and mission assurance) organization, 225\nsoftware\n\nCACE tools, 239\nin contingency planning, 114\ndata management, 161\u2013162\nsecurity of, 165\nverification and validation, 104\u2013105\n\nsolicitation of contract, 219, 222\u2013227\nsource evaluation board, 226\nSOW (statement of work), 223, 224\u2013225, 232, 317\u2013320\nspace asset protection, 260\nspace situational awareness (SSA), 260\nspace systems tradeoffs, 316\nspace transportation system (STS), 8\u201315\nsparing/logistics models, 64\nspecialty engineering, 62\u201363\nspecialty requirements, 42, 43\nspecifications, design, 61\nSRB (standing review board), 168, 170\nSRR (System Requirements Review), 174\nSSA (space situational awareness), 260\nstakeholders\n\n\n\nIndex\n\nNASA Systems Engineering Handbook ? 339\n\nCACE process, 234, 235\u2013236, 240\nand CCB, 154\nand CM, 151\ncommitment to technical plan, 120\u2013121\nexpectations definition, 31, 33\u201339\nand HF, 67, 247\nidentifying, 33\u201334\nimportance of ongoing communication, 40\nand MOEs, 192\nand requirements management, 131\nand SAR, 182\nand validation, 98, 99, 100, 103\n\nstandards, 47, 48, 224, 240, 243\nstanding review board (SRB), 168, 170\nstate diagrams, 290\u2013291\nstatement of work (SOW), 223, 224\u2013225, 232, 317\u2013320\nstate transition diagrams, 290, 291\nstatus accounting, configuration, 154\nstatus reporting in technical assessment, 167, 190, 195\u2013196\nSTS (space transportation system), 8\u201315\nsubcontractors, working with, 229, 231\nsubsystem elements, verification of, 82\nsuccess criteria, 34, 35, 57\u201359\nsurveillance of contractor operations, 225, 227\nsustainment (see Phase E)\nsystem, definition, 3\nSystem Acceptance Review (SAR), 91, 182\nSystem Definition Review (SDR), 176\nsystem design processes\n\ndesign solution, 31, 55\u201369, 81, 234\u2013241\nexample, 8\u201312\nand HF, 68\nand interface management, 54\nlogical decomposition, 31, 49\u201354, 120\noverview, 4, 5, 6, 31\u201332\nrequirements definition, 5, 31, 40\u201348, 246\u2013247\nstakeholder expectations definition, 31, 33\u201339\n\nSystem Integration Review (SIR), 180\nSystem Internationale (SI) (metric system), 261\nsystem-of-systems, 51\nSystem Requirements Review (SRR), 174\nsystems, verification of elements, 82\nsystems analysis, 203\u2013205\nsystems engineer\n\nCACE role of, 237\nand QA, 64\u201365\nresponsibilities of, 3\u20134\n\nsystems engineering (SE)\ndefinition, 3\nfundamentals of, 3\u201317\noverview, 1\nprocess metrics, 195\u2013196\nand project/program life cycle, 19\nSE engine, 5, 6\u201315, 59\nspecialty integration into, 62\u201363\n\ntask analysis for HF, 68, 251\u2013252\n\ntask order contracts, 225\nTA (technology assessment), 62, 293\u2013298\nteams, mission development\n\nand CACE process, 234, 235\u2013236, 240\u2013241\ncontract development role, 218\u2013219, 223, 225\ncontractor evaluation role, 234\nand decision solution definition, 57\nHF specialists on, 246\nimportance of technical management processes, 111\ninspection of purchased products, 74\u201375\nrequirements validation role, 133\nrisk management role, 63, 143\nsurveillance of contractor operations, 227, 228\u2013229\ntrade study leadership, 208\n\ntechnical assessment process, 5, 62, 166\u2013170, 190\u2013196, 222\n(see also reviews)\n\ntechnical data package, 160\ntechnical management processes\n\n(see also configuration management (CM); decision anal-\nysis; reviews; risk/risk management)\n\nassessment, 5, 62, 166\u2013196, 222\nand contracts, 218\u2013219, 220\u2013222\ndata management, 122, 158\u2013165, 222\ninterface management, 54, 81\u201382, 111, 136\u2013138, 221\noverview, 5\u20136, 111\nplanning, 112\u2013130\nrequirements management, 131\u2013135, 166\n\ntechnical performance measures (TPMs)\nobjectives hierarchy, 214\u2013215\npurpose of, 192\u2013195\nquantification of, 147\nand risk management, 139, 148\u2013149\nand safety requirements, 45\nutility and value functions of, 213\n\ntechnical processes overview, 4\u20136\n(see also product realization processes; system design pro-\n\ncesses; technical management processes)\ntechnical readiness level (TRL) scale, 293, 295, 296\u2013298\ntechnical requirements definition, 31, 40\u201348\n\n(see also requirements)\ntechnical risk, 139, 144\ntechnical solution definition processes\n\ndesign solution definition, 31, 55\u201369, 81, 234\u2013241\nand HF, 68\nlogical decomposition, 31, 49\u201354, 120\noverview, 5\n\ntechnical work directives and planning, 121\ntechnology assessment (TA), 62, 293\u2013298\ntechnology development, 56, 62\n\n(see also Phase A; Phase B)\ntechnology infusion assessment, 293\u2013298\ntechnology maturity assessment (TMA), 293, 294, 297\ntechnology readiness assessment report (TRAR), 293, 295\ntermination review, 169\ntest and evaluation (T&E), 100\ntesting\n\n(see also Phase D)\n\n\n\n340 ? NASA Systems Engineering Handbook\n\nIndex\n\ncomponents of, 39\nintegration, 81\npost-transition to end user, 109\nin product realization processes, 72\nvalidation, 98\nverification, 83, 85, 91\u201392, 93\u201397, 98\n\ntesting verification type, 86\nTest Readiness Review (TRR), 92, 181\ntest validation type, 100\nT&E (test and evaluation), 100\ntiers, definition, 8\ntightly coupled programs, 170\ntimeline analyses (TLAs), 52, 54\ntimelines, operation, 37\u201338, 68\ntimeline sheets (TLSs), 54\ntiming analysis, 290\u2013291\nTLAs (timeline analyses), 52, 54\nTLSs (timeline sheets), 54\nTMA (technology maturity assessment), 293, 294, 297\nTPM (technical performance measures) (see technical perfor-\n\nmance measures (TPMs))\ntraceability of requirements, 132\ntradeoffs, summary of types, 316\ntrade study process, 57, 59, 128, 142\u2013143, 205\u2013209\ntraining and engineering tool selection, 244\ntransition process, 71, 106\u2013110, 231\u2013233\ntransportability requirements, 110\nTRAR (technology readiness assessment report), 293, 295\ntriggering data and functional flow analysis, 285\nTRL (technical readiness level) scale, 293, 295, 296\u2013298\nTRR (test readiness review), 92, 181\n\nuncertainty, 64, 147\n(see also risk/risk management)\n\nuncoupled programs, 169\nunit curve approach, 129\nUnited States Munitions List (USML), 164\nusability evaluation and design, 68\nutility analysis, 212\u2013213\n\nVAC (variance at completion), 191\nvalidation\n\n(see also testing)\ndesign solution, 60, 62\nand interface management, 137\nin life cycle phases, 25\nplanning, 100, 284, 301\u2013302\nprocess metrics for, 196\nproduct, 71, 72, 75, 98\u2013105\nof requirements, 46, 132\u2013133, 280\u2013281, 284\nvs. verification, 15, 83, 88, 98\n\nvariance at completion (VAC), 191\nvariances, control of project, 190\u2013191\nvendor stability and engineering tool selection, 245\nverification\n\n(see also inspections; testing)\nconfiguration, 91, 155\u2013156\ndesign solution, 59\u201360\nand HF, 68\nof interfaces, 82, 137\nin life cycle phases, 25\nplanning, 84\u201386\nprocess metrics for, 196\nproduct, 71, 72, 75, 82, 83\u201397\nprogram guidance, 89\nof requirements, 47, 48, 282\u2013283\nsample plan outline, 301\u2013302\nsoftware tools, 104\u2013105\nand system design processes, 32\nvs. validation, 15, 83, 88, 98\n\nwaivers, 154\nweighted cost-effectiveness analysis, 210\nwork breakdown structure (WBS)\n\nand cost/schedule control measures, 190\nand logical decomposition, 52\nand technical measures, 194\u2013195\nin technical planning, 116\u2013117, 122\u2013125, 126\n\nworkflow diagrams, technical planning, 115\nworkload assessment for HF, 68, 255\n\n\n\nTo request print or electronic copies or provide comments, \ncontact the Office of the Chief Engineer via \n\nSP6105rev1SEHandbook@nasa.gov \n\nElectronic copies are also available from\nNASA Center for AeroSpace Information\n\n7115 Standard Drive\nHanover, MD 21076-1320\n\nat\nhttp://ntrs.nasa.gov/\n\nFor sale by the Superintendent of Documents, U.S. Government Printing Office\nInternet: bookstore.gpo.gov   Phone: toll free (866) 512-1800;   DC area (202) 512-1800\n\nFax: (202) 512-2104 Mail: Stop IDCC, Washington, DC 20402-0001\n\nISBN 978-0-16-079747-7\n\n\n\n\n\n\n"}